{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: DeprecationWarning: update is deprecated. Use replace_one, update_one or update_many instead.\n",
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: DeprecationWarning: update is deprecated. Use replace_one, update_one or update_many instead.\n"
     ]
    }
   ],
   "source": [
    "replace_list = [['데일리코디','데일리룩'],['데일리패션','데일리룩'],['bts','방탄소년단'],\n",
    "                ['강아지','멍스타그램'],['고양이','냥스타그램'],['꽃','꽃스타그램'],['부산카페','카페'],\n",
    "                ['부산맛집','맛집'],['홍대맛집','맛집'],['선팔해요','선팔환영'],['소통해요','소통환영'],\n",
    "                ['아기','아기스타그램'],['아들','아들스타그램'],['팔로미','팔로우미'],\n",
    "                ['맞팔해요', '맞팔환영'], ['맞팔해용', '맞팔환영'],['선팔맞팔환영', '선팔하면맞팔'], \n",
    "                ['선팔맞팔', '선팔하면맞팔'],['selca', '셀카'], ['사랑', '사랑해'], ['얼스타', '얼스타그램'], \n",
    "                ['daily', '데일리'],['dailylook','데일리룩'], ['diet', '다이어트'], ['ootd','오오티디'],\n",
    "                ['fashion','패션'],['follow','팔로우'],['followme','팔로우미'],['foodstagram','푸드스타그램'],\n",
    "                ['style','스타일'],['travel','여행'],['wedding','웨딩'],['selstagram','셀스타그램'],['selfie','셀피'],\n",
    "                ['lfl','likeforlikes'],['iphone','아이폰'],['baby','베이비'],['beauty','뷰티'],['cafe','카페'],\n",
    "                ['catstagram','캣스타그램'],['dessert','디저트'],['dogstagram','독스타그램'],['fitness','휘트니스'],\n",
    "                ['instafood','인스타푸드'],['instagram','인스타그램'],['model','모델'],['onthetable','온더테이블'],\n",
    "                ['petstagram','펫스타그램'],['instacat','냥스타그램']\n",
    "]\n",
    "\n",
    "for words in replace_list:\n",
    "    for data in datas.find({\"tags\":{\"$in\":['#'+words[0]]}}, {'_id':1, 'tags':1, \"trans_tagapi\":1}):\n",
    "        datas.update({\"_id\":data[\"_id\"]}, { \"$pull\": {\"tags\":\"#\"+words[0]}})\n",
    "        datas.update({\"_id\":data[\"_id\"]}, { \"$push\": {\"tags\":\"#\"+words[1]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: update is deprecated. Use replace_one, update_one or update_many instead.\n"
     ]
    }
   ],
   "source": [
    "remove_list = ['한강','황민현','l','f','홍대','부산','이태원','대구','대전','서울','수원','창원',\n",
    "               '청주','광주', '해운대', '한남동', '제주도', '마산', '광안리', '울산', '강남',\n",
    "               'seoul', '인천','서면','동명동','동성로','둔산동','압구정','잠실','전주','제주',\n",
    "               '청담','가로수길','건대','경주','구미','구시청','남포동','대구맛집','베트남','부천',\n",
    "               '부평','상무지구','연남동','인계동','일본','일산','천안','청담동','충장로','japan','korea'\n",
    "]\n",
    "\n",
    "for word in remove_list:\n",
    "    for data in datas.find({\"tags\":{\"$in\":['#'+word]}}, {'_id':1, 'tags':1, \"trans_tagapi\":1}):\n",
    "        datas.update({\"_id\":data[\"_id\"]}, { \"$pull\": {\"tags\":\"#\"+word}})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "conn = MongoClient('192.168.0.5', 27017)\n",
    "instar = conn.instar\n",
    "datas = instar.tagdetail\n",
    "\n",
    "trans_data = []\n",
    "tag_data = []\n",
    "\n",
    "for data in datas.find({\"trans_tagapi\":{\"$nin\":[None]}}, {'_id':0, 'tags':1, \"trans_tagapi\":1}):\n",
    "    trans_data.append([trans.strip(\" \") for trans in data['trans_tagapi']])\n",
    "    tag_data.append([tag[1:].lower().strip(\" \") for tag in data['tags']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89629, 89629)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_data), len(trans_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_if_exists(L, value):\n",
    "#     try:\n",
    "#         L.remove(value)\n",
    "#     except ValueError:\n",
    "#         pass\n",
    "\n",
    "# def replace_if_exits(L, value):  # value = [pre, post]    \n",
    "#     try:\n",
    "#         L.remove(value[0])\n",
    "#         L.append(value[1])\n",
    "#     except ValueError:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trans, tag in zip(trans_data, tag_data):\n",
    "    # 한글자 영단어, 지역명 제거\n",
    "    \n",
    "    if len(tag) == 0:\n",
    "        tag_data.remove(tag)\n",
    "        trans_data.remove(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49929, 49929)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_data), len(trans_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10918\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(trans_data)\n",
    "img_data = tokenizer.texts_to_sequences(trans_data)\n",
    "\n",
    "maxlen = max([len(img) for img in img_data])\n",
    "\n",
    "V = len(tokenizer.word_index) + 1\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "word_map = {}\n",
    "\n",
    "for word in reverse_word_map.items():\n",
    "    word_map[word[1]] = word[0]\n",
    "    \n",
    "    \n",
    "with open('word2idx.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(word_map, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "wtv = Word2Vec(tag_data, min_count=50)\n",
    "\n",
    "tag_idx = []\n",
    "trans_idx = []\n",
    "ang = []\n",
    "for trans, tag, i in zip(img_data, tag_data, trans_data):\n",
    "    imsi = []\n",
    "    for t in tag:\n",
    "        try:\n",
    "            imsi.append(wtv.wv.vocab[t].index)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    if imsi:\n",
    "        tag_idx.append(imsi)\n",
    "        trans_idx.append(trans)\n",
    "        ang.append(i)\n",
    "\n",
    "wtv.save('word2vec.model')       \n",
    "        \n",
    "wtv.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 19,\n",
       " '체육': 2114,\n",
       " '전기 자동차': 3061,\n",
       " 'gravure idol': 6162,\n",
       " '장식': 168,\n",
       " '빠에야': 4150,\n",
       " '위 핏': 4595,\n",
       " '시원한 문신 아티스트': 10591,\n",
       " '오리 기러기와 백조': 3710,\n",
       " '시원한 운전': 7734,\n",
       " '스틱 및 공 게임': 7682,\n",
       " '포유 동물과 같은 돼지': 6281,\n",
       " '안무': 1515,\n",
       " '식사를': 1154,\n",
       " '중거리 달리기': 9217,\n",
       " '바이에른 크림': 1684,\n",
       " '시원한 선글라스': 4667,\n",
       " '넓은 콩': 7661,\n",
       " '의상 디자인': 1380,\n",
       " '중국 장미': 3391,\n",
       " '머리카락 겹친 머리': 9198,\n",
       " '전기 톱 조각': 10803,\n",
       " '머리카락을 계층화 된': 7058,\n",
       " '그릴 데스': 10326,\n",
       " '와인 랙': 6805,\n",
       " '잠수복': 2121,\n",
       " '창틀 창문': 10627,\n",
       " '카펠리니': 2874,\n",
       " '컵류': 9495,\n",
       " '프리 핸드': 6782,\n",
       " '코바늘 편직물': 7736,\n",
       " '트렌치 외투': 3129,\n",
       " '조리기구 및 베이크웨어': 7585,\n",
       " '얼굴 털': 499,\n",
       " '바다새': 6398,\n",
       " '행복': 117,\n",
       " '세계 랠리 자동차': 9430,\n",
       " '나무들': 6814,\n",
       " '폴란드 타트라 몰이': 8936,\n",
       " '디오': 7783,\n",
       " '살찌 콘': 8742,\n",
       " '블링 블 링': 3471,\n",
       " '이탈리아어 달콤한 고추': 7666,\n",
       " '마스카라': 966,\n",
       " '외륜': 3180,\n",
       " '강아지 유형': 3054,\n",
       " '시베리아어': 10663,\n",
       " '뜨거운 공기 ballooning': 6564,\n",
       " '분말': 2467,\n",
       " '인물 사진': 1816,\n",
       " '프티 4': 2349,\n",
       " '시원한 무릎': 9909,\n",
       " '고기 볶음': 3123,\n",
       " 'houseplant': 640,\n",
       " 'stock photography 그림자': 9879,\n",
       " 'sicilian 피자': 8741,\n",
       " '천문 물체': 6796,\n",
       " 'samoyed 같은 개': 6927,\n",
       " '스패니얼': 8244,\n",
       " '10 핀 볼링': 4695,\n",
       " '동물 이주': 5323,\n",
       " '눈썹': 23,\n",
       " '캐딜락': 6787,\n",
       " '알루미늄 캔': 2364,\n",
       " '턱시도': 363,\n",
       " '멋진 광고': 5734,\n",
       " '스노우 슈즈': 8584,\n",
       " '식물': 56,\n",
       " '이탈리아 음식': 291,\n",
       " '낙지': 10118,\n",
       " '스니커즈': 9745,\n",
       " '인스턴트 삶은 양고기': 1091,\n",
       " '와인 셀러': 7635,\n",
       " '코티지 파이': 7329,\n",
       " '격자 축구': 9340,\n",
       " '육류 · 육류 · 양고기 · 양고기 · 동물 사료 · 식품 · 요리 · 고베 규 · 마쓰 사카 쇠고기 · 육체 · 농산물': 9588,\n",
       " '혼다': 4306,\n",
       " '지불 카드': 8642,\n",
       " '키르': 7830,\n",
       " '캐러멜 색': 6260,\n",
       " '쿵푸': 9314,\n",
       " '나이트 스탠드': 6552,\n",
       " '알레 vongole': 7157,\n",
       " '가재': 4717,\n",
       " '캐비닛': 1317,\n",
       " '오셔': 7593,\n",
       " '꼬치 꼬치밥': 7252,\n",
       " '양배추 수프 다이어트': 6044,\n",
       " 'jangles': 6683,\n",
       " '금속': 416,\n",
       " '자외선 차단제': 5008,\n",
       " '동물성 지방': 2347,\n",
       " '정식 착용 셔츠': 6733,\n",
       " '리프': 5067,\n",
       " '중심가': 8827,\n",
       " '재배': 9394,\n",
       " '점프': 2215,\n",
       " '트릭 샷': 6004,\n",
       " '수 자원': 8075,\n",
       " '일본 원숭이': 8206,\n",
       " '훔쳐보기': 10565,\n",
       " '패밀리': 9479,\n",
       " '당구 테이블': 5854,\n",
       " '식품 저장실': 4398,\n",
       " '스냅 샷': 36,\n",
       " '리플렉스 카메라': 2697,\n",
       " '대산 괴': 2469,\n",
       " '박장국': 6017,\n",
       " 'bánh canh': 7053,\n",
       " '숫자': 882,\n",
       " '노르딕 결합': 5761,\n",
       " '파스텔': 717,\n",
       " '침구': 908,\n",
       " '복숭아 꽃': 7688,\n",
       " '그레고리네 같은 개': 7386,\n",
       " '아메리칸 와이어 헤어': 1654,\n",
       " '낮에': 7764,\n",
       " 'muroidea': 3550,\n",
       " '삼림지': 1750,\n",
       " '옥외 신발': 2404,\n",
       " '스카치 collie': 5615,\n",
       " 'korean jindo dog': 9877,\n",
       " '아가': 7539,\n",
       " '굴 버섯': 7828,\n",
       " '스파게티': 462,\n",
       " '소세지': 9021,\n",
       " '러시아 장난감': 5798,\n",
       " '카페 테이블': 8233,\n",
       " '플레이트': 4384,\n",
       " '귀 같은 고양이': 3679,\n",
       " '추기경': 7418,\n",
       " '서핑 장비 및 용품': 2223,\n",
       " '유대류': 9085,\n",
       " '립 아이 스테이크': 3220,\n",
       " '튀김': 397,\n",
       " '공동 공연 예술': 8084,\n",
       " 'bacardi cocktail': 9743,\n",
       " 'schnauzer 같은 포유 동물': 6941,\n",
       " '관악기': 2576,\n",
       " 'nopal': 9302,\n",
       " '이마 얼 테리어의 글렌': 10794,\n",
       " '뉴스 캐스터': 6895,\n",
       " '커피 마 카치아': 9134,\n",
       " '말티즈': 7233,\n",
       " '야구 필드': 4537,\n",
       " '개성 쇼': 10149,\n",
       " '그랜드 앵글로 프랑소아 tricolore': 8611,\n",
       " '차인 mein': 8646,\n",
       " '양궁': 5247,\n",
       " '유리를 마시다': 4028,\n",
       " '크랜베리': 2652,\n",
       " '도로 용 사이클링': 7839,\n",
       " '냉동 디저트': 208,\n",
       " '프리 조명': 4190,\n",
       " '개 품종 그룹 푸들': 8192,\n",
       " '독일 스피츠 미텔': 4063,\n",
       " '행진 밴드': 6073,\n",
       " '전기 파랑': 703,\n",
       " '발톱 같은 고양이': 9680,\n",
       " '부라 이비스': 9246,\n",
       " '핫떡': 4271,\n",
       " '교통 수단': 759,\n",
       " '희귀 품견': 7261,\n",
       " '하품': 2473,\n",
       " '종탑': 9900,\n",
       " '새해': 3321,\n",
       " '카레 mee': 1832,\n",
       " '긴급 차량': 8400,\n",
       " '클래스': 2650,\n",
       " '미녀': 4194,\n",
       " '잎 식물': 6452,\n",
       " '신속한 운송': 6928,\n",
       " '토지 많이': 3097,\n",
       " '배낭 지느러미 현상': 7159,\n",
       " '한국어 음식': 5398,\n",
       " '\\u200b\\u200b스노우 보드': 9130,\n",
       " '국내 짧은 머리 고양이 같은 고양이': 7196,\n",
       " '캔틸레버 다리': 9405,\n",
       " '녹색 콩': 3935,\n",
       " '팝콘': 2860,\n",
       " '키 라임': 4687,\n",
       " '두 번 돼지 고기 요리법': 10849,\n",
       " '해바라기 씨앗': 4517,\n",
       " '흰색 목자': 10788,\n",
       " '위너 melange': 5837,\n",
       " '동물 소스 식품': 1031,\n",
       " 'naporitan': 1525,\n",
       " '제조법': 524,\n",
       " '주사위 게임': 9512,\n",
       " '폭스 바겐 자동차': 9424,\n",
       " '리모콘': 8149,\n",
       " '한식': 4555,\n",
       " '마쿠 노우치': 9142,\n",
       " '빗물': 10024,\n",
       " '먼치킨 같은 고양이': 6052,\n",
       " '수 중': 2590,\n",
       " '반바지': 412,\n",
       " '장거리 달리기': 2148,\n",
       " '히메 상처': 3167,\n",
       " '머리 묶음': 7100,\n",
       " '롤러 스케이트': 9458,\n",
       " '회의실': 3591,\n",
       " '일러스트 레이션': 1221,\n",
       " '픽시 커트': 8897,\n",
       " 'kombu': 10766,\n",
       " '중화': 8967,\n",
       " '성전': 7417,\n",
       " '동물 원료 식품': 3339,\n",
       " '요크셔 푸딩': 6173,\n",
       " '도보': 2767,\n",
       " 'rice': 10445,\n",
       " '별': 1747,\n",
       " '오트밀 건포도 쿠키': 10104,\n",
       " '듀엣': 2369,\n",
       " '양반': 8082,\n",
       " '포유류 개': 3515,\n",
       " '모피 같은 고양이': 3933,\n",
       " 'roti': 10520,\n",
       " '의료 방사선': 4453,\n",
       " '딸기 파이': 3925,\n",
       " 'cavachon 같은 개': 7229,\n",
       " '제품 디자인': 9730,\n",
       " '아메리칸 코커 스패니얼': 9968,\n",
       " '데본 렉스': 6414,\n",
       " '일본 스 피츠': 1804,\n",
       " '쌀': 527,\n",
       " '캘리포니아 스타일의 피자': 8208,\n",
       " '소매업': 10179,\n",
       " '카니발란': 223,\n",
       " '디스플레이 윈도우': 5751,\n",
       " 'cut 컷': 4021,\n",
       " '보 라 도아': 10188,\n",
       " '월귤 나무속': 8022,\n",
       " '트러스 다리': 8820,\n",
       " '터키 앙고라 같은 고양이': 5775,\n",
       " '베이컨': 2736,\n",
       " '물질과 같은 개': 10025,\n",
       " '로고': 263,\n",
       " '알파 카': 7438,\n",
       " '래퍼': 6604,\n",
       " '조리기구 및 제빵기구': 9159,\n",
       " '천체 개체': 3514,\n",
       " 'tamaskan 개 같은 포유 동물': 8170,\n",
       " '나무 걷는 워커 coonhound': 8607,\n",
       " '호주 목자': 4936,\n",
       " '바삭 튀김 치킨': 7803,\n",
       " 'farofa': 5869,\n",
       " '마야 도시': 9330,\n",
       " '실로 양치기 개': 10138,\n",
       " '수컷': 6369,\n",
       " '동행 개': 3698,\n",
       " '자동차 같은 개': 10671,\n",
       " '주둥이 같은 개가 포유류': 10223,\n",
       " 'bouldering': 4759,\n",
       " 'sōmen': 1407,\n",
       " 'child model': 10741,\n",
       " '영화 스튜디오': 6351,\n",
       " '피망 및 칠리 페퍼': 7659,\n",
       " '농산물': 870,\n",
       " '도심': 2283,\n",
       " '무도회': 2794,\n",
       " '발레 플랫': 1860,\n",
       " '핫도그 롤빵': 4476,\n",
       " '페스토': 10529,\n",
       " 'layered hair': 3690,\n",
       " '과자류': 2967,\n",
       " '수원': 9613,\n",
       " '감자 웨지': 4589,\n",
       " '서체': 592,\n",
       " '그릴': 748,\n",
       " '새펄 리': 7790,\n",
       " '연못': 1674,\n",
       " '밸리': 7712,\n",
       " 'kalimotxo': 7296,\n",
       " '애완 동물': 6677,\n",
       " '큐 스포츠': 3927,\n",
       " '순록': 2710,\n",
       " '핫 초콜릿': 1543,\n",
       " '옷 걸이': 719,\n",
       " '반미 미': 9104,\n",
       " '아메리칸 에스키모 개 운동 단체': 8694,\n",
       " '전송': 6384,\n",
       " '쁘띠 바셋 그리폰 벤텐': 10793,\n",
       " '서쪽 시베리아 laika': 9974,\n",
       " '결혼 기념일': 4070,\n",
       " '리트리버': 1206,\n",
       " '찬장': 3296,\n",
       " 'hotteok': 6099,\n",
       " 'friendship': 7700,\n",
       " '컴퓨터 벽지': 158,\n",
       " '날짜 팜': 5902,\n",
       " '독일어 스 피 츠': 1952,\n",
       " '버즈 컷': 6088,\n",
       " '침대 스커트': 4569,\n",
       " '시계': 838,\n",
       " '간식류': 8059,\n",
       " '범선': 4912,\n",
       " '스푼': 2574,\n",
       " '쇠붙이': 9519,\n",
       " '청력': 9964,\n",
       " '독서': 1287,\n",
       " 'meze': 563,\n",
       " '육체적 운동': 9047,\n",
       " 'canh 추아': 9122,\n",
       " '스테인드 글라스': 5981,\n",
       " '리셉션': 7437,\n",
       " '체력 단련': 4233,\n",
       " '채소 햄버거': 2065,\n",
       " '하우스에서': 4953,\n",
       " 'stock photography 베일': 5767,\n",
       " '우먼': 3218,\n",
       " '현대 소나타': 9541,\n",
       " '물 공원': 8942,\n",
       " '스케이팅': 8702,\n",
       " '중국어 음식': 7998,\n",
       " '벨기에 양치기 malinois': 4406,\n",
       " '미용실': 1799,\n",
       " '개인용 컴퓨터': 1239,\n",
       " '박스': 1377,\n",
       " '포도 나무 가족': 3886,\n",
       " '꼬치': 1420,\n",
       " '기숙사': 6505,\n",
       " '온화한 침엽수 림': 2492,\n",
       " '하리에': 8609,\n",
       " '파테 티에리': 5011,\n",
       " '욕실 싱크대': 2471,\n",
       " '자': 4220,\n",
       " '신화 생물': 3803,\n",
       " '전자 기기': 548,\n",
       " '칼럼': 2914,\n",
       " 'stock photography 광고': 9794,\n",
       " '돼지 찹': 5763,\n",
       " '마스카 포네': 4889,\n",
       " '벽시계': 10146,\n",
       " '화이트 워터 카약': 10383,\n",
       " '이케 바나': 4322,\n",
       " '훈제 햄': 6484,\n",
       " '쥬스': 4381,\n",
       " '슈퍼 바이크 레이싱': 9026,\n",
       " '픽업 트럭': 6429,\n",
       " 'stock photography 자동차 창 부품': 10502,\n",
       " '밀키 방법': 9181,\n",
       " '두 번 요리 돼지 고기': 7824,\n",
       " '아다 나 케밥': 8250,\n",
       " '가장자리 달린 스포츠': 6984,\n",
       " '울프 도그': 5964,\n",
       " '카디 건 웨일스 어 corgi': 2641,\n",
       " '므 스나 베': 8447,\n",
       " '팩 동물': 6127,\n",
       " '닭고기': 817,\n",
       " '꽃 무늬 디자인': 196,\n",
       " 'syrniki': 7325,\n",
       " '법 집행': 3040,\n",
       " '호주 양치기': 7334,\n",
       " '깃털': 1714,\n",
       " '굴 rockefeller': 9378,\n",
       " '배관 공사': 5411,\n",
       " '텍스트 포유류': 10659,\n",
       " '전자 부품': 6251,\n",
       " '쓴 오렌지': 1820,\n",
       " '키스': 1391,\n",
       " 'fur': 9944,\n",
       " '보석류': 833,\n",
       " '튀김 요리': 2088,\n",
       " '클립 아트 같은 고양이': 7966,\n",
       " '무늬': 2042,\n",
       " '알파인 스키': 2775,\n",
       " '힐역': 10384,\n",
       " 'namul': 1753,\n",
       " '스코틀랜드의 배': 4550,\n",
       " '토끼와 토끼': 5033,\n",
       " '패밀리 카': 1909,\n",
       " '데님': 406,\n",
       " '육교': 2606,\n",
       " 'box': 8504,\n",
       " '파티오': 10709,\n",
       " '음료': 94,\n",
       " 'barbet 같은 포유 동물': 8191,\n",
       " '겹겹이 머리띠': 7520,\n",
       " 'sapsali': 3519,\n",
       " 'kindan jindo 개': 9977,\n",
       " '멍청이 자르기': 10730,\n",
       " '오프 로딩': 5236,\n",
       " '수염 난 콜리의 글렌': 8496,\n",
       " '나폴레옹 고양이': 2225,\n",
       " '겹겹이 쌓인 머리': 9631,\n",
       " '인간의 머리카락 색깔': 319,\n",
       " 'nepeta': 9870,\n",
       " '해변과 같은 개': 8921,\n",
       " '숲': 588,\n",
       " '카메라 및 광학': 3307,\n",
       " '직렬 식 자전거': 10078,\n",
       " '비콘': 8812,\n",
       " '차고 문': 9661,\n",
       " '서빙웨어': 5467,\n",
       " '왕게': 1647,\n",
       " '철': 3331,\n",
       " '알팔파 콩나물': 7691,\n",
       " '이그제큐티브 자동차': 3115,\n",
       " '드래곤 리': 1976,\n",
       " '라미안 요리법': 6701,\n",
       " '연예인': 9090,\n",
       " '초밥': 938,\n",
       " '중소 규모 고양이': 10493,\n",
       " '제발': 9584,\n",
       " 'bakeware': 7036,\n",
       " '경주': 1045,\n",
       " '황색': 1278,\n",
       " '이탈리아 아이스': 7462,\n",
       " '검문고': 10165,\n",
       " '숄더백': 923,\n",
       " '과수원': 1920,\n",
       " '웅크 리기': 7762,\n",
       " '조종사': 5938,\n",
       " '복합 건물': 10660,\n",
       " '교배 개': 8304,\n",
       " 'hime cut': 1505,\n",
       " '승마 부츠': 9193,\n",
       " '작품': 6180,\n",
       " 'ecoregion': 549,\n",
       " '알람 시계': 6488,\n",
       " '첼로': 3521,\n",
       " '당나라 soo는한다.': 10500,\n",
       " '보이즈': 8574,\n",
       " '태양 두부 찌개': 10881,\n",
       " '시원한 모자': 6209,\n",
       " '코끼리': 4795,\n",
       " '카레 카레': 4850,\n",
       " '얼어 붙은 누나 텍': 8164,\n",
       " '틴토 드 베라 노': 10710,\n",
       " '덴마크 생과자': 1438,\n",
       " '그라비아 아이돌': 333,\n",
       " '애 마를 낳는 동물': 10256,\n",
       " '물리': 9720,\n",
       " '고대 개 품종과 같은 개': 6034,\n",
       " '더블베이스': 8711,\n",
       " '은 합성 고무': 10686,\n",
       " '갈기': 5224,\n",
       " '봄': 201,\n",
       " '박쥐 및 공 게임': 5253,\n",
       " '강당': 1298,\n",
       " '차가운 mein': 3553,\n",
       " '뮤지컬 극장': 1700,\n",
       " '스포츠가 아닌 그룹': 1774,\n",
       " '캠프': 9482,\n",
       " '왕 찰스 스패니얼': 8909,\n",
       " '부대 찌개': 6135,\n",
       " 'entlebucher 산 개': 8367,\n",
       " '애완견': 4658,\n",
       " '여자': 42,\n",
       " '도복': 3587,\n",
       " '국물': 1819,\n",
       " '타트': 3612,\n",
       " '건물': 142,\n",
       " '피트니스 및 인물 경쟁': 3423,\n",
       " '콘택트 렌즈': 3057,\n",
       " '느린 요리기구': 8781,\n",
       " '결혼식 의식': 6719,\n",
       " '사탕 수수': 8030,\n",
       " '개 품종 그룹과 같은 개 물 개': 10411,\n",
       " '키슈': 5088,\n",
       " '고추 게': 7990,\n",
       " '초콜렛': 5062,\n",
       " '아일랜드어 소프트 코팅 wheaten 테리어': 9650,\n",
       " '블레이저': 642,\n",
       " '과즙 기': 9532,\n",
       " '용의': 7647,\n",
       " 'american 와이어 헤어': 7837,\n",
       " '목수': 10837,\n",
       " '굽고있는 음식': 7584,\n",
       " '퀴토 햄트': 6337,\n",
       " '맘모스': 7246,\n",
       " '빵가루 입힌 음식': 5087,\n",
       " '시클로 크로스': 5979,\n",
       " '테이블과': 2502,\n",
       " '식품 법원': 7109,\n",
       " '지질 학적 현상': 1770,\n",
       " '현대 무용': 4019,\n",
       " '내 얼굴': 6133,\n",
       " '화장품': 146,\n",
       " '렌즈': 2910,\n",
       " '에어 스포츠': 2573,\n",
       " 'stock photography 산': 9467,\n",
       " '토목 차량': 7146,\n",
       " '로드 트립': 3751,\n",
       " '전체 아침 식사': 313,\n",
       " '경주 트랙': 4631,\n",
       " 'napoleon cat': 9943,\n",
       " 'soto mie': 4649,\n",
       " '포메 라니아 인 같은 개': 8136,\n",
       " '멸치 식품': 10915,\n",
       " 'maremma sheepdog': 6075,\n",
       " '습기': 8554,\n",
       " '일본 턱': 3336,\n",
       " '필름 느와르': 3748,\n",
       " '스킨 케어': 1130,\n",
       " '야구 방망이': 5901,\n",
       " 'latte': 5077,\n",
       " '나무 숟가락': 8125,\n",
       " '음향 일렉트릭 기타': 6528,\n",
       " '트라이던트': 4916,\n",
       " 'mudhol 하운드': 7943,\n",
       " '스노': 3244,\n",
       " 'moussaka': 3276,\n",
       " '떡볶이': 1257,\n",
       " '수염처럼 포유류': 8174,\n",
       " '흔들 의자': 6441,\n",
       " '자동차 자동차 부품': 7652,\n",
       " '봉제 완구': 2001,\n",
       " '풀 사이즈 자동차': 1015,\n",
       " '록 콘서트': 1336,\n",
       " '컨벤션 센터': 2488,\n",
       " '당근 케이크': 3465,\n",
       " '꽃 차': 7065,\n",
       " 'boerboel': 7833,\n",
       " '조감도': 1540,\n",
       " '차 우게': 8288,\n",
       " '기타': 2169,\n",
       " '전기 파란색': 3170,\n",
       " '스프루스': 1285,\n",
       " '부드러운 쉘 게': 9051,\n",
       " '장난감 블록': 4026,\n",
       " '빛': 260,\n",
       " '안장': 9880,\n",
       " '차량용': 3581,\n",
       " '실내 식물': 6360,\n",
       " '아구 짐': 4365,\n",
       " '목본 식물': 9770,\n",
       " '스페인 요리': 1907,\n",
       " '하이 모자': 6582,\n",
       " '리스 토렛 토우': 7513,\n",
       " 'stock photography 사각형': 9123,\n",
       " '레이크 랜드 테리어': 6574,\n",
       " 'tinto 드 verano': 7378,\n",
       " '비지니스 맨': 3646,\n",
       " '초상화 사진': 610,\n",
       " '연어': 1001,\n",
       " '삼바': 9101,\n",
       " '물고기 연못': 5389,\n",
       " '정원': 587,\n",
       " '시코쿠': 2217,\n",
       " '설렁탕 요리': 6777,\n",
       " '김치 찌개': 688,\n",
       " '담홍색': 3329,\n",
       " '사워 스프': 9213,\n",
       " '폭스 바겐 gli': 9426,\n",
       " '높은 굽 신발': 5584,\n",
       " '목': 11,\n",
       " '호수 지구': 1717,\n",
       " '퐁당': 1018,\n",
       " '가죽': 507,\n",
       " '래브라도 리트리버': 1485,\n",
       " '부다리 찌개': 5597,\n",
       " '썰매 개 같은': 8669,\n",
       " '\\u200b\\u200b해안 및 해양 지형': 10443,\n",
       " '물 본문': 3780,\n",
       " '샴페인 유리 잔': 1821,\n",
       " '비엔나 소시지': 6386,\n",
       " '코스프레': 1507,\n",
       " '집에 감자 튀김': 5438,\n",
       " '인종': 982,\n",
       " '외벽': 10456,\n",
       " 'scotch collie': 8638,\n",
       " '드레스 셔츠': 878,\n",
       " '창 처리': 1311,\n",
       " '얼어 붙은 얼음': 6010,\n",
       " '별장': 1399,\n",
       " '보디 가드': 4952,\n",
       " '스포츠가 아닌 그룹과 같은 개': 2936,\n",
       " '자갈': 9954,\n",
       " '붕대': 1590,\n",
       " '중국 허브 차': 6659,\n",
       " '연안 및 해양 landforms': 10120,\n",
       " '계란 삶은': 5908,\n",
       " '새틴': 1417,\n",
       " '바지': 261,\n",
       " '뼈 자르기': 8508,\n",
       " '카츄아': 7821,\n",
       " '신부 들러리': 1626,\n",
       " '입술 광택': 1135,\n",
       " '생선 조각': 1823,\n",
       " '튀 링겐 소시지': 8826,\n",
       " '자재': 4319,\n",
       " '분홍색': 238,\n",
       " '자동차 오디오': 5092,\n",
       " '공식 거주지': 9791,\n",
       " '중세 시대': 8944,\n",
       " '포르쉐 복스 터': 8560,\n",
       " '새장': 10423,\n",
       " '묘소': 5421,\n",
       " '정체성 문서': 5534,\n",
       " '일년 공장': 10023,\n",
       " '학술 대회': 2721,\n",
       " '버클': 3689,\n",
       " '가자 탕': 2127,\n",
       " '스 피 츠 mittel': 9143,\n",
       " '온기리 요리': 9141,\n",
       " '철사': 5172,\n",
       " '전자 키보드': 4732,\n",
       " '티 파티': 5956,\n",
       " 'bichon frisé 같은 강아지. 그룹': 9605,\n",
       " '감기 컷': 2418,\n",
       " '대학교': 7038,\n",
       " '슈마이': 10773,\n",
       " 'vienna 소시지': 5764,\n",
       " '오버 헤드 프레스': 2577,\n",
       " '가족 차': 2635,\n",
       " 'product': 2031,\n",
       " '일광 장식': 10359,\n",
       " '담 갈비': 6215,\n",
       " '차우 메인': 3608,\n",
       " '라일락': 974,\n",
       " '여행 트레일러': 8998,\n",
       " '머리 억제 장치': 9339,\n",
       " '낚시': 2021,\n",
       " '멀티미디어': 448,\n",
       " '케이크 장식': 726,\n",
       " '울트라 마라톤': 2975,\n",
       " '비둘기와 비둘기': 7810,\n",
       " 'manti': 2843,\n",
       " '국방 국': 8708,\n",
       " '늑대': 9875,\n",
       " '마세라티 지브리': 9562,\n",
       " 'welsh corgi': 7371,\n",
       " '야외 구두': 2646,\n",
       " '자동차 대리점': 3976,\n",
       " '모음곡': 9665,\n",
       " '바이 뮤당': 10913,\n",
       " '휴대폰 케이스': 4756,\n",
       " '상호 작용 구두': 7687,\n",
       " '주황색': 2776,\n",
       " '소형 기기': 4416,\n",
       " '코스 프레': 1855,\n",
       " '탁자': 3235,\n",
       " 'alaunt': 8415,\n",
       " '구호': 10257,\n",
       " '클립 아트 같은 포유류': 10589,\n",
       " '수확': 2056,\n",
       " '테이블': 65,\n",
       " '아비시 니아': 8680,\n",
       " '아이': 31,\n",
       " '오래된 성장 숲': 3431,\n",
       " '치킨 앤 칩스': 5372,\n",
       " '말린 대구': 8732,\n",
       " '식기류 세트': 4706,\n",
       " '안전 벨트': 1570,\n",
       " '카나 번 강아지와 같은 개': 5995,\n",
       " 'bmw': 1709,\n",
       " '자전거 드라이브 트레인 부품': 3555,\n",
       " '어선': 5238,\n",
       " '손가락 요리': 9431,\n",
       " '머리카락을 계층화 된 입술': 7590,\n",
       " '딥 프라이팬': 10427,\n",
       " 'bernese 산 개': 8366,\n",
       " '올빼미': 5634,\n",
       " '금관 악기': 3419,\n",
       " '성능': 456,\n",
       " '고기 찹쌀': 2838,\n",
       " '당근': 10132,\n",
       " '경쟁 행사': 1796,\n",
       " 'bouillabaisse': 3140,\n",
       " '철인 3': 5315,\n",
       " '뷔페': 552,\n",
       " '하늘 사건': 6834,\n",
       " 'pallone': 9230,\n",
       " '기린': 3240,\n",
       " '사슬': 2090,\n",
       " 'kooikerhondje': 6113,\n",
       " '스포츠 유니폼': 1089,\n",
       " '귀 머리 색소 속눈썹': 7433,\n",
       " '클래식 자동차': 3835,\n",
       " '올리브 마요네즈': 9908,\n",
       " '자동차 엔진 부품': 6518,\n",
       " '정물 사진 술': 10404,\n",
       " '꼬치 튀김 요리': 4266,\n",
       " 'bandy': 8274,\n",
       " '서늘함': 10815,\n",
       " '밀짚': 9719,\n",
       " '카이 양': 1826,\n",
       " '수트': 2182,\n",
       " '트롬본 유형': 9098,\n",
       " '동물 피난처': 8835,\n",
       " '캘리포니아 스팽글': 10787,\n",
       " '카리 디안 새우': 3060,\n",
       " '천장 고정 장치': 4366,\n",
       " '목재 바닥': 8487,\n",
       " '장난감 푸들처럼 개': 8357,\n",
       " '잡채': 5000,\n",
       " '칼국수국': 10259,\n",
       " '하이힐': 1329,\n",
       " '패션 디자인': 408,\n",
       " '토루 로쉐': 10868,\n",
       " '계측기': 2437,\n",
       " 'lacustrine 일반': 10191,\n",
       " '슬라이딩 도어': 10358,\n",
       " '깔창': 8221,\n",
       " '볼 \\u200b\\u200b게임': 8327,\n",
       " '메트로폴리탄': 7108,\n",
       " '라이팅': 10313,\n",
       " 'aquifoliaceae': 6216,\n",
       " '플랫 패널 디스플레이': 8341,\n",
       " '세탁실': 4864,\n",
       " '슬픔': 9810,\n",
       " '경찰': 1435,\n",
       " '접시 점심 식사': 478,\n",
       " '일안 리플렉스 카메라': 3887,\n",
       " 'tarn': 10072,\n",
       " '워터 피스': 9511,\n",
       " '경재와 같은 고양이': 9113,\n",
       " 'leaf vegetable': 7471,\n",
       " '스키 폴': 3970,\n",
       " '보트 및 보트 용품 및 용품': 6007,\n",
       " '세라믹 타일': 9399,\n",
       " '배꼽': 5141,\n",
       " '관중': 3565,\n",
       " '장미 꽃가루': 4409,\n",
       " 'tibetan 음식': 3462,\n",
       " '패션 그림': 8532,\n",
       " '지하철역': 4265,\n",
       " '꼬집음': 3795,\n",
       " '신부': 200,\n",
       " '시계탑': 5334,\n",
       " 'poblano': 7665,\n",
       " '웅변술': 7340,\n",
       " '몰타': 665,\n",
       " 'sorbus': 6218,\n",
       " '탄두리 닭': 4801,\n",
       " 'chinese food': 3801,\n",
       " '이탈리아 그레이 하운드와 같은 개': 5612,\n",
       " '토핑': 396,\n",
       " '에메랄드': 4345,\n",
       " '멋지다': 5090,\n",
       " '선': 421,\n",
       " 'zwiebelkuchen': 3122,\n",
       " '필드': 538,\n",
       " '배기 시스템': 4921,\n",
       " 'dog': 6079,\n",
       " '인쇄': 9866,\n",
       " '맞춤형 자동차': 3340,\n",
       " '나비 넥타이': 5707,\n",
       " '중등 학교': 1962,\n",
       " '구운 식품': 1156,\n",
       " '고고학 사이트': 2002,\n",
       " 'schipperke': 8492,\n",
       " '구겨져 조개 굴 홍합과 가리비': 7350,\n",
       " '트랙': 1935,\n",
       " '미니어처 오스트레일리아 양치기': 7335,\n",
       " '퀴 토 햄프': 1702,\n",
       " 'chartreux 같은 고양이': 5288,\n",
       " '포메 라니아': 1477,\n",
       " '워킹화': 6385,\n",
       " '흰 코트': 3291,\n",
       " '날개': 1392,\n",
       " '쿠스 쿠스': 4396,\n",
       " 'pappardelle': 5179,\n",
       " '사진 작가': 10243,\n",
       " 'taste': 6774,\n",
       " '터키 반 등의 고양이': 7777,\n",
       " '바깥 쪽': 9311,\n",
       " 'christmas lights': 3499,\n",
       " '관엽 식물': 3475,\n",
       " '키즈 샌드위치': 8207,\n",
       " '조향 부품': 2462,\n",
       " '이탈리안 음식': 5745,\n",
       " '세미나': 1785,\n",
       " '항공 우주 공학': 1923,\n",
       " '스포츠 경기장': 271,\n",
       " '식물 군락': 3899,\n",
       " 'korokke': 3516,\n",
       " '오토 쿠튀르': 9663,\n",
       " '바나나 빵': 6119,\n",
       " '경로': 701,\n",
       " '쿠바': 10865,\n",
       " '크라운': 8444,\n",
       " '시내 관광 명소': 6028,\n",
       " '크로스 컨트리 달리기': 9947,\n",
       " '콘크리트 다리': 10125,\n",
       " 'coton 드 tulear 같은 개': 7486,\n",
       " '\\u200b\\u200b운동복': 9749,\n",
       " '필름 카메라': 3703,\n",
       " 'charadriiformes': 9060,\n",
       " '컴패니언 개': 10409,\n",
       " '브루 쉐 타': 7653,\n",
       " '고기 찹쌀밥': 6368,\n",
       " '크루저': 4981,\n",
       " '이구아나': 6529,\n",
       " '접합': 7165,\n",
       " '개인 부양 장치': 3303,\n",
       " '무스탕 말': 9882,\n",
       " '푸들 교배 같은 강아지': 9470,\n",
       " '장미 주문 장미': 7018,\n",
       " 'roti canai': 9337,\n",
       " '데이지 가족': 1845,\n",
       " '딸 같은 강아지': 10330,\n",
       " '사카나': 2527,\n",
       " '위로 식품': 10598,\n",
       " '빨간 하늘에서 아침': 4057,\n",
       " '용의 리': 3315,\n",
       " '적합성 및 그림 경쟁': 7878,\n",
       " '클로즈 업': 3131,\n",
       " 'cupcake': 8804,\n",
       " '드리프트': 7076,\n",
       " '관절': 44,\n",
       " 'heteromeles': 4849,\n",
       " '아시아 스프': 1532,\n",
       " 'sobrassada': 7040,\n",
       " '자동차 등록 번호판': 5408,\n",
       " '감정': 286,\n",
       " '인물': 7863,\n",
       " 'almshouse': 8541,\n",
       " '파두다 아스 투 리아나': 9006,\n",
       " '열대 식물': 8840,\n",
       " '현관': 3596,\n",
       " '연회': 2565,\n",
       " '허리': 189,\n",
       " 'couch': 8959,\n",
       " '미술 갤러리': 4668,\n",
       " '두더지': 9929,\n",
       " '나무 마루': 2939,\n",
       " '마구간': 2064,\n",
       " '중심': 855,\n",
       " '레인': 4339,\n",
       " '시원한 운동복': 6550,\n",
       " '오래된 영어 테리어': 8688,\n",
       " '부즈': 9989,\n",
       " '돌고래 산호': 6961,\n",
       " '홍합': 956,\n",
       " '나무와 같은 개': 9969,\n",
       " '데친 계란': 6413,\n",
       " '허구의 성격': 10159,\n",
       " '밥캣': 9988,\n",
       " 'koenigsegg ccx': 6727,\n",
       " '브라운 빵': 8260,\n",
       " '사무실 의자': 4078,\n",
       " '멋진 제품': 6043,\n",
       " '근육': 139,\n",
       " '반찬류': 5039,\n",
       " '선글라스': 298,\n",
       " '복합 활': 9962,\n",
       " 'eye liner': 9856,\n",
       " '포카 치아': 8211,\n",
       " 'byzantine 아키텍처': 4775,\n",
       " '까만 벨트': 10499,\n",
       " '종이 제품': 1028,\n",
       " '얼버 고양이': 9499,\n",
       " '채소 버거': 10373,\n",
       " '눈 그림자': 3715,\n",
       " '견목': 5512,\n",
       " '북극곰': 5044,\n",
       " '머리 부속품': 3840,\n",
       " '음식 그룹': 5045,\n",
       " '공예': 2786,\n",
       " '유기물': 8138,\n",
       " '한국 진도견': 10052,\n",
       " '웨일스 어 sheepdog': 7093,\n",
       " '튀김 튀김': 10661,\n",
       " '초목': 9461,\n",
       " '달콤한 옥수수': 7798,\n",
       " '글씨': 2569,\n",
       " '버킷': 10861,\n",
       " '비디오 게임 액세서리': 8723,\n",
       " '왜건': 8890,\n",
       " '머리 액세서리': 844,\n",
       " '수목': 6104,\n",
       " '유네스코 세계 유산 사이트': 6862,\n",
       " '네덜란드어 팬케이크': 10273,\n",
       " '정글 모드': 7988,\n",
       " '직업': 898,\n",
       " '황소 테리어': 6271,\n",
       " '숙주': 10274,\n",
       " 'sakhalin husky': 6513,\n",
       " '라사 사 apso': 8579,\n",
       " '조리기구 및 빵집': 2287,\n",
       " '휴대 전화 액세서리': 1161,\n",
       " '공공 유틸리티': 1563,\n",
       " '야키니쿠': 474,\n",
       " '전': 1651,\n",
       " '무심한 킹 찰스 발바리': 4005,\n",
       " '시라 타키 소바': 5783,\n",
       " '버팔로 햄버거': 1980,\n",
       " '키퍼': 6179,\n",
       " '서퍼 머리': 4049,\n",
       " '시골': 5384,\n",
       " '팬케이크': 3076,\n",
       " '머리 겹친 머리카락': 7401,\n",
       " '쇠고기 음식': 7180,\n",
       " '프라이팬': 3116,\n",
       " '음악 개최지': 4946,\n",
       " '머리 가제': 4056,\n",
       " '접시 점심': 788,\n",
       " '절지 동물': 4964,\n",
       " '중저음': 7485,\n",
       " '구식': 5832,\n",
       " '야자 나무': 3088,\n",
       " '트럭 베드 부품': 10441,\n",
       " '민물 게': 9052,\n",
       " '베이스 기타': 2740,\n",
       " '공정': 4967,\n",
       " '쇠고기 쇠고기': 10335,\n",
       " '당당한 집': 4966,\n",
       " '전투 스포츠': 3105,\n",
       " '라스베리': 7758,\n",
       " '오믈렛': 3298,\n",
       " '푸들과 같은 개': 5831,\n",
       " '빨간 피망': 8455,\n",
       " '포메 라니아 인': 1803,\n",
       " '벵골': 2386,\n",
       " '역사': 1562,\n",
       " '사각 크림': 9083,\n",
       " 'batchoy': 1738,\n",
       " '스틱 및 볼 게임': 10157,\n",
       " 'masala chai': 7780,\n",
       " '수하물 및 가방': 1271,\n",
       " '짠 직물': 10828,\n",
       " '휘핑 크림': 329,\n",
       " '포유류 같은 주둥이': 4647,\n",
       " '소파': 543,\n",
       " '장대': 5766,\n",
       " '상업용 차량': 2603,\n",
       " '조류 이동': 10460,\n",
       " '아일랜드 소프트 코트 밀 테리어': 10791,\n",
       " '머리 억제': 4129,\n",
       " '취미': 9583,\n",
       " 'indomie': 4254,\n",
       " '과일 파이': 8106,\n",
       " '스카치 콜리': 3173,\n",
       " '디스크 자키': 5823,\n",
       " '츠 쿠 다니': 10076,\n",
       " 'suit': 10439,\n",
       " 'spaniel': 7710,\n",
       " '개 품종과 같은 개.': 6942,\n",
       " 'morkie': 924,\n",
       " '천장 고정물': 5508,\n",
       " '몬타시오': 8035,\n",
       " 'vitis': 3154,\n",
       " 'saimin 식품': 5680,\n",
       " '속바지': 6537,\n",
       " '팀 스포츠': 484,\n",
       " '가득 차있는 아침': 3504,\n",
       " '냄비 bagnat': 3580,\n",
       " '동물 이전': 9544,\n",
       " '보트 및 보트 장비 및 소모품': 6008,\n",
       " '킹 목사': 6302,\n",
       " '포유 동물과 같은 강아지': 8243,\n",
       " '로비 의자': 6649,\n",
       " '머리카락 종범': 9354,\n",
       " '결혼 예식 식료품': 9850,\n",
       " '굴 요리법': 8631,\n",
       " '패턴': 77,\n",
       " '레이어드 머리': 10907,\n",
       " '유채': 5523,\n",
       " '야생 동물': 1367,\n",
       " '한국 타코': 4925,\n",
       " '서쪽 고지 흰색 테리어와 같은 개': 10687,\n",
       " '피트니스 및 도형 경쟁': 7850,\n",
       " '라 고트 로마 마놀로': 10454,\n",
       " '살라미': 2094,\n",
       " '도어': 3262,\n",
       " '경기 이벤트': 2363,\n",
       " '슈라 스쿠 음식': 6817,\n",
       " '장면': 647,\n",
       " '쿠 스 쿠 스': 10625,\n",
       " '오케스트라': 3482,\n",
       " '소아 아얌': 6645,\n",
       " '새벽 국수': 10765,\n",
       " '시보레 순항': 10670,\n",
       " '스토브': 8443,\n",
       " '어쿠스틱 기타': 3508,\n",
       " '대초원 같은 개': 9734,\n",
       " '래그 돌과 같은 고양이': 6906,\n",
       " '보육원': 3501,\n",
       " '파전': 3973,\n",
       " '미니 밴': 1459,\n",
       " '모래': 326,\n",
       " '카디건 welsh corgi 같은 개': 9772,\n",
       " '앞으로': 2095,\n",
       " 'businessperson': 3252,\n",
       " '수영복 밑': 6123,\n",
       " '타파스': 2883,\n",
       " '예배': 5630,\n",
       " '벽화': 1077,\n",
       " '조인트': 297,\n",
       " '하천의 하천': 6031,\n",
       " '혼례': 7255,\n",
       " 'olde 영어 bulldogge 같은 개': 5742,\n",
       " '경목': 2240,\n",
       " '휴대 통신 기기': 1361,\n",
       " '오디오 장비': 1080,\n",
       " '발처럼 고양이': 8168,\n",
       " '그림자와 같은 고양이': 9087,\n",
       " '맥주 칵테일': 1492,\n",
       " '중세': 8126,\n",
       " '고양이 주둥이': 6749,\n",
       " '생선 카레': 10047,\n",
       " 'abbey': 10646,\n",
       " '분필': 4811,\n",
       " '일반적인 애완용 앵무새 앵무새': 8162,\n",
       " '기타리스트': 1701,\n",
       " '잔디 가족': 533,\n",
       " '바이올리스트': 8655,\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'art': <gensim.models.keyedvectors.Vocab at 0x193346d8>,\n",
       " 'artwork': <gensim.models.keyedvectors.Vocab at 0x19362a20>,\n",
       " 'coffee': <gensim.models.keyedvectors.Vocab at 0x1934aa90>,\n",
       " 'dailypic': <gensim.models.keyedvectors.Vocab at 0x19362208>,\n",
       " 'dog': <gensim.models.keyedvectors.Vocab at 0x1934af28>,\n",
       " 'drawing': <gensim.models.keyedvectors.Vocab at 0x19334c88>,\n",
       " 'fff': <gensim.models.keyedvectors.Vocab at 0x19334080>,\n",
       " 'flower': <gensim.models.keyedvectors.Vocab at 0x1931e7f0>,\n",
       " 'food': <gensim.models.keyedvectors.Vocab at 0x19309c50>,\n",
       " 'foodie': <gensim.models.keyedvectors.Vocab at 0x1931ed30>,\n",
       " 'girl': <gensim.models.keyedvectors.Vocab at 0x1934a710>,\n",
       " 'got': <gensim.models.keyedvectors.Vocab at 0x19334860>,\n",
       " 'ig': <gensim.models.keyedvectors.Vocab at 0x19334b70>,\n",
       " 'illust': <gensim.models.keyedvectors.Vocab at 0x19362b38>,\n",
       " 'illustration': <gensim.models.keyedvectors.Vocab at 0x1934ad30>,\n",
       " 'instadaily': <gensim.models.keyedvectors.Vocab at 0x1934acc0>,\n",
       " 'instagood': <gensim.models.keyedvectors.Vocab at 0x19334128>,\n",
       " 'instalike': <gensim.models.keyedvectors.Vocab at 0x1931ea58>,\n",
       " 'instasize': <gensim.models.keyedvectors.Vocab at 0x1931e0f0>,\n",
       " 'jmt': <gensim.models.keyedvectors.Vocab at 0x1931e1d0>,\n",
       " 'korean': <gensim.models.keyedvectors.Vocab at 0x1934a748>,\n",
       " 'koreanfood': <gensim.models.keyedvectors.Vocab at 0x1931e080>,\n",
       " 'kpop': <gensim.models.keyedvectors.Vocab at 0x1934af98>,\n",
       " 'like': <gensim.models.keyedvectors.Vocab at 0x19309e10>,\n",
       " 'likeforfollow': <gensim.models.keyedvectors.Vocab at 0x19362d68>,\n",
       " 'likeforlikes': <gensim.models.keyedvectors.Vocab at 0x1931e8d0>,\n",
       " 'love': <gensim.models.keyedvectors.Vocab at 0x1934ae48>,\n",
       " 'me': <gensim.models.keyedvectors.Vocab at 0x193628d0>,\n",
       " 'outfit': <gensim.models.keyedvectors.Vocab at 0x19334828>,\n",
       " 'photo': <gensim.models.keyedvectors.Vocab at 0x19334f28>,\n",
       " 'photography': <gensim.models.keyedvectors.Vocab at 0x1934a7b8>,\n",
       " 'portrait': <gensim.models.keyedvectors.Vocab at 0x17c5c278>,\n",
       " 'puppy': <gensim.models.keyedvectors.Vocab at 0x193099b0>,\n",
       " 'ulzzang': <gensim.models.keyedvectors.Vocab at 0x193621d0>,\n",
       " 'workout': <gensim.models.keyedvectors.Vocab at 0x19362ba8>,\n",
       " '가족': <gensim.models.keyedvectors.Vocab at 0x19334438>,\n",
       " '가족여행': <gensim.models.keyedvectors.Vocab at 0x17c5c198>,\n",
       " '간식': <gensim.models.keyedvectors.Vocab at 0x1934ae10>,\n",
       " '감성': <gensim.models.keyedvectors.Vocab at 0x19362e48>,\n",
       " '감성사진': <gensim.models.keyedvectors.Vocab at 0x19334908>,\n",
       " '강남클럽': <gensim.models.keyedvectors.Vocab at 0x19309e48>,\n",
       " '거울샷': <gensim.models.keyedvectors.Vocab at 0x19309f28>,\n",
       " '겨울': <gensim.models.keyedvectors.Vocab at 0x1934a828>,\n",
       " '겨울네일': <gensim.models.keyedvectors.Vocab at 0x1931eac8>,\n",
       " '겨울여행': <gensim.models.keyedvectors.Vocab at 0x1931e908>,\n",
       " '겨울코디': <gensim.models.keyedvectors.Vocab at 0x1934aef0>,\n",
       " '결혼': <gensim.models.keyedvectors.Vocab at 0x19309f98>,\n",
       " '결혼식': <gensim.models.keyedvectors.Vocab at 0x1931eb00>,\n",
       " '결혼준비': <gensim.models.keyedvectors.Vocab at 0x19309ac8>,\n",
       " '고': <gensim.models.keyedvectors.Vocab at 0x193095f8>,\n",
       " '고딩': <gensim.models.keyedvectors.Vocab at 0x19309c18>,\n",
       " '고마워': <gensim.models.keyedvectors.Vocab at 0x19334d68>,\n",
       " '공감': <gensim.models.keyedvectors.Vocab at 0x1934abe0>,\n",
       " '공부': <gensim.models.keyedvectors.Vocab at 0x19334a20>,\n",
       " '공부스타그램': <gensim.models.keyedvectors.Vocab at 0x19362a58>,\n",
       " '공스타그램': <gensim.models.keyedvectors.Vocab at 0x19334240>,\n",
       " '굿밤': <gensim.models.keyedvectors.Vocab at 0x1934a5f8>,\n",
       " '귀걸이': <gensim.models.keyedvectors.Vocab at 0x1931e7b8>,\n",
       " '귀여워': <gensim.models.keyedvectors.Vocab at 0x192c9c50>,\n",
       " '귀요미': <gensim.models.keyedvectors.Vocab at 0x19334198>,\n",
       " '그림': <gensim.models.keyedvectors.Vocab at 0x1931e5c0>,\n",
       " '글귀': <gensim.models.keyedvectors.Vocab at 0x193627b8>,\n",
       " '글스타그램': <gensim.models.keyedvectors.Vocab at 0x19334ba8>,\n",
       " '기록': <gensim.models.keyedvectors.Vocab at 0x193626a0>,\n",
       " '꽃다발': <gensim.models.keyedvectors.Vocab at 0x19362278>,\n",
       " '꽃스타그램': <gensim.models.keyedvectors.Vocab at 0x1931e9b0>,\n",
       " '남매맘': <gensim.models.keyedvectors.Vocab at 0x1934a8d0>,\n",
       " '냥스타그램': <gensim.models.keyedvectors.Vocab at 0x19309cf8>,\n",
       " '네일': <gensim.models.keyedvectors.Vocab at 0x1934aac8>,\n",
       " '네일아트': <gensim.models.keyedvectors.Vocab at 0x1931e320>,\n",
       " '눈': <gensim.models.keyedvectors.Vocab at 0x1931e240>,\n",
       " '눈바디': <gensim.models.keyedvectors.Vocab at 0x1934a588>,\n",
       " '다이렉트': <gensim.models.keyedvectors.Vocab at 0x19362860>,\n",
       " '다이어터': <gensim.models.keyedvectors.Vocab at 0x1931e978>,\n",
       " '다이어트': <gensim.models.keyedvectors.Vocab at 0x1934a4a8>,\n",
       " '단발': <gensim.models.keyedvectors.Vocab at 0x1931ec18>,\n",
       " '닭띠맘': <gensim.models.keyedvectors.Vocab at 0x19309c88>,\n",
       " '닭띠아기': <gensim.models.keyedvectors.Vocab at 0x19334c18>,\n",
       " '대학생': <gensim.models.keyedvectors.Vocab at 0x19334588>,\n",
       " '댓글': <gensim.models.keyedvectors.Vocab at 0x19309710>,\n",
       " '댕댕이': <gensim.models.keyedvectors.Vocab at 0x19362668>,\n",
       " '데이트': <gensim.models.keyedvectors.Vocab at 0x1934a128>,\n",
       " '데이트룩': <gensim.models.keyedvectors.Vocab at 0x19309898>,\n",
       " '데일리': <gensim.models.keyedvectors.Vocab at 0x19309828>,\n",
       " '데일리그램': <gensim.models.keyedvectors.Vocab at 0x19362c50>,\n",
       " '데일리룩': <gensim.models.keyedvectors.Vocab at 0x19309a20>,\n",
       " '도치맘': <gensim.models.keyedvectors.Vocab at 0x1934a518>,\n",
       " '등원룩': <gensim.models.keyedvectors.Vocab at 0x19362c18>,\n",
       " '디저트': <gensim.models.keyedvectors.Vocab at 0x1934a7f0>,\n",
       " '딸기': <gensim.models.keyedvectors.Vocab at 0x19309b70>,\n",
       " '딸맘': <gensim.models.keyedvectors.Vocab at 0x1931ec50>,\n",
       " '딸바보': <gensim.models.keyedvectors.Vocab at 0x193626d8>,\n",
       " '딸스타그램': <gensim.models.keyedvectors.Vocab at 0x1900c940>,\n",
       " '럽스타그램': <gensim.models.keyedvectors.Vocab at 0x193623c8>,\n",
       " '마카롱': <gensim.models.keyedvectors.Vocab at 0x193348d0>,\n",
       " '마켓': <gensim.models.keyedvectors.Vocab at 0x19334e10>,\n",
       " '맘스타그램': <gensim.models.keyedvectors.Vocab at 0x1934a978>,\n",
       " '맛스타그램': <gensim.models.keyedvectors.Vocab at 0x17c5cf60>,\n",
       " '맛있다그램': <gensim.models.keyedvectors.Vocab at 0x1934a2b0>,\n",
       " '맛집': <gensim.models.keyedvectors.Vocab at 0x1934ad68>,\n",
       " '맞팔': <gensim.models.keyedvectors.Vocab at 0x1931e0b8>,\n",
       " '맞팔댓글': <gensim.models.keyedvectors.Vocab at 0x1931eda0>,\n",
       " '맞팔선팔': <gensim.models.keyedvectors.Vocab at 0x19334c50>,\n",
       " '맞팔환영': <gensim.models.keyedvectors.Vocab at 0x1931e780>,\n",
       " '맥주': <gensim.models.keyedvectors.Vocab at 0x193349e8>,\n",
       " '먹방': <gensim.models.keyedvectors.Vocab at 0x19309908>,\n",
       " '먹부림': <gensim.models.keyedvectors.Vocab at 0x1934a208>,\n",
       " '먹스타그램': <gensim.models.keyedvectors.Vocab at 0x1931e9e8>,\n",
       " '멍스타그램': <gensim.models.keyedvectors.Vocab at 0x19334668>,\n",
       " '메이크업': <gensim.models.keyedvectors.Vocab at 0x19362198>,\n",
       " '모델': <gensim.models.keyedvectors.Vocab at 0x1934a908>,\n",
       " '몸매': <gensim.models.keyedvectors.Vocab at 0x19362160>,\n",
       " '몸스타그램': <gensim.models.keyedvectors.Vocab at 0x1931ef60>,\n",
       " '바다': <gensim.models.keyedvectors.Vocab at 0x17cc9470>,\n",
       " '바디프로필': <gensim.models.keyedvectors.Vocab at 0x1934a6d8>,\n",
       " '반려견': <gensim.models.keyedvectors.Vocab at 0x1931e550>,\n",
       " '반려묘': <gensim.models.keyedvectors.Vocab at 0x1931e358>,\n",
       " '방탄소년단': <gensim.models.keyedvectors.Vocab at 0x1931edd8>,\n",
       " '베이비스타그램': <gensim.models.keyedvectors.Vocab at 0x1931ef98>,\n",
       " '부부스타그램': <gensim.models.keyedvectors.Vocab at 0x19362d30>,\n",
       " '북스타그램': <gensim.models.keyedvectors.Vocab at 0x19334a58>,\n",
       " '분위기': <gensim.models.keyedvectors.Vocab at 0x19334e48>,\n",
       " '불토': <gensim.models.keyedvectors.Vocab at 0x1931e940>,\n",
       " '뷰티스타그램': <gensim.models.keyedvectors.Vocab at 0x1931efd0>,\n",
       " '블로그마켓': <gensim.models.keyedvectors.Vocab at 0x19362b70>,\n",
       " '비키니': <gensim.models.keyedvectors.Vocab at 0x1931e2e8>,\n",
       " '사랑해': <gensim.models.keyedvectors.Vocab at 0x1931e4a8>,\n",
       " '사진스타그램': <gensim.models.keyedvectors.Vocab at 0x193627f0>,\n",
       " '새댁스타그램': <gensim.models.keyedvectors.Vocab at 0x17c5c588>,\n",
       " '새댁일상': <gensim.models.keyedvectors.Vocab at 0x19309cc0>,\n",
       " '새벽': <gensim.models.keyedvectors.Vocab at 0x19309940>,\n",
       " '생일': <gensim.models.keyedvectors.Vocab at 0x193097b8>,\n",
       " '생후': <gensim.models.keyedvectors.Vocab at 0x1931e470>,\n",
       " '선물': <gensim.models.keyedvectors.Vocab at 0x1931e588>,\n",
       " '선팔하면맞팔': <gensim.models.keyedvectors.Vocab at 0x19334f60>,\n",
       " '선팔환영': <gensim.models.keyedvectors.Vocab at 0x19334cf8>,\n",
       " '세젤귀': <gensim.models.keyedvectors.Vocab at 0x19309668>,\n",
       " '세젤예': <gensim.models.keyedvectors.Vocab at 0x1931ea90>,\n",
       " '셀기꾼': <gensim.models.keyedvectors.Vocab at 0x1934a470>,\n",
       " '셀스타': <gensim.models.keyedvectors.Vocab at 0x19334be0>,\n",
       " '셀스타그램': <gensim.models.keyedvectors.Vocab at 0x19362128>,\n",
       " '셀카': <gensim.models.keyedvectors.Vocab at 0x1931ecf8>,\n",
       " '셀카그램': <gensim.models.keyedvectors.Vocab at 0x19309b00>,\n",
       " '셀피': <gensim.models.keyedvectors.Vocab at 0x19334208>,\n",
       " '셀피그램': <gensim.models.keyedvectors.Vocab at 0x1934a438>,\n",
       " '소통': <gensim.models.keyedvectors.Vocab at 0x1934a780>,\n",
       " '소통환영': <gensim.models.keyedvectors.Vocab at 0x1934aa58>,\n",
       " '소확행': <gensim.models.keyedvectors.Vocab at 0x19362ac8>,\n",
       " '송년회': <gensim.models.keyedvectors.Vocab at 0x193344e0>,\n",
       " '쇼핑': <gensim.models.keyedvectors.Vocab at 0x19334dd8>,\n",
       " '쇼핑몰': <gensim.models.keyedvectors.Vocab at 0x1931e390>,\n",
       " '술스타그램': <gensim.models.keyedvectors.Vocab at 0x1931eef0>,\n",
       " '스냅': <gensim.models.keyedvectors.Vocab at 0x193624e0>,\n",
       " '스냅사진': <gensim.models.keyedvectors.Vocab at 0x193347f0>,\n",
       " '스타일': <gensim.models.keyedvectors.Vocab at 0x1931e898>,\n",
       " '신사': <gensim.models.keyedvectors.Vocab at 0x191f19b0>,\n",
       " '신상': <gensim.models.keyedvectors.Vocab at 0x19309978>,\n",
       " '신혼부부': <gensim.models.keyedvectors.Vocab at 0x1931e128>,\n",
       " '신혼스타그램': <gensim.models.keyedvectors.Vocab at 0x1931e278>,\n",
       " '신혼집': <gensim.models.keyedvectors.Vocab at 0x193625c0>,\n",
       " '신혼집인테리어': <gensim.models.keyedvectors.Vocab at 0x1931eba8>,\n",
       " '아기모델': <gensim.models.keyedvectors.Vocab at 0x1934a198>,\n",
       " '아기스타그램': <gensim.models.keyedvectors.Vocab at 0x1934a358>,\n",
       " '아기옷': <gensim.models.keyedvectors.Vocab at 0x19362588>,\n",
       " '아들맘': <gensim.models.keyedvectors.Vocab at 0x1931ef28>,\n",
       " '아들바보': <gensim.models.keyedvectors.Vocab at 0x19309550>,\n",
       " '아들스타그램': <gensim.models.keyedvectors.Vocab at 0x19362cf8>,\n",
       " '아웃핏': <gensim.models.keyedvectors.Vocab at 0x19362748>,\n",
       " '아이폰': <gensim.models.keyedvectors.Vocab at 0x1934a940>,\n",
       " '아지트샵': <gensim.models.keyedvectors.Vocab at 0x1931e518>,\n",
       " '아침': <gensim.models.keyedvectors.Vocab at 0x19334978>,\n",
       " '안녕': <gensim.models.keyedvectors.Vocab at 0x1934a2e8>,\n",
       " '애스타그램': <gensim.models.keyedvectors.Vocab at 0x19362518>,\n",
       " '야식': <gensim.models.keyedvectors.Vocab at 0x19334710>,\n",
       " '얼스타그램': <gensim.models.keyedvectors.Vocab at 0x19334550>,\n",
       " '여행사진': <gensim.models.keyedvectors.Vocab at 0x193342b0>,\n",
       " '여행스타그램': <gensim.models.keyedvectors.Vocab at 0x1934a278>,\n",
       " '여행에미치다': <gensim.models.keyedvectors.Vocab at 0x19334d30>,\n",
       " '연말': <gensim.models.keyedvectors.Vocab at 0x19309a90>,\n",
       " '연말룩': <gensim.models.keyedvectors.Vocab at 0x1931ed68>,\n",
       " '연말파티': <gensim.models.keyedvectors.Vocab at 0x193345f8>,\n",
       " '예비맘': <gensim.models.keyedvectors.Vocab at 0x193344a8>,\n",
       " '예비신부': <gensim.models.keyedvectors.Vocab at 0x1934afd0>,\n",
       " '예쁜아기': <gensim.models.keyedvectors.Vocab at 0x193622e8>,\n",
       " '예신': <gensim.models.keyedvectors.Vocab at 0x19334630>,\n",
       " '오늘': <gensim.models.keyedvectors.Vocab at 0x1934ae80>,\n",
       " '오늘의훈남': <gensim.models.keyedvectors.Vocab at 0x19334390>,\n",
       " '오늘의훈녀': <gensim.models.keyedvectors.Vocab at 0x19362cc0>,\n",
       " '오오티디': <gensim.models.keyedvectors.Vocab at 0x193095c0>,\n",
       " '온더테이블': <gensim.models.keyedvectors.Vocab at 0x19362e10>,\n",
       " '옷스타그램': <gensim.models.keyedvectors.Vocab at 0x1931e668>,\n",
       " '와인': <gensim.models.keyedvectors.Vocab at 0x19362240>,\n",
       " '요리스타그램': <gensim.models.keyedvectors.Vocab at 0x19362438>,\n",
       " '운동스타그램': <gensim.models.keyedvectors.Vocab at 0x19362908>,\n",
       " '운동하는남자': <gensim.models.keyedvectors.Vocab at 0x1934a390>,\n",
       " '운동하는여자': <gensim.models.keyedvectors.Vocab at 0x193620f0>,\n",
       " '워너원': <gensim.models.keyedvectors.Vocab at 0x1931ecc0>,\n",
       " '워킹맘': <gensim.models.keyedvectors.Vocab at 0x193098d0>,\n",
       " '원데이클래스': <gensim.models.keyedvectors.Vocab at 0x1931e630>,\n",
       " '원피스': <gensim.models.keyedvectors.Vocab at 0x1931ee80>,\n",
       " '웨딩': <gensim.models.keyedvectors.Vocab at 0x19362390>,\n",
       " '웨딩드레스': <gensim.models.keyedvectors.Vocab at 0x19362a90>,\n",
       " '웨딩촬영': <gensim.models.keyedvectors.Vocab at 0x1934a0b8>,\n",
       " '웨이트': <gensim.models.keyedvectors.Vocab at 0x19334da0>,\n",
       " '육아': <gensim.models.keyedvectors.Vocab at 0x1931e198>,\n",
       " '육아그램': <gensim.models.keyedvectors.Vocab at 0x1934a630>,\n",
       " '육아기록': <gensim.models.keyedvectors.Vocab at 0x19334ac8>,\n",
       " '육아맘': <gensim.models.keyedvectors.Vocab at 0x19362358>,\n",
       " '육아맘일상': <gensim.models.keyedvectors.Vocab at 0x19362550>,\n",
       " '육아소통': <gensim.models.keyedvectors.Vocab at 0x19334320>,\n",
       " '육아스타그램': <gensim.models.keyedvectors.Vocab at 0x193340b8>,\n",
       " '육아일기': <gensim.models.keyedvectors.Vocab at 0x1934a668>,\n",
       " '육아일상': <gensim.models.keyedvectors.Vocab at 0x193625f8>,\n",
       " '육아템': <gensim.models.keyedvectors.Vocab at 0x1931e5f8>,\n",
       " '음식': <gensim.models.keyedvectors.Vocab at 0x1931ea20>,\n",
       " '인물사진': <gensim.models.keyedvectors.Vocab at 0x19309ef0>,\n",
       " '인생사진': <gensim.models.keyedvectors.Vocab at 0x1934aa20>,\n",
       " '인스타그램': <gensim.models.keyedvectors.Vocab at 0x1934a9b0>,\n",
       " '인스타데일리': <gensim.models.keyedvectors.Vocab at 0x19362be0>,\n",
       " '인스타베이비': <gensim.models.keyedvectors.Vocab at 0x19334f98>,\n",
       " '인스타푸드': <gensim.models.keyedvectors.Vocab at 0x19334518>,\n",
       " '인친': <gensim.models.keyedvectors.Vocab at 0x19309dd8>,\n",
       " '인친해요': <gensim.models.keyedvectors.Vocab at 0x1934a898>,\n",
       " '인친환영': <gensim.models.keyedvectors.Vocab at 0x1931e3c8>,\n",
       " '인테리어': <gensim.models.keyedvectors.Vocab at 0x19362630>,\n",
       " '일러스트': <gensim.models.keyedvectors.Vocab at 0x19334160>,\n",
       " '일반인모델': <gensim.models.keyedvectors.Vocab at 0x193097f0>,\n",
       " '일본여행': <gensim.models.keyedvectors.Vocab at 0x1934ab38>,\n",
       " '일상': <gensim.models.keyedvectors.Vocab at 0x19334470>,\n",
       " '일상그램': <gensim.models.keyedvectors.Vocab at 0x193624a8>,\n",
       " '일상기록': <gensim.models.keyedvectors.Vocab at 0x1934a1d0>,\n",
       " '일상스타그램': <gensim.models.keyedvectors.Vocab at 0x19362048>,\n",
       " '일요일': <gensim.models.keyedvectors.Vocab at 0x1934a160>,\n",
       " '임산부': <gensim.models.keyedvectors.Vocab at 0x19334400>,\n",
       " '임신': <gensim.models.keyedvectors.Vocab at 0x1931e4e0>,\n",
       " '저녁': <gensim.models.keyedvectors.Vocab at 0x1931e6a0>,\n",
       " '전신샷': <gensim.models.keyedvectors.Vocab at 0x19362780>,\n",
       " '젊줌마': <gensim.models.keyedvectors.Vocab at 0x193620b8>,\n",
       " '점심': <gensim.models.keyedvectors.Vocab at 0x1934a080>,\n",
       " '젤네일': <gensim.models.keyedvectors.Vocab at 0x19334b00>,\n",
       " '존맛': <gensim.models.keyedvectors.Vocab at 0x19334048>,\n",
       " '존맛탱': <gensim.models.keyedvectors.Vocab at 0x1934add8>,\n",
       " '좋반': <gensim.models.keyedvectors.Vocab at 0x1934acf8>,\n",
       " '좋아요': <gensim.models.keyedvectors.Vocab at 0x19334cc0>,\n",
       " '좋아요그램': <gensim.models.keyedvectors.Vocab at 0x193629e8>,\n",
       " '좋아요반사': <gensim.models.keyedvectors.Vocab at 0x193629b0>,\n",
       " '좋튀': <gensim.models.keyedvectors.Vocab at 0x193340f0>,\n",
       " '주말': <gensim.models.keyedvectors.Vocab at 0x1934a550>,\n",
       " '주말끝': <gensim.models.keyedvectors.Vocab at 0x193347b8>,\n",
       " '주부': <gensim.models.keyedvectors.Vocab at 0x19309d30>,\n",
       " '주부스타그램': <gensim.models.keyedvectors.Vocab at 0x193346a0>,\n",
       " '줌마그램': <gensim.models.keyedvectors.Vocab at 0x19334fd0>,\n",
       " '줌마스타그램': <gensim.models.keyedvectors.Vocab at 0x1931e828>,\n",
       " '중': <gensim.models.keyedvectors.Vocab at 0x193096a0>,\n",
       " '직딩': <gensim.models.keyedvectors.Vocab at 0x19309860>,\n",
       " '직장인': <gensim.models.keyedvectors.Vocab at 0x19334a90>,\n",
       " '직장인스타그램': <gensim.models.keyedvectors.Vocab at 0x19362940>,\n",
       " '집꾸미기': <gensim.models.keyedvectors.Vocab at 0x19309588>,\n",
       " '집밥': <gensim.models.keyedvectors.Vocab at 0x1934ada0>,\n",
       " '집밥스타그램': <gensim.models.keyedvectors.Vocab at 0x193341d0>,\n",
       " '집스타그램': <gensim.models.keyedvectors.Vocab at 0x193345c0>,\n",
       " '책스타그램': <gensim.models.keyedvectors.Vocab at 0x1934a400>,\n",
       " '첫줄': <gensim.models.keyedvectors.Vocab at 0x1931eb70>,\n",
       " '초보맘': <gensim.models.keyedvectors.Vocab at 0x1934ac50>,\n",
       " '촬영': <gensim.models.keyedvectors.Vocab at 0x19334358>,\n",
       " '출근': <gensim.models.keyedvectors.Vocab at 0x19334780>,\n",
       " '취미': <gensim.models.keyedvectors.Vocab at 0x193622b0>,\n",
       " '친구': <gensim.models.keyedvectors.Vocab at 0x19334278>,\n",
       " '친스타그램': <gensim.models.keyedvectors.Vocab at 0x1931ec88>,\n",
       " '카페': <gensim.models.keyedvectors.Vocab at 0x1931ebe0>,\n",
       " '카페스타그램': <gensim.models.keyedvectors.Vocab at 0x1934ab00>,\n",
       " '카페투어': <gensim.models.keyedvectors.Vocab at 0x1934ac18>,\n",
       " '커플': <gensim.models.keyedvectors.Vocab at 0x1934a6a0>,\n",
       " '커플룩': <gensim.models.keyedvectors.Vocab at 0x1934a860>,\n",
       " '커피': <gensim.models.keyedvectors.Vocab at 0x1931e2b0>,\n",
       " '케이크': <gensim.models.keyedvectors.Vocab at 0x1934ac88>,\n",
       " '코덕': <gensim.models.keyedvectors.Vocab at 0x1931e400>,\n",
       " '코디': <gensim.models.keyedvectors.Vocab at 0x193342e8>,\n",
       " '쿡스타그램': <gensim.models.keyedvectors.Vocab at 0x1934a3c8>,\n",
       " '크리스마스': <gensim.models.keyedvectors.Vocab at 0x19334748>,\n",
       " '크리스마스네일': <gensim.models.keyedvectors.Vocab at 0x1934af60>,\n",
       " '크리스마스선물': <gensim.models.keyedvectors.Vocab at 0x19334898>,\n",
       " '타투': <gensim.models.keyedvectors.Vocab at 0x19362470>,\n",
       " '토요일': <gensim.models.keyedvectors.Vocab at 0x19362c88>,\n",
       " '퇴근': <gensim.models.keyedvectors.Vocab at 0x1931e160>,\n",
       " '파티': <gensim.models.keyedvectors.Vocab at 0x1931e6d8>,\n",
       " '팔로우': <gensim.models.keyedvectors.Vocab at 0x193349b0>,\n",
       " '팔로우미': <gensim.models.keyedvectors.Vocab at 0x19334ef0>,\n",
       " '팔로우환영': <gensim.models.keyedvectors.Vocab at 0x19309eb8>,\n",
       " '팔로워': <gensim.models.keyedvectors.Vocab at 0x19362320>,\n",
       " '팔로잉': <gensim.models.keyedvectors.Vocab at 0x1934ab70>,\n",
       " '패션': <gensim.models.keyedvectors.Vocab at 0x1934a0f0>,\n",
       " '패션스타그램': <gensim.models.keyedvectors.Vocab at 0x193099e8>,\n",
       " '패피': <gensim.models.keyedvectors.Vocab at 0x19309b38>,\n",
       " '펫스타그램': <gensim.models.keyedvectors.Vocab at 0x193343c8>,\n",
       " '푸드스타그램': <gensim.models.keyedvectors.Vocab at 0x19309a58>,\n",
       " '프로필': <gensim.models.keyedvectors.Vocab at 0x192c9c18>,\n",
       " '플레이팅': <gensim.models.keyedvectors.Vocab at 0x1934aba8>,\n",
       " '피드': <gensim.models.keyedvectors.Vocab at 0x19362710>,\n",
       " '필라테스': <gensim.models.keyedvectors.Vocab at 0x1934a320>,\n",
       " '하객룩': <gensim.models.keyedvectors.Vocab at 0x1934aeb8>,\n",
       " '핫플레이스': <gensim.models.keyedvectors.Vocab at 0x19362400>,\n",
       " '행복': <gensim.models.keyedvectors.Vocab at 0x1931e438>,\n",
       " '헬스': <gensim.models.keyedvectors.Vocab at 0x19334e80>,\n",
       " '헬스타그램': <gensim.models.keyedvectors.Vocab at 0x19362898>,\n",
       " '협찬': <gensim.models.keyedvectors.Vocab at 0x19309f60>,\n",
       " '홈데코': <gensim.models.keyedvectors.Vocab at 0x19362080>,\n",
       " '홈스타그램': <gensim.models.keyedvectors.Vocab at 0x1934a4e0>,\n",
       " '홈스타일링': <gensim.models.keyedvectors.Vocab at 0x1934a240>,\n",
       " '홈카페': <gensim.models.keyedvectors.Vocab at 0x1931ee10>,\n",
       " '홈쿡': <gensim.models.keyedvectors.Vocab at 0x19362b00>,\n",
       " '화보': <gensim.models.keyedvectors.Vocab at 0x19309630>,\n",
       " '황금개띠맘': <gensim.models.keyedvectors.Vocab at 0x17c5c080>,\n",
       " '훈남': <gensim.models.keyedvectors.Vocab at 0x19362dd8>,\n",
       " '훈녀': <gensim.models.keyedvectors.Vocab at 0x1934a048>,\n",
       " '휘트니스': <gensim.models.keyedvectors.Vocab at 0x1934a9e8>,\n",
       " '휴가': <gensim.models.keyedvectors.Vocab at 0x19334eb8>,\n",
       " '힐링': <gensim.models.keyedvectors.Vocab at 0x19334b38>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtv.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8285, 8285)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_idx), len(trans_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 319)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = wtv.wv.vectors.shape[0] + 1\n",
    "maxlen, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def generate_data(X, Y, maxlen, vocab_size):\n",
    "    for imgs, tags in zip(X, Y):\n",
    "        for i, tag in enumerate(tags):\n",
    "            inputs = []\n",
    "            targets = []\n",
    "            if imgs[i:i+4]:\n",
    "                inputs.append(imgs[i:i+4])\n",
    "                targets.append(tag)\n",
    "        \n",
    "            inputs_sequence = sequence.pad_sequences(inputs, maxlen=4)        \n",
    "            yield (inputs_sequence, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for x, y in generate_data(trans_idx, tag_idx, maxlen, vocab_size):\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "    \n",
    "X = np.concatenate(X)\n",
    "Y = np.concatenate(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = list(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(V, 128, input_length=4))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(256,\n",
    "                 3,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"626pt\" viewBox=\"0.00 0.00 449.00 626.00\" width=\"449pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 622)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-622 445,-622 445,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 427931632 -->\n",
       "<g class=\"node\" id=\"node1\"><title>427931632</title>\n",
       "<polygon fill=\"none\" points=\"62,-498.5 62,-544.5 379,-544.5 379,-498.5 62,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-517.8\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"225,-498.5 225,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"253\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"225,-521.5 281,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"253\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"281,-498.5 281,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330\" y=\"-529.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"281,-521.5 379,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330\" y=\"-506.3\">(None, 4, 128)</text>\n",
       "</g>\n",
       "<!-- 314768800 -->\n",
       "<g class=\"node\" id=\"node2\"><title>314768800</title>\n",
       "<polygon fill=\"none\" points=\"78.5,-415.5 78.5,-461.5 362.5,-461.5 362.5,-415.5 78.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-434.8\">dropout_1: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"208.5,-415.5 208.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"208.5,-438.5 264.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"264.5,-415.5 264.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313.5\" y=\"-446.3\">(None, 4, 128)</text>\n",
       "<polyline fill=\"none\" points=\"264.5,-438.5 362.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313.5\" y=\"-423.3\">(None, 4, 128)</text>\n",
       "</g>\n",
       "<!-- 427931632&#45;&gt;314768800 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>427931632-&gt;314768800</title>\n",
       "<path d=\"M220.5,-498.366C220.5,-490.152 220.5,-480.658 220.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224,-471.607 220.5,-461.607 217,-471.607 224,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 427932024 -->\n",
       "<g class=\"node\" id=\"node3\"><title>427932024</title>\n",
       "<polygon fill=\"none\" points=\"80,-332.5 80,-378.5 361,-378.5 361,-332.5 80,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-351.8\">conv1d_1: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"207,-332.5 207,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"207,-355.5 263,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"263,-332.5 263,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"312\" y=\"-363.3\">(None, 4, 128)</text>\n",
       "<polyline fill=\"none\" points=\"263,-355.5 361,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"312\" y=\"-340.3\">(None, 2, 256)</text>\n",
       "</g>\n",
       "<!-- 314768800&#45;&gt;427932024 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>314768800-&gt;427932024</title>\n",
       "<path d=\"M220.5,-415.366C220.5,-407.152 220.5,-397.658 220.5,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224,-388.607 220.5,-378.607 217,-388.607 224,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 427931688 -->\n",
       "<g class=\"node\" id=\"node4\"><title>427931688</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 441,-295.5 441,-249.5 0,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-268.8\">global_max_pooling1d_1: GlobalMaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"287,-249.5 287,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"287,-272.5 343,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"343,-249.5 343,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"392\" y=\"-280.3\">(None, 2, 256)</text>\n",
       "<polyline fill=\"none\" points=\"343,-272.5 441,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"392\" y=\"-257.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 427932024&#45;&gt;427931688 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>427932024-&gt;427931688</title>\n",
       "<path d=\"M220.5,-332.366C220.5,-324.152 220.5,-314.658 220.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224,-305.607 220.5,-295.607 217,-305.607 224,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 428361824 -->\n",
       "<g class=\"node\" id=\"node5\"><title>428361824</title>\n",
       "<polygon fill=\"none\" points=\"99,-166.5 99,-212.5 342,-212.5 342,-166.5 99,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151\" y=\"-185.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"203,-166.5 203,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"203,-189.5 259,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"259,-166.5 259,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300.5\" y=\"-197.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"259,-189.5 342,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300.5\" y=\"-174.3\">(None, 128)</text>\n",
       "</g>\n",
       "<!-- 427931688&#45;&gt;428361824 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>427931688-&gt;428361824</title>\n",
       "<path d=\"M220.5,-249.366C220.5,-241.152 220.5,-231.658 220.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224,-222.607 220.5,-212.607 217,-222.607 224,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 428361712 -->\n",
       "<g class=\"node\" id=\"node6\"><title>428361712</title>\n",
       "<polygon fill=\"none\" points=\"86,-83.5 86,-129.5 355,-129.5 355,-83.5 86,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151\" y=\"-102.8\">dropout_2: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"216,-83.5 216,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"244\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"216,-106.5 272,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"244\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"272,-83.5 272,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313.5\" y=\"-114.3\">(None, 128)</text>\n",
       "<polyline fill=\"none\" points=\"272,-106.5 355,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313.5\" y=\"-91.3\">(None, 128)</text>\n",
       "</g>\n",
       "<!-- 428361824&#45;&gt;428361712 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>428361824-&gt;428361712</title>\n",
       "<path d=\"M220.5,-166.366C220.5,-158.152 220.5,-148.658 220.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224,-139.607 220.5,-129.607 217,-139.607 224,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 428104448 -->\n",
       "<g class=\"node\" id=\"node7\"><title>428104448</title>\n",
       "<polygon fill=\"none\" points=\"99,-0.5 99,-46.5 342,-46.5 342,-0.5 99,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151\" y=\"-19.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"203,-0.5 203,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"203,-23.5 259,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"259,-0.5 259,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300.5\" y=\"-31.3\">(None, 128)</text>\n",
       "<polyline fill=\"none\" points=\"259,-23.5 342,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300.5\" y=\"-8.3\">(None, 319)</text>\n",
       "</g>\n",
       "<!-- 428361712&#45;&gt;428104448 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>428361712-&gt;428104448</title>\n",
       "<path d=\"M220.5,-83.3664C220.5,-75.1516 220.5,-65.6579 220.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224,-56.6068 220.5,-46.6068 217,-56.6069 224,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 427931912 -->\n",
       "<g class=\"node\" id=\"node8\"><title>427931912</title>\n",
       "<polygon fill=\"none\" points=\"182,-581.5 182,-617.5 259,-617.5 259,-581.5 182,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-595.8\">427931912</text>\n",
       "</g>\n",
       "<!-- 427931912&#45;&gt;427931632 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>427931912-&gt;427931632</title>\n",
       "<path d=\"M220.5,-581.254C220.5,-573.363 220.5,-563.749 220.5,-554.602\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224,-554.591 220.5,-544.591 217,-554.591 224,-554.591\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "Train on 47366 samples, validate on 5419 samples\n",
      "Epoch 1/100\n",
      "47366/47366 [==============================] - 7s 153us/step - loss: 5.1641 - acc: 0.0449 - val_loss: 5.0679 - val_acc: 0.0469\n",
      "Epoch 2/100\n",
      "47366/47366 [==============================] - 7s 141us/step - loss: 5.0184 - acc: 0.0505 - val_loss: 5.0520 - val_acc: 0.0487\n",
      "Epoch 3/100\n",
      "47366/47366 [==============================] - 7s 140us/step - loss: 4.9702 - acc: 0.0550 - val_loss: 5.0401 - val_acc: 0.0515\n",
      "Epoch 4/100\n",
      "47366/47366 [==============================] - 7s 139us/step - loss: 4.9120 - acc: 0.0585 - val_loss: 5.0366 - val_acc: 0.0513\n",
      "Epoch 5/100\n",
      "47366/47366 [==============================] - 7s 138us/step - loss: 4.8493 - acc: 0.0677 - val_loss: 5.0425 - val_acc: 0.0489\n",
      "Epoch 6/100\n",
      "47366/47366 [==============================] - 7s 140us/step - loss: 4.7754 - acc: 0.0790 - val_loss: 5.0606 - val_acc: 0.0454\n",
      "Epoch 7/100\n",
      "47366/47366 [==============================] - 7s 139us/step - loss: 4.6906 - acc: 0.0903 - val_loss: 5.0930 - val_acc: 0.0463\n",
      "Epoch 8/100\n",
      "47366/47366 [==============================] - 7s 140us/step - loss: 4.5969 - acc: 0.1027 - val_loss: 5.1209 - val_acc: 0.0448\n",
      "Epoch 9/100\n",
      "47366/47366 [==============================] - 7s 140us/step - loss: 4.5010 - acc: 0.1168 - val_loss: 5.1783 - val_acc: 0.0441\n",
      "Epoch 10/100\n",
      "47366/47366 [==============================] - 7s 139us/step - loss: 4.4003 - acc: 0.1305 - val_loss: 5.2347 - val_acc: 0.0415\n",
      "Epoch 11/100\n",
      "47366/47366 [==============================] - 7s 144us/step - loss: 4.3067 - acc: 0.1431 - val_loss: 5.2874 - val_acc: 0.0402\n",
      "Epoch 12/100\n",
      "47366/47366 [==============================] - 7s 142us/step - loss: 4.2067 - acc: 0.1579 - val_loss: 5.3440 - val_acc: 0.0376\n",
      "Epoch 13/100\n",
      "47366/47366 [==============================] - 7s 138us/step - loss: 4.1209 - acc: 0.1690 - val_loss: 5.3980 - val_acc: 0.0365\n",
      "Epoch 14/100\n",
      "47366/47366 [==============================] - 7s 139us/step - loss: 4.0420 - acc: 0.1787 - val_loss: 5.4739 - val_acc: 0.0365\n",
      "Epoch 15/100\n",
      "47366/47366 [==============================] - 7s 140us/step - loss: 3.9620 - acc: 0.1921 - val_loss: 5.5088 - val_acc: 0.0345\n",
      "Epoch 16/100\n",
      "47366/47366 [==============================] - 7s 140us/step - loss: 3.8862 - acc: 0.2006 - val_loss: 5.5609 - val_acc: 0.0334\n",
      "Epoch 17/100\n",
      "47366/47366 [==============================] - 7s 140us/step - loss: 3.8136 - acc: 0.2106 - val_loss: 5.6417 - val_acc: 0.0317\n",
      "Epoch 18/100\n",
      "47366/47366 [==============================] - 7s 141us/step - loss: 3.7395 - acc: 0.2225 - val_loss: 5.7212 - val_acc: 0.0343\n",
      "Epoch 19/100\n",
      "47366/47366 [==============================] - 7s 141us/step - loss: 3.6804 - acc: 0.2316 - val_loss: 5.7499 - val_acc: 0.0343\n",
      "Epoch 20/100\n",
      "47366/47366 [==============================] - 7s 142us/step - loss: 3.6348 - acc: 0.2354 - val_loss: 5.7990 - val_acc: 0.0304\n",
      "Epoch 21/100\n",
      "47366/47366 [==============================] - 7s 141us/step - loss: 3.5799 - acc: 0.2431 - val_loss: 5.8470 - val_acc: 0.0317\n",
      "Epoch 22/100\n",
      "47366/47366 [==============================] - 7s 142us/step - loss: 3.5282 - acc: 0.2518 - val_loss: 5.9059 - val_acc: 0.0321\n",
      "Epoch 23/100\n",
      "47366/47366 [==============================] - 7s 142us/step - loss: 3.4847 - acc: 0.2593 - val_loss: 5.9614 - val_acc: 0.0299\n",
      "Epoch 24/100\n",
      "47366/47366 [==============================] - 7s 142us/step - loss: 3.4455 - acc: 0.2665 - val_loss: 6.0161 - val_acc: 0.0308\n",
      "Epoch 25/100\n",
      "47366/47366 [==============================] - 7s 144us/step - loss: 3.4043 - acc: 0.2719 - val_loss: 6.0442 - val_acc: 0.0328\n",
      "Epoch 26/100\n",
      "47366/47366 [==============================] - 7s 142us/step - loss: 3.3654 - acc: 0.2771 - val_loss: 6.1004 - val_acc: 0.0336\n",
      "Epoch 27/100\n",
      "47366/47366 [==============================] - 7s 142us/step - loss: 3.3223 - acc: 0.2837 - val_loss: 6.1453 - val_acc: 0.0319\n",
      "Epoch 28/100\n",
      "47366/47366 [==============================] - 7s 143us/step - loss: 3.3000 - acc: 0.2860 - val_loss: 6.1955 - val_acc: 0.0292\n",
      "Epoch 29/100\n",
      "47366/47366 [==============================] - 7s 144us/step - loss: 3.2644 - acc: 0.2902 - val_loss: 6.2115 - val_acc: 0.0316\n",
      "Epoch 30/100\n",
      "47366/47366 [==============================] - 7s 143us/step - loss: 3.2299 - acc: 0.2972 - val_loss: 6.2519 - val_acc: 0.0343\n",
      "Epoch 31/100\n",
      "47366/47366 [==============================] - 7s 143us/step - loss: 3.2137 - acc: 0.2981 - val_loss: 6.2971 - val_acc: 0.0325\n",
      "Epoch 32/100\n",
      "47366/47366 [==============================] - 7s 142us/step - loss: 3.1780 - acc: 0.3035 - val_loss: 6.3065 - val_acc: 0.0336\n",
      "Epoch 33/100\n",
      "47366/47366 [==============================] - 7s 144us/step - loss: 3.1576 - acc: 0.3049 - val_loss: 6.3672 - val_acc: 0.0312\n",
      "Epoch 34/100\n",
      "47366/47366 [==============================] - 7s 145us/step - loss: 3.1326 - acc: 0.3112 - val_loss: 6.3869 - val_acc: 0.0328\n",
      "Epoch 35/100\n",
      "47366/47366 [==============================] - 7s 144us/step - loss: 3.1138 - acc: 0.3114 - val_loss: 6.4402 - val_acc: 0.0306\n",
      "Epoch 36/100\n",
      "47366/47366 [==============================] - 7s 144us/step - loss: 3.0891 - acc: 0.3145 - val_loss: 6.4564 - val_acc: 0.0314\n",
      "Epoch 37/100\n",
      "47366/47366 [==============================] - 7s 145us/step - loss: 3.0602 - acc: 0.3211 - val_loss: 6.4873 - val_acc: 0.0317\n",
      "Epoch 38/100\n",
      "47366/47366 [==============================] - 7s 142us/step - loss: 3.0357 - acc: 0.3237 - val_loss: 6.5120 - val_acc: 0.0299\n",
      "Epoch 39/100\n",
      "47366/47366 [==============================] - 7s 144us/step - loss: 3.0210 - acc: 0.3272 - val_loss: 6.5699 - val_acc: 0.0314\n",
      "Epoch 40/100\n",
      "47366/47366 [==============================] - 7s 144us/step - loss: 3.0009 - acc: 0.3289 - val_loss: 6.5928 - val_acc: 0.0293\n",
      "Epoch 41/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.9788 - acc: 0.3320 - val_loss: 6.6202 - val_acc: 0.0275\n",
      "Epoch 42/100\n",
      "47366/47366 [==============================] - 7s 145us/step - loss: 2.9730 - acc: 0.3331 - val_loss: 6.6667 - val_acc: 0.0303\n",
      "Epoch 43/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.9462 - acc: 0.3391 - val_loss: 6.6680 - val_acc: 0.0310\n",
      "Epoch 44/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.9312 - acc: 0.3372 - val_loss: 6.6878 - val_acc: 0.0293\n",
      "Epoch 45/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.9134 - acc: 0.3413 - val_loss: 6.7383 - val_acc: 0.0286\n",
      "Epoch 46/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.9055 - acc: 0.3428 - val_loss: 6.8009 - val_acc: 0.0282\n",
      "Epoch 47/100\n",
      "47366/47366 [==============================] - 7s 145us/step - loss: 2.8878 - acc: 0.3451 - val_loss: 6.7647 - val_acc: 0.0304\n",
      "Epoch 48/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.8780 - acc: 0.3460 - val_loss: 6.8199 - val_acc: 0.0266\n",
      "Epoch 49/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.8594 - acc: 0.3507 - val_loss: 6.8331 - val_acc: 0.0279\n",
      "Epoch 50/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.8386 - acc: 0.3529 - val_loss: 6.8322 - val_acc: 0.0290\n",
      "Epoch 51/100\n",
      "47366/47366 [==============================] - 7s 145us/step - loss: 2.8432 - acc: 0.3517 - val_loss: 6.8376 - val_acc: 0.0280\n",
      "Epoch 52/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.8257 - acc: 0.3546 - val_loss: 6.8946 - val_acc: 0.0292\n",
      "Epoch 53/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.8130 - acc: 0.3569 - val_loss: 6.9305 - val_acc: 0.0292\n",
      "Epoch 54/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.7985 - acc: 0.3591 - val_loss: 6.9458 - val_acc: 0.0277\n",
      "Epoch 55/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.7848 - acc: 0.3588 - val_loss: 6.9745 - val_acc: 0.0290\n",
      "Epoch 56/100\n",
      "47366/47366 [==============================] - 7s 144us/step - loss: 2.7736 - acc: 0.3623 - val_loss: 6.9882 - val_acc: 0.0288\n",
      "Epoch 57/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.7585 - acc: 0.3655 - val_loss: 7.0533 - val_acc: 0.0284\n",
      "Epoch 58/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.7580 - acc: 0.3629 - val_loss: 7.0247 - val_acc: 0.0290\n",
      "Epoch 59/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.7440 - acc: 0.3669 - val_loss: 7.0568 - val_acc: 0.0303\n",
      "Epoch 60/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.7341 - acc: 0.3676 - val_loss: 7.0580 - val_acc: 0.0273\n",
      "Epoch 61/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.7189 - acc: 0.3702 - val_loss: 7.0447 - val_acc: 0.0288\n",
      "Epoch 62/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.7117 - acc: 0.3692 - val_loss: 7.0788 - val_acc: 0.0273\n",
      "Epoch 63/100\n",
      "47366/47366 [==============================] - 7s 148us/step - loss: 2.7011 - acc: 0.3725 - val_loss: 7.1173 - val_acc: 0.0295\n",
      "Epoch 64/100\n",
      "47366/47366 [==============================] - 7s 148us/step - loss: 2.6945 - acc: 0.3759 - val_loss: 7.1076 - val_acc: 0.0310\n",
      "Epoch 65/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.6826 - acc: 0.3741 - val_loss: 7.1248 - val_acc: 0.0290\n",
      "Epoch 66/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.6893 - acc: 0.3737 - val_loss: 7.1289 - val_acc: 0.0273\n",
      "Epoch 67/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.6671 - acc: 0.3773 - val_loss: 7.1375 - val_acc: 0.0268\n",
      "Epoch 68/100\n",
      "47366/47366 [==============================] - 7s 148us/step - loss: 2.6599 - acc: 0.3804 - val_loss: 7.1991 - val_acc: 0.0288\n",
      "Epoch 69/100\n",
      "47366/47366 [==============================] - 7s 148us/step - loss: 2.6630 - acc: 0.3793 - val_loss: 7.2332 - val_acc: 0.0293\n",
      "Epoch 70/100\n",
      "47366/47366 [==============================] - 7s 149us/step - loss: 2.6415 - acc: 0.3819 - val_loss: 7.2202 - val_acc: 0.0280\n",
      "Epoch 71/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.6421 - acc: 0.3808 - val_loss: 7.2056 - val_acc: 0.0284\n",
      "Epoch 72/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.6296 - acc: 0.3809 - val_loss: 7.2206 - val_acc: 0.0275\n",
      "Epoch 73/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.6284 - acc: 0.3846 - val_loss: 7.2266 - val_acc: 0.0275\n",
      "Epoch 74/100\n",
      "47366/47366 [==============================] - 7s 145us/step - loss: 2.6247 - acc: 0.3854 - val_loss: 7.2329 - val_acc: 0.0288\n",
      "Epoch 75/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.6129 - acc: 0.3855 - val_loss: 7.2971 - val_acc: 0.0295\n",
      "Epoch 76/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.6073 - acc: 0.3853 - val_loss: 7.3120 - val_acc: 0.0273\n",
      "Epoch 77/100\n",
      "47366/47366 [==============================] - 7s 148us/step - loss: 2.5969 - acc: 0.3878 - val_loss: 7.3390 - val_acc: 0.0295\n",
      "Epoch 78/100\n",
      "47366/47366 [==============================] - 7s 148us/step - loss: 2.5883 - acc: 0.3895 - val_loss: 7.3702 - val_acc: 0.0293\n",
      "Epoch 79/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.5902 - acc: 0.3850 - val_loss: 7.3644 - val_acc: 0.0306\n",
      "Epoch 80/100\n",
      "47366/47366 [==============================] - 7s 146us/step - loss: 2.5814 - acc: 0.3934 - val_loss: 7.3725 - val_acc: 0.0304\n",
      "Epoch 81/100\n",
      "47366/47366 [==============================] - 7s 149us/step - loss: 2.5763 - acc: 0.3904 - val_loss: 7.3490 - val_acc: 0.0280\n",
      "Epoch 82/100\n",
      "47366/47366 [==============================] - 7s 152us/step - loss: 2.5719 - acc: 0.3903 - val_loss: 7.3764 - val_acc: 0.0264\n",
      "Epoch 83/100\n",
      "47366/47366 [==============================] - 7s 149us/step - loss: 2.5670 - acc: 0.3908 - val_loss: 7.4130 - val_acc: 0.0293\n",
      "Epoch 84/100\n",
      "47366/47366 [==============================] - 7s 150us/step - loss: 2.5537 - acc: 0.3939 - val_loss: 7.3772 - val_acc: 0.0279\n",
      "Epoch 85/100\n",
      "47366/47366 [==============================] - 7s 148us/step - loss: 2.5510 - acc: 0.3950 - val_loss: 7.4413 - val_acc: 0.0292\n",
      "Epoch 86/100\n",
      "47366/47366 [==============================] - 7s 150us/step - loss: 2.5354 - acc: 0.3982 - val_loss: 7.4747 - val_acc: 0.0273\n",
      "Epoch 87/100\n",
      "47366/47366 [==============================] - 7s 148us/step - loss: 2.5388 - acc: 0.3971 - val_loss: 7.4554 - val_acc: 0.0286\n",
      "Epoch 88/100\n",
      "47366/47366 [==============================] - 7s 149us/step - loss: 2.5457 - acc: 0.3977 - val_loss: 7.4149 - val_acc: 0.0264\n",
      "Epoch 89/100\n",
      "47366/47366 [==============================] - 7s 147us/step - loss: 2.5405 - acc: 0.3965 - val_loss: 7.4652 - val_acc: 0.0284\n",
      "Epoch 90/100\n",
      "47366/47366 [==============================] - 7s 149us/step - loss: 2.5227 - acc: 0.3993 - val_loss: 7.4884 - val_acc: 0.0258\n",
      "Epoch 91/100\n",
      "47366/47366 [==============================] - 7s 145us/step - loss: 2.5087 - acc: 0.4017 - val_loss: 7.5139 - val_acc: 0.0264\n",
      "Epoch 92/100\n",
      "47366/47366 [==============================] - 7s 148us/step - loss: 2.5154 - acc: 0.3997 - val_loss: 7.5222 - val_acc: 0.0279\n",
      "Epoch 93/100\n",
      "47366/47366 [==============================] - 7s 149us/step - loss: 2.4986 - acc: 0.4036 - val_loss: 7.4590 - val_acc: 0.0279\n",
      "Epoch 94/100\n",
      "47366/47366 [==============================] - 7s 150us/step - loss: 2.4976 - acc: 0.4051 - val_loss: 7.5281 - val_acc: 0.0269\n",
      "Epoch 95/100\n",
      "47366/47366 [==============================] - 7s 150us/step - loss: 2.4982 - acc: 0.4027 - val_loss: 7.4941 - val_acc: 0.0279\n",
      "Epoch 96/100\n",
      "47366/47366 [==============================] - 7s 150us/step - loss: 2.4994 - acc: 0.4019 - val_loss: 7.5388 - val_acc: 0.0269\n",
      "Epoch 97/100\n",
      "47366/47366 [==============================] - 7s 148us/step - loss: 2.4920 - acc: 0.4078 - val_loss: 7.5269 - val_acc: 0.0304\n",
      "Epoch 98/100\n",
      "47366/47366 [==============================] - 7s 150us/step - loss: 2.4916 - acc: 0.4037 - val_loss: 7.5302 - val_acc: 0.0280\n",
      "Epoch 99/100\n",
      "47366/47366 [==============================] - 7s 149us/step - loss: 2.4870 - acc: 0.4021 - val_loss: 7.5632 - val_acc: 0.0275\n",
      "Epoch 100/100\n",
      "47366/47366 [==============================] - 7s 150us/step - loss: 2.4805 - acc: 0.4077 - val_loss: 7.5597 - val_acc: 0.0253\n",
      "5419/5419 [==============================] - 0s 36us/step\n",
      "[7.559698524939703, 0.025281417235652335]\n",
      "\n",
      "Fold  1\n",
      "Train on 47395 samples, validate on 5390 samples\n",
      "Epoch 1/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 3.0964 - acc: 0.3624 - val_loss: 1.9180 - val_acc: 0.5896\n",
      "Epoch 2/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 3.0062 - acc: 0.3651 - val_loss: 2.0112 - val_acc: 0.5575\n",
      "Epoch 3/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.9564 - acc: 0.3664 - val_loss: 2.0990 - val_acc: 0.5334\n",
      "Epoch 4/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.9017 - acc: 0.3659 - val_loss: 2.1665 - val_acc: 0.5083\n",
      "Epoch 5/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.8674 - acc: 0.3664 - val_loss: 2.2426 - val_acc: 0.4798\n",
      "Epoch 6/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.8382 - acc: 0.3710 - val_loss: 2.3106 - val_acc: 0.4638\n",
      "Epoch 7/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.8084 - acc: 0.3710 - val_loss: 2.3665 - val_acc: 0.4495\n",
      "Epoch 8/100\n",
      "47395/47395 [==============================] - 7s 147us/step - loss: 2.7952 - acc: 0.3684 - val_loss: 2.4223 - val_acc: 0.4364\n",
      "Epoch 9/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.7593 - acc: 0.3758 - val_loss: 2.4712 - val_acc: 0.4212\n",
      "Epoch 10/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.7531 - acc: 0.3723 - val_loss: 2.5331 - val_acc: 0.4059\n",
      "Epoch 11/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.7482 - acc: 0.3723 - val_loss: 2.5825 - val_acc: 0.3946\n",
      "Epoch 12/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.7249 - acc: 0.3752 - val_loss: 2.6245 - val_acc: 0.3787\n",
      "Epoch 13/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.7000 - acc: 0.3790 - val_loss: 2.6820 - val_acc: 0.3657\n",
      "Epoch 14/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.6964 - acc: 0.3760 - val_loss: 2.7243 - val_acc: 0.3616\n",
      "Epoch 15/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.6796 - acc: 0.3805 - val_loss: 2.7714 - val_acc: 0.3417\n",
      "Epoch 16/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.6662 - acc: 0.3832 - val_loss: 2.8114 - val_acc: 0.3429\n",
      "Epoch 17/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.6591 - acc: 0.3831 - val_loss: 2.8720 - val_acc: 0.3314\n",
      "Epoch 18/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.6552 - acc: 0.3854 - val_loss: 2.9050 - val_acc: 0.3243\n",
      "Epoch 19/100\n",
      "47395/47395 [==============================] - 7s 148us/step - loss: 2.6432 - acc: 0.3841 - val_loss: 2.9458 - val_acc: 0.3174\n",
      "Epoch 20/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.6351 - acc: 0.3862 - val_loss: 2.9816 - val_acc: 0.3074\n",
      "Epoch 21/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.6252 - acc: 0.3849 - val_loss: 3.0254 - val_acc: 0.2968\n",
      "Epoch 22/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.6109 - acc: 0.3874 - val_loss: 3.0637 - val_acc: 0.2931\n",
      "Epoch 23/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.6092 - acc: 0.3871 - val_loss: 3.0992 - val_acc: 0.2907\n",
      "Epoch 24/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.6032 - acc: 0.3889 - val_loss: 3.1397 - val_acc: 0.2774\n",
      "Epoch 25/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.5968 - acc: 0.3883 - val_loss: 3.1668 - val_acc: 0.2709\n",
      "Epoch 26/100\n",
      "47395/47395 [==============================] - 7s 148us/step - loss: 2.5917 - acc: 0.3878 - val_loss: 3.2180 - val_acc: 0.2664\n",
      "Epoch 27/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.5837 - acc: 0.3921 - val_loss: 3.2412 - val_acc: 0.2594\n",
      "Epoch 28/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.5810 - acc: 0.3941 - val_loss: 3.2895 - val_acc: 0.2542\n",
      "Epoch 29/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.5749 - acc: 0.3893 - val_loss: 3.3099 - val_acc: 0.2497\n",
      "Epoch 30/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.5567 - acc: 0.3977 - val_loss: 3.3591 - val_acc: 0.2436\n",
      "Epoch 31/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.5634 - acc: 0.3926 - val_loss: 3.3968 - val_acc: 0.2443\n",
      "Epoch 32/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.5461 - acc: 0.3989 - val_loss: 3.4296 - val_acc: 0.2373\n",
      "Epoch 33/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.5463 - acc: 0.3971 - val_loss: 3.4663 - val_acc: 0.2338\n",
      "Epoch 34/100\n",
      "47395/47395 [==============================] - 7s 148us/step - loss: 2.5415 - acc: 0.3962 - val_loss: 3.5047 - val_acc: 0.2282\n",
      "Epoch 35/100\n",
      "47395/47395 [==============================] - 7s 148us/step - loss: 2.5364 - acc: 0.3995 - val_loss: 3.5239 - val_acc: 0.2289\n",
      "Epoch 36/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.5249 - acc: 0.4000 - val_loss: 3.5518 - val_acc: 0.2212\n",
      "Epoch 37/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.5218 - acc: 0.4028 - val_loss: 3.5933 - val_acc: 0.2158\n",
      "Epoch 38/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.5201 - acc: 0.3988 - val_loss: 3.6103 - val_acc: 0.2134\n",
      "Epoch 39/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.5155 - acc: 0.4010 - val_loss: 3.6519 - val_acc: 0.2052\n",
      "Epoch 40/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.5201 - acc: 0.4015 - val_loss: 3.6727 - val_acc: 0.2041\n",
      "Epoch 41/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4949 - acc: 0.4044 - val_loss: 3.7175 - val_acc: 0.2065\n",
      "Epoch 42/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.4995 - acc: 0.4045 - val_loss: 3.7144 - val_acc: 0.2009\n",
      "Epoch 43/100\n",
      "47395/47395 [==============================] - 7s 147us/step - loss: 2.4917 - acc: 0.4068 - val_loss: 3.7730 - val_acc: 0.2020\n",
      "Epoch 44/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.4962 - acc: 0.4070 - val_loss: 3.8081 - val_acc: 0.1941\n",
      "Epoch 45/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4939 - acc: 0.4039 - val_loss: 3.8224 - val_acc: 0.1904\n",
      "Epoch 46/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.4858 - acc: 0.4041 - val_loss: 3.8541 - val_acc: 0.1900\n",
      "Epoch 47/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4813 - acc: 0.4040 - val_loss: 3.8660 - val_acc: 0.1866\n",
      "Epoch 48/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4763 - acc: 0.4084 - val_loss: 3.8969 - val_acc: 0.1907\n",
      "Epoch 49/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4655 - acc: 0.4075 - val_loss: 3.9225 - val_acc: 0.1865\n",
      "Epoch 50/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4704 - acc: 0.4072 - val_loss: 3.9576 - val_acc: 0.1829\n",
      "Epoch 51/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.4629 - acc: 0.4095 - val_loss: 3.9678 - val_acc: 0.1759\n",
      "Epoch 52/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.4545 - acc: 0.4124 - val_loss: 4.0222 - val_acc: 0.1675\n",
      "Epoch 53/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4572 - acc: 0.4080 - val_loss: 4.0450 - val_acc: 0.1742\n",
      "Epoch 54/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.4472 - acc: 0.4122 - val_loss: 4.0544 - val_acc: 0.1709\n",
      "Epoch 55/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4595 - acc: 0.4060 - val_loss: 4.0810 - val_acc: 0.1668\n",
      "Epoch 56/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.4442 - acc: 0.4126 - val_loss: 4.1240 - val_acc: 0.1670\n",
      "Epoch 57/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4445 - acc: 0.4112 - val_loss: 4.1436 - val_acc: 0.1666\n",
      "Epoch 58/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4424 - acc: 0.4124 - val_loss: 4.1951 - val_acc: 0.1660\n",
      "Epoch 59/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.4449 - acc: 0.4105 - val_loss: 4.1788 - val_acc: 0.1607\n",
      "Epoch 60/100\n",
      "47395/47395 [==============================] - 7s 146us/step - loss: 2.4369 - acc: 0.4131 - val_loss: 4.2148 - val_acc: 0.1616\n",
      "Epoch 61/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4388 - acc: 0.4132 - val_loss: 4.2412 - val_acc: 0.1590\n",
      "Epoch 62/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.4223 - acc: 0.4155 - val_loss: 4.2563 - val_acc: 0.1544\n",
      "Epoch 63/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4369 - acc: 0.4140 - val_loss: 4.2850 - val_acc: 0.1501\n",
      "Epoch 64/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.4262 - acc: 0.4147 - val_loss: 4.3431 - val_acc: 0.1549\n",
      "Epoch 65/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.4162 - acc: 0.4162 - val_loss: 4.3438 - val_acc: 0.1527\n",
      "Epoch 66/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.4186 - acc: 0.4168 - val_loss: 4.3233 - val_acc: 0.1510\n",
      "Epoch 67/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.4130 - acc: 0.4163 - val_loss: 4.3793 - val_acc: 0.1486\n",
      "Epoch 68/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.4082 - acc: 0.4180 - val_loss: 4.3945 - val_acc: 0.1479\n",
      "Epoch 69/100\n",
      "47395/47395 [==============================] - 7s 152us/step - loss: 2.4147 - acc: 0.4169 - val_loss: 4.4084 - val_acc: 0.1456\n",
      "Epoch 70/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4009 - acc: 0.4194 - val_loss: 4.4497 - val_acc: 0.1449\n",
      "Epoch 71/100\n",
      "47395/47395 [==============================] - 7s 146us/step - loss: 2.3992 - acc: 0.4201 - val_loss: 4.4566 - val_acc: 0.1395\n",
      "Epoch 72/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.3952 - acc: 0.4193 - val_loss: 4.5069 - val_acc: 0.1384\n",
      "Epoch 73/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4055 - acc: 0.4164 - val_loss: 4.5537 - val_acc: 0.1365\n",
      "Epoch 74/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.4007 - acc: 0.4178 - val_loss: 4.5305 - val_acc: 0.1386\n",
      "Epoch 75/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3972 - acc: 0.4203 - val_loss: 4.5433 - val_acc: 0.1421\n",
      "Epoch 76/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.3768 - acc: 0.4234 - val_loss: 4.5902 - val_acc: 0.1380\n",
      "Epoch 77/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3886 - acc: 0.4206 - val_loss: 4.6257 - val_acc: 0.1325\n",
      "Epoch 78/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3811 - acc: 0.4235 - val_loss: 4.6501 - val_acc: 0.1293\n",
      "Epoch 79/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.3868 - acc: 0.4209 - val_loss: 4.6466 - val_acc: 0.1288\n",
      "Epoch 80/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.3847 - acc: 0.4217 - val_loss: 4.6469 - val_acc: 0.1291\n",
      "Epoch 81/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.3762 - acc: 0.4229 - val_loss: 4.6719 - val_acc: 0.1275\n",
      "Epoch 82/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3732 - acc: 0.4232 - val_loss: 4.7064 - val_acc: 0.1317\n",
      "Epoch 83/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3640 - acc: 0.4263 - val_loss: 4.7329 - val_acc: 0.1295\n",
      "Epoch 84/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3608 - acc: 0.4236 - val_loss: 4.7225 - val_acc: 0.1265\n",
      "Epoch 85/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3688 - acc: 0.4210 - val_loss: 4.7653 - val_acc: 0.1250\n",
      "Epoch 86/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.3671 - acc: 0.4249 - val_loss: 4.7678 - val_acc: 0.1269\n",
      "Epoch 87/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3524 - acc: 0.4274 - val_loss: 4.7805 - val_acc: 0.1236\n",
      "Epoch 88/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3593 - acc: 0.4276 - val_loss: 4.8110 - val_acc: 0.1223\n",
      "Epoch 89/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.3677 - acc: 0.4242 - val_loss: 4.8307 - val_acc: 0.1223\n",
      "Epoch 90/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3526 - acc: 0.4273 - val_loss: 4.8588 - val_acc: 0.1215\n",
      "Epoch 91/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3610 - acc: 0.4244 - val_loss: 4.8401 - val_acc: 0.1193\n",
      "Epoch 92/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3550 - acc: 0.4279 - val_loss: 4.9024 - val_acc: 0.1221\n",
      "Epoch 93/100\n",
      "47395/47395 [==============================] - 7s 149us/step - loss: 2.3430 - acc: 0.4293 - val_loss: 4.9058 - val_acc: 0.1191\n",
      "Epoch 94/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3447 - acc: 0.4252 - val_loss: 4.8940 - val_acc: 0.1145\n",
      "Epoch 95/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3437 - acc: 0.4308 - val_loss: 4.9375 - val_acc: 0.1169\n",
      "Epoch 96/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3344 - acc: 0.4289 - val_loss: 4.9802 - val_acc: 0.1158\n",
      "Epoch 97/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3353 - acc: 0.4308 - val_loss: 4.9659 - val_acc: 0.1161\n",
      "Epoch 98/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.3459 - acc: 0.4269 - val_loss: 4.9655 - val_acc: 0.1156\n",
      "Epoch 99/100\n",
      "47395/47395 [==============================] - 7s 151us/step - loss: 2.3396 - acc: 0.4304 - val_loss: 5.0006 - val_acc: 0.1098\n",
      "Epoch 100/100\n",
      "47395/47395 [==============================] - 7s 150us/step - loss: 2.3268 - acc: 0.4314 - val_loss: 5.0288 - val_acc: 0.1139\n",
      "5390/5390 [==============================] - 0s 34us/step\n",
      "[5.028758281325586, 0.11391465677732882]\n",
      "\n",
      "Fold  2\n",
      "Train on 47422 samples, validate on 5363 samples\n",
      "Epoch 1/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.7400 - acc: 0.3889 - val_loss: 1.5318 - val_acc: 0.6521\n",
      "Epoch 2/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.6748 - acc: 0.3904 - val_loss: 1.6049 - val_acc: 0.6327\n",
      "Epoch 3/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.6321 - acc: 0.3909 - val_loss: 1.6769 - val_acc: 0.6071\n",
      "Epoch 4/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.6008 - acc: 0.3944 - val_loss: 1.7422 - val_acc: 0.5930\n",
      "Epoch 5/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.5721 - acc: 0.3970 - val_loss: 1.7992 - val_acc: 0.5706\n",
      "Epoch 6/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.5588 - acc: 0.3975 - val_loss: 1.8415 - val_acc: 0.5624\n",
      "Epoch 7/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.5347 - acc: 0.3986 - val_loss: 1.9059 - val_acc: 0.5448\n",
      "Epoch 8/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.5186 - acc: 0.4025 - val_loss: 1.9476 - val_acc: 0.5275\n",
      "Epoch 9/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.5125 - acc: 0.3995 - val_loss: 1.9867 - val_acc: 0.5135\n",
      "Epoch 10/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.4883 - acc: 0.4058 - val_loss: 2.0291 - val_acc: 0.5072\n",
      "Epoch 11/100\n",
      "47422/47422 [==============================] - 7s 151us/step - loss: 2.4937 - acc: 0.4049 - val_loss: 2.0721 - val_acc: 0.4958\n",
      "Epoch 12/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.4863 - acc: 0.4061 - val_loss: 2.1172 - val_acc: 0.4863\n",
      "Epoch 13/100\n",
      "47422/47422 [==============================] - 7s 153us/step - loss: 2.4746 - acc: 0.4051 - val_loss: 2.1452 - val_acc: 0.4745\n",
      "Epoch 14/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4613 - acc: 0.4061 - val_loss: 2.1852 - val_acc: 0.4649\n",
      "Epoch 15/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4573 - acc: 0.4068 - val_loss: 2.2150 - val_acc: 0.4563\n",
      "Epoch 16/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4547 - acc: 0.4103 - val_loss: 2.2549 - val_acc: 0.4462\n",
      "Epoch 17/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4519 - acc: 0.4062 - val_loss: 2.2971 - val_acc: 0.4339\n",
      "Epoch 18/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4490 - acc: 0.4092 - val_loss: 2.3320 - val_acc: 0.4313\n",
      "Epoch 19/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4311 - acc: 0.4112 - val_loss: 2.3570 - val_acc: 0.4251\n",
      "Epoch 20/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4387 - acc: 0.4100 - val_loss: 2.3916 - val_acc: 0.4141\n",
      "Epoch 21/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4223 - acc: 0.4150 - val_loss: 2.4325 - val_acc: 0.4013\n",
      "Epoch 22/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4209 - acc: 0.4136 - val_loss: 2.4557 - val_acc: 0.4039\n",
      "Epoch 23/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4277 - acc: 0.4112 - val_loss: 2.4847 - val_acc: 0.3994\n",
      "Epoch 24/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4158 - acc: 0.4168 - val_loss: 2.5086 - val_acc: 0.3882\n",
      "Epoch 25/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4148 - acc: 0.4133 - val_loss: 2.5357 - val_acc: 0.3828\n",
      "Epoch 26/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.4088 - acc: 0.4164 - val_loss: 2.5716 - val_acc: 0.3789\n",
      "Epoch 27/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3995 - acc: 0.4186 - val_loss: 2.6062 - val_acc: 0.3729\n",
      "Epoch 28/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3983 - acc: 0.4187 - val_loss: 2.6310 - val_acc: 0.3714\n",
      "Epoch 29/100\n",
      "47422/47422 [==============================] - 7s 151us/step - loss: 2.4036 - acc: 0.4138 - val_loss: 2.6646 - val_acc: 0.3629\n",
      "Epoch 30/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3934 - acc: 0.4184 - val_loss: 2.6828 - val_acc: 0.3565\n",
      "Epoch 31/100\n",
      "47422/47422 [==============================] - 7s 151us/step - loss: 2.3925 - acc: 0.4169 - val_loss: 2.7159 - val_acc: 0.3541\n",
      "Epoch 32/100\n",
      "47422/47422 [==============================] - 7s 151us/step - loss: 2.3831 - acc: 0.4194 - val_loss: 2.7578 - val_acc: 0.3425\n",
      "Epoch 33/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3926 - acc: 0.4157 - val_loss: 2.7812 - val_acc: 0.3412\n",
      "Epoch 34/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3754 - acc: 0.4233 - val_loss: 2.8034 - val_acc: 0.3317\n",
      "Epoch 35/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3928 - acc: 0.4182 - val_loss: 2.8232 - val_acc: 0.3323\n",
      "Epoch 36/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3781 - acc: 0.4213 - val_loss: 2.8611 - val_acc: 0.3218\n",
      "Epoch 37/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3777 - acc: 0.4180 - val_loss: 2.8809 - val_acc: 0.3220\n",
      "Epoch 38/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3600 - acc: 0.4253 - val_loss: 2.9080 - val_acc: 0.3175\n",
      "Epoch 39/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3620 - acc: 0.4241 - val_loss: 2.9274 - val_acc: 0.3183\n",
      "Epoch 40/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3619 - acc: 0.4211 - val_loss: 2.9585 - val_acc: 0.3103\n",
      "Epoch 41/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3622 - acc: 0.4237 - val_loss: 2.9811 - val_acc: 0.3058\n",
      "Epoch 42/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3627 - acc: 0.4230 - val_loss: 3.0081 - val_acc: 0.3010\n",
      "Epoch 43/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3667 - acc: 0.4214 - val_loss: 3.0274 - val_acc: 0.3028\n",
      "Epoch 44/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3527 - acc: 0.4254 - val_loss: 3.0655 - val_acc: 0.2983\n",
      "Epoch 45/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3478 - acc: 0.4250 - val_loss: 3.0636 - val_acc: 0.2976\n",
      "Epoch 46/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3549 - acc: 0.4229 - val_loss: 3.0994 - val_acc: 0.2909\n",
      "Epoch 47/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3436 - acc: 0.4251 - val_loss: 3.1295 - val_acc: 0.2855\n",
      "Epoch 48/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3431 - acc: 0.4275 - val_loss: 3.1478 - val_acc: 0.2819\n",
      "Epoch 49/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3359 - acc: 0.4282 - val_loss: 3.1731 - val_acc: 0.2831\n",
      "Epoch 50/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3398 - acc: 0.4257 - val_loss: 3.2021 - val_acc: 0.2773\n",
      "Epoch 51/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3450 - acc: 0.4265 - val_loss: 3.2304 - val_acc: 0.2724\n",
      "Epoch 52/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3328 - acc: 0.4295 - val_loss: 3.2343 - val_acc: 0.2683\n",
      "Epoch 53/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3256 - acc: 0.4272 - val_loss: 3.2573 - val_acc: 0.2696\n",
      "Epoch 54/100\n",
      "47422/47422 [==============================] - 7s 153us/step - loss: 2.3359 - acc: 0.4290 - val_loss: 3.2846 - val_acc: 0.2678\n",
      "Epoch 55/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3394 - acc: 0.4254 - val_loss: 3.2971 - val_acc: 0.2655\n",
      "Epoch 56/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3251 - acc: 0.4296 - val_loss: 3.3069 - val_acc: 0.2614\n",
      "Epoch 57/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3293 - acc: 0.4272 - val_loss: 3.3170 - val_acc: 0.2594\n",
      "Epoch 58/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3322 - acc: 0.4287 - val_loss: 3.3501 - val_acc: 0.2553\n",
      "Epoch 59/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3293 - acc: 0.4295 - val_loss: 3.3622 - val_acc: 0.2560\n",
      "Epoch 60/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3171 - acc: 0.4302 - val_loss: 3.3823 - val_acc: 0.2499\n",
      "Epoch 61/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3157 - acc: 0.4328 - val_loss: 3.4067 - val_acc: 0.2541\n",
      "Epoch 62/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3084 - acc: 0.4340 - val_loss: 3.4508 - val_acc: 0.2476\n",
      "Epoch 63/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3217 - acc: 0.4295 - val_loss: 3.4431 - val_acc: 0.2420\n",
      "Epoch 64/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3111 - acc: 0.4303 - val_loss: 3.4814 - val_acc: 0.2409\n",
      "Epoch 65/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3132 - acc: 0.4295 - val_loss: 3.4772 - val_acc: 0.2372\n",
      "Epoch 66/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3214 - acc: 0.4295 - val_loss: 3.4909 - val_acc: 0.2415\n",
      "Epoch 67/100\n",
      "47422/47422 [==============================] - 7s 148us/step - loss: 2.3057 - acc: 0.4329 - val_loss: 3.5300 - val_acc: 0.2344\n",
      "Epoch 68/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3084 - acc: 0.4293 - val_loss: 3.5480 - val_acc: 0.2323\n",
      "Epoch 69/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3252 - acc: 0.4280 - val_loss: 3.5429 - val_acc: 0.2331\n",
      "Epoch 70/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3135 - acc: 0.4307 - val_loss: 3.5810 - val_acc: 0.2329\n",
      "Epoch 71/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3010 - acc: 0.4336 - val_loss: 3.6053 - val_acc: 0.2273\n",
      "Epoch 72/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3097 - acc: 0.4322 - val_loss: 3.6219 - val_acc: 0.2264\n",
      "Epoch 73/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.3014 - acc: 0.4353 - val_loss: 3.6469 - val_acc: 0.2232\n",
      "Epoch 74/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.3029 - acc: 0.4321 - val_loss: 3.6364 - val_acc: 0.2225\n",
      "Epoch 75/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2951 - acc: 0.4361 - val_loss: 3.6709 - val_acc: 0.2193\n",
      "Epoch 76/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2830 - acc: 0.4360 - val_loss: 3.6847 - val_acc: 0.2204\n",
      "Epoch 77/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2980 - acc: 0.4350 - val_loss: 3.7243 - val_acc: 0.2202\n",
      "Epoch 78/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2916 - acc: 0.4360 - val_loss: 3.7193 - val_acc: 0.2156\n",
      "Epoch 79/100\n",
      "47422/47422 [==============================] - 7s 151us/step - loss: 2.2799 - acc: 0.4378 - val_loss: 3.7528 - val_acc: 0.2129\n",
      "Epoch 80/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.2936 - acc: 0.4338 - val_loss: 3.7644 - val_acc: 0.2131\n",
      "Epoch 81/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2964 - acc: 0.4369 - val_loss: 3.7660 - val_acc: 0.2128\n",
      "Epoch 82/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2848 - acc: 0.4381 - val_loss: 3.7762 - val_acc: 0.2103\n",
      "Epoch 83/100\n",
      "47422/47422 [==============================] - 7s 151us/step - loss: 2.2809 - acc: 0.4367 - val_loss: 3.8022 - val_acc: 0.2096\n",
      "Epoch 84/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2831 - acc: 0.4369 - val_loss: 3.8465 - val_acc: 0.2040\n",
      "Epoch 85/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.2824 - acc: 0.4363 - val_loss: 3.8501 - val_acc: 0.2049\n",
      "Epoch 86/100\n",
      "47422/47422 [==============================] - 7s 151us/step - loss: 2.2872 - acc: 0.4348 - val_loss: 3.8803 - val_acc: 0.2027\n",
      "Epoch 87/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2828 - acc: 0.4359 - val_loss: 3.8628 - val_acc: 0.1962\n",
      "Epoch 88/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2803 - acc: 0.4384 - val_loss: 3.8824 - val_acc: 0.1973\n",
      "Epoch 89/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2820 - acc: 0.4374 - val_loss: 3.9159 - val_acc: 0.1943\n",
      "Epoch 90/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.2694 - acc: 0.4402 - val_loss: 3.9319 - val_acc: 0.1965\n",
      "Epoch 91/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2795 - acc: 0.4365 - val_loss: 4.0001 - val_acc: 0.1937\n",
      "Epoch 92/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2706 - acc: 0.4388 - val_loss: 3.9611 - val_acc: 0.1954\n",
      "Epoch 93/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.2729 - acc: 0.4401 - val_loss: 3.9909 - val_acc: 0.1926\n",
      "Epoch 94/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2735 - acc: 0.4361 - val_loss: 4.0093 - val_acc: 0.1934\n",
      "Epoch 95/100\n",
      "47422/47422 [==============================] - 7s 148us/step - loss: 2.2606 - acc: 0.4413 - val_loss: 4.0302 - val_acc: 0.1870\n",
      "Epoch 96/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2659 - acc: 0.4403 - val_loss: 4.0570 - val_acc: 0.1909\n",
      "Epoch 97/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.2608 - acc: 0.4388 - val_loss: 4.0861 - val_acc: 0.1889\n",
      "Epoch 98/100\n",
      "47422/47422 [==============================] - 7s 149us/step - loss: 2.2688 - acc: 0.4379 - val_loss: 4.0767 - val_acc: 0.1859\n",
      "Epoch 99/100\n",
      "47422/47422 [==============================] - 7s 151us/step - loss: 2.2712 - acc: 0.4384 - val_loss: 4.1235 - val_acc: 0.1840\n",
      "Epoch 100/100\n",
      "47422/47422 [==============================] - 7s 150us/step - loss: 2.2574 - acc: 0.4403 - val_loss: 4.1307 - val_acc: 0.1846\n",
      "5363/5363 [==============================] - 0s 36us/step\n",
      "[4.1307398390463215, 0.18459817267011044]\n",
      "\n",
      "Fold  3\n",
      "Train on 47454 samples, validate on 5331 samples\n",
      "Epoch 1/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.5961 - acc: 0.3994 - val_loss: 1.4189 - val_acc: 0.6732\n",
      "Epoch 2/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.5387 - acc: 0.4024 - val_loss: 1.4686 - val_acc: 0.6556\n",
      "Epoch 3/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.5056 - acc: 0.4069 - val_loss: 1.5346 - val_acc: 0.6355\n",
      "Epoch 4/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.4691 - acc: 0.4102 - val_loss: 1.5834 - val_acc: 0.6239\n",
      "Epoch 5/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.4425 - acc: 0.4124 - val_loss: 1.6355 - val_acc: 0.6027\n",
      "Epoch 6/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.4295 - acc: 0.4128 - val_loss: 1.6840 - val_acc: 0.5896\n",
      "Epoch 7/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.4105 - acc: 0.4172 - val_loss: 1.7299 - val_acc: 0.5755\n",
      "Epoch 8/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.4018 - acc: 0.4195 - val_loss: 1.7551 - val_acc: 0.5644\n",
      "Epoch 9/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3866 - acc: 0.4212 - val_loss: 1.8114 - val_acc: 0.5528\n",
      "Epoch 10/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3876 - acc: 0.4202 - val_loss: 1.8388 - val_acc: 0.5397\n",
      "Epoch 11/100\n",
      "47454/47454 [==============================] - 7s 149us/step - loss: 2.3598 - acc: 0.4238 - val_loss: 1.8769 - val_acc: 0.5324\n",
      "Epoch 12/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3636 - acc: 0.4222 - val_loss: 1.9208 - val_acc: 0.5226\n",
      "Epoch 13/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.3629 - acc: 0.4224 - val_loss: 1.9540 - val_acc: 0.5162\n",
      "Epoch 14/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.3513 - acc: 0.4233 - val_loss: 1.9767 - val_acc: 0.5076\n",
      "Epoch 15/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.3518 - acc: 0.4235 - val_loss: 2.0150 - val_acc: 0.4967\n",
      "Epoch 16/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.3323 - acc: 0.4269 - val_loss: 2.0434 - val_acc: 0.4915\n",
      "Epoch 17/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3365 - acc: 0.4230 - val_loss: 2.0696 - val_acc: 0.4838\n",
      "Epoch 18/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3362 - acc: 0.4244 - val_loss: 2.1049 - val_acc: 0.4748\n",
      "Epoch 19/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.3248 - acc: 0.4292 - val_loss: 2.1316 - val_acc: 0.4708\n",
      "Epoch 20/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3206 - acc: 0.4298 - val_loss: 2.1532 - val_acc: 0.4628\n",
      "Epoch 21/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.3229 - acc: 0.4251 - val_loss: 2.1831 - val_acc: 0.4571\n",
      "Epoch 22/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3225 - acc: 0.4299 - val_loss: 2.2164 - val_acc: 0.4464\n",
      "Epoch 23/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3210 - acc: 0.4299 - val_loss: 2.2280 - val_acc: 0.4423\n",
      "Epoch 24/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3234 - acc: 0.4292 - val_loss: 2.2625 - val_acc: 0.4389\n",
      "Epoch 25/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3160 - acc: 0.4264 - val_loss: 2.2785 - val_acc: 0.4314\n",
      "Epoch 26/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3169 - acc: 0.4284 - val_loss: 2.3208 - val_acc: 0.4221\n",
      "Epoch 27/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3052 - acc: 0.4300 - val_loss: 2.3374 - val_acc: 0.4183\n",
      "Epoch 28/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.3197 - acc: 0.4289 - val_loss: 2.3671 - val_acc: 0.4123\n",
      "Epoch 29/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2995 - acc: 0.4334 - val_loss: 2.4007 - val_acc: 0.4005\n",
      "Epoch 30/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2960 - acc: 0.4330 - val_loss: 2.4170 - val_acc: 0.4039\n",
      "Epoch 31/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.3049 - acc: 0.4325 - val_loss: 2.4373 - val_acc: 0.3994\n",
      "Epoch 32/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2927 - acc: 0.4312 - val_loss: 2.4594 - val_acc: 0.3943\n",
      "Epoch 33/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2994 - acc: 0.4314 - val_loss: 2.4795 - val_acc: 0.3898\n",
      "Epoch 34/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2838 - acc: 0.4366 - val_loss: 2.4988 - val_acc: 0.3808\n",
      "Epoch 35/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2767 - acc: 0.4364 - val_loss: 2.5201 - val_acc: 0.3840\n",
      "Epoch 36/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2902 - acc: 0.4315 - val_loss: 2.5410 - val_acc: 0.3782\n",
      "Epoch 37/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2828 - acc: 0.4326 - val_loss: 2.5618 - val_acc: 0.3731\n",
      "Epoch 38/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2896 - acc: 0.4340 - val_loss: 2.5796 - val_acc: 0.3693\n",
      "Epoch 39/100\n",
      "47454/47454 [==============================] - 7s 153us/step - loss: 2.2805 - acc: 0.4367 - val_loss: 2.6219 - val_acc: 0.3652\n",
      "Epoch 40/100\n",
      "47454/47454 [==============================] - 7s 152us/step - loss: 2.2789 - acc: 0.4369 - val_loss: 2.6434 - val_acc: 0.3618\n",
      "Epoch 41/100\n",
      "47454/47454 [==============================] - 8s 163us/step - loss: 2.2761 - acc: 0.4370 - val_loss: 2.6696 - val_acc: 0.3581\n",
      "Epoch 42/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2829 - acc: 0.4364 - val_loss: 2.6848 - val_acc: 0.3532\n",
      "Epoch 43/100\n",
      "47454/47454 [==============================] - 7s 149us/step - loss: 2.2757 - acc: 0.4364 - val_loss: 2.7166 - val_acc: 0.3444\n",
      "Epoch 44/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2585 - acc: 0.4414 - val_loss: 2.7362 - val_acc: 0.3448\n",
      "Epoch 45/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2743 - acc: 0.4406 - val_loss: 2.7520 - val_acc: 0.3414\n",
      "Epoch 46/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2763 - acc: 0.4348 - val_loss: 2.7748 - val_acc: 0.3296\n",
      "Epoch 47/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2577 - acc: 0.4378 - val_loss: 2.7996 - val_acc: 0.3324\n",
      "Epoch 48/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2630 - acc: 0.4378 - val_loss: 2.8226 - val_acc: 0.3279\n",
      "Epoch 49/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2646 - acc: 0.4401 - val_loss: 2.8249 - val_acc: 0.3260\n",
      "Epoch 50/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2624 - acc: 0.4382 - val_loss: 2.8414 - val_acc: 0.3292\n",
      "Epoch 51/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2532 - acc: 0.4399 - val_loss: 2.8604 - val_acc: 0.3181\n",
      "Epoch 52/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2528 - acc: 0.4411 - val_loss: 2.8833 - val_acc: 0.3211\n",
      "Epoch 53/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2625 - acc: 0.4389 - val_loss: 2.8924 - val_acc: 0.3193\n",
      "Epoch 54/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2504 - acc: 0.4394 - val_loss: 2.9203 - val_acc: 0.3196\n",
      "Epoch 55/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2517 - acc: 0.4393 - val_loss: 2.9296 - val_acc: 0.3140\n",
      "Epoch 56/100\n",
      "47454/47454 [==============================] - 7s 148us/step - loss: 2.2475 - acc: 0.4421 - val_loss: 2.9554 - val_acc: 0.3101\n",
      "Epoch 57/100\n",
      "47454/47454 [==============================] - 7s 152us/step - loss: 2.2652 - acc: 0.4379 - val_loss: 2.9696 - val_acc: 0.3114\n",
      "Epoch 58/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2492 - acc: 0.4417 - val_loss: 2.9969 - val_acc: 0.3078\n",
      "Epoch 59/100\n",
      "47454/47454 [==============================] - 7s 149us/step - loss: 2.2403 - acc: 0.4444 - val_loss: 3.0061 - val_acc: 0.3048\n",
      "Epoch 60/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2413 - acc: 0.4426 - val_loss: 3.0259 - val_acc: 0.3058\n",
      "Epoch 61/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2474 - acc: 0.4413 - val_loss: 3.0371 - val_acc: 0.3035\n",
      "Epoch 62/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2421 - acc: 0.4411 - val_loss: 3.0453 - val_acc: 0.3001\n",
      "Epoch 63/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2514 - acc: 0.4404 - val_loss: 3.0581 - val_acc: 0.2951\n",
      "Epoch 64/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2424 - acc: 0.4434 - val_loss: 3.0967 - val_acc: 0.2900\n",
      "Epoch 65/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2462 - acc: 0.4443 - val_loss: 3.1101 - val_acc: 0.2866\n",
      "Epoch 66/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2370 - acc: 0.4406 - val_loss: 3.1315 - val_acc: 0.2825\n",
      "Epoch 67/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2398 - acc: 0.4426 - val_loss: 3.1358 - val_acc: 0.2878\n",
      "Epoch 68/100\n",
      "47454/47454 [==============================] - 7s 149us/step - loss: 2.2403 - acc: 0.4431 - val_loss: 3.1595 - val_acc: 0.2819\n",
      "Epoch 69/100\n",
      "47454/47454 [==============================] - 7s 149us/step - loss: 2.2329 - acc: 0.4438 - val_loss: 3.1918 - val_acc: 0.2808\n",
      "Epoch 70/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2381 - acc: 0.4454 - val_loss: 3.2123 - val_acc: 0.2744\n",
      "Epoch 71/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2377 - acc: 0.4417 - val_loss: 3.2116 - val_acc: 0.2767\n",
      "Epoch 72/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2401 - acc: 0.4417 - val_loss: 3.2256 - val_acc: 0.2757\n",
      "Epoch 73/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2329 - acc: 0.4461 - val_loss: 3.2589 - val_acc: 0.2741\n",
      "Epoch 74/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2233 - acc: 0.4473 - val_loss: 3.2693 - val_acc: 0.2712\n",
      "Epoch 75/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2197 - acc: 0.4472 - val_loss: 3.2877 - val_acc: 0.2667\n",
      "Epoch 76/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2326 - acc: 0.4433 - val_loss: 3.3058 - val_acc: 0.2589\n",
      "Epoch 77/100\n",
      "47454/47454 [==============================] - 7s 149us/step - loss: 2.2251 - acc: 0.4458 - val_loss: 3.3027 - val_acc: 0.2617\n",
      "Epoch 78/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2145 - acc: 0.4488 - val_loss: 3.3244 - val_acc: 0.2607\n",
      "Epoch 79/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2170 - acc: 0.4452 - val_loss: 3.3622 - val_acc: 0.2600\n",
      "Epoch 80/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2284 - acc: 0.4442 - val_loss: 3.3668 - val_acc: 0.2581\n",
      "Epoch 81/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2250 - acc: 0.4443 - val_loss: 3.3941 - val_acc: 0.2557\n",
      "Epoch 82/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2164 - acc: 0.4490 - val_loss: 3.4081 - val_acc: 0.2547\n",
      "Epoch 83/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2199 - acc: 0.4476 - val_loss: 3.4277 - val_acc: 0.2555\n",
      "Epoch 84/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2178 - acc: 0.4466 - val_loss: 3.4161 - val_acc: 0.2523\n",
      "Epoch 85/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2110 - acc: 0.4475 - val_loss: 3.4522 - val_acc: 0.2514\n",
      "Epoch 86/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2183 - acc: 0.4465 - val_loss: 3.4650 - val_acc: 0.2484\n",
      "Epoch 87/100\n",
      "47454/47454 [==============================] - 7s 149us/step - loss: 2.2109 - acc: 0.4491 - val_loss: 3.4737 - val_acc: 0.2485\n",
      "Epoch 88/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2152 - acc: 0.4465 - val_loss: 3.4889 - val_acc: 0.2472\n",
      "Epoch 89/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2114 - acc: 0.4503 - val_loss: 3.5101 - val_acc: 0.2448\n",
      "Epoch 90/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2186 - acc: 0.4476 - val_loss: 3.5031 - val_acc: 0.2454\n",
      "Epoch 91/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2053 - acc: 0.4512 - val_loss: 3.5460 - val_acc: 0.2439\n",
      "Epoch 92/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2118 - acc: 0.4494 - val_loss: 3.5331 - val_acc: 0.2407\n",
      "Epoch 93/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2129 - acc: 0.4467 - val_loss: 3.5559 - val_acc: 0.2407\n",
      "Epoch 94/100\n",
      "47454/47454 [==============================] - 7s 151us/step - loss: 2.2095 - acc: 0.4517 - val_loss: 3.5716 - val_acc: 0.2395\n",
      "Epoch 95/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2087 - acc: 0.4473 - val_loss: 3.5913 - val_acc: 0.2324\n",
      "Epoch 96/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2212 - acc: 0.4460 - val_loss: 3.5825 - val_acc: 0.2337\n",
      "Epoch 97/100\n",
      "47454/47454 [==============================] - 7s 149us/step - loss: 2.2011 - acc: 0.4490 - val_loss: 3.6240 - val_acc: 0.2335\n",
      "Epoch 98/100\n",
      "47454/47454 [==============================] - 7s 149us/step - loss: 2.2006 - acc: 0.4500 - val_loss: 3.6365 - val_acc: 0.2275\n",
      "Epoch 99/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.2026 - acc: 0.4505 - val_loss: 3.6454 - val_acc: 0.2298\n",
      "Epoch 100/100\n",
      "47454/47454 [==============================] - 7s 150us/step - loss: 2.1963 - acc: 0.4506 - val_loss: 3.6688 - val_acc: 0.2341\n",
      "5331/5331 [==============================] - 0s 34us/step\n",
      "[3.668790863996329, 0.2341024198422086]\n",
      "\n",
      "Fold  4\n",
      "Train on 47478 samples, validate on 5307 samples\n",
      "Epoch 1/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.5035 - acc: 0.4109 - val_loss: 1.3492 - val_acc: 0.6908\n",
      "Epoch 2/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.4488 - acc: 0.4172 - val_loss: 1.4010 - val_acc: 0.6678\n",
      "Epoch 3/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.4116 - acc: 0.4202 - val_loss: 1.4527 - val_acc: 0.6493\n",
      "Epoch 4/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.3830 - acc: 0.4234 - val_loss: 1.5053 - val_acc: 0.6346\n",
      "Epoch 5/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.3640 - acc: 0.4242 - val_loss: 1.5416 - val_acc: 0.6265\n",
      "Epoch 6/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.3470 - acc: 0.4258 - val_loss: 1.5898 - val_acc: 0.6090\n",
      "Epoch 7/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.3384 - acc: 0.4266 - val_loss: 1.6166 - val_acc: 0.5979\n",
      "Epoch 8/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.3289 - acc: 0.4279 - val_loss: 1.6599 - val_acc: 0.5838\n",
      "Epoch 9/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.3179 - acc: 0.4308 - val_loss: 1.7118 - val_acc: 0.5711\n",
      "Epoch 10/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.3129 - acc: 0.4304 - val_loss: 1.7305 - val_acc: 0.5611\n",
      "Epoch 11/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.3009 - acc: 0.4328 - val_loss: 1.7640 - val_acc: 0.5576\n",
      "Epoch 12/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2948 - acc: 0.4328 - val_loss: 1.7923 - val_acc: 0.5448\n",
      "Epoch 13/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2912 - acc: 0.4323 - val_loss: 1.8352 - val_acc: 0.5331\n",
      "Epoch 14/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.2900 - acc: 0.4344 - val_loss: 1.8587 - val_acc: 0.5306\n",
      "Epoch 15/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2843 - acc: 0.4333 - val_loss: 1.8986 - val_acc: 0.5187\n",
      "Epoch 16/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.2814 - acc: 0.4350 - val_loss: 1.9271 - val_acc: 0.5073\n",
      "Epoch 17/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.2631 - acc: 0.4359 - val_loss: 1.9517 - val_acc: 0.4992\n",
      "Epoch 18/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2651 - acc: 0.4379 - val_loss: 1.9801 - val_acc: 0.4973\n",
      "Epoch 19/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2620 - acc: 0.4407 - val_loss: 2.0140 - val_acc: 0.4944\n",
      "Epoch 20/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2502 - acc: 0.4399 - val_loss: 2.0275 - val_acc: 0.4850\n",
      "Epoch 21/100\n",
      "47478/47478 [==============================] - 7s 152us/step - loss: 2.2595 - acc: 0.4362 - val_loss: 2.0545 - val_acc: 0.4811\n",
      "Epoch 22/100\n",
      "47478/47478 [==============================] - 7s 152us/step - loss: 2.2556 - acc: 0.4383 - val_loss: 2.0828 - val_acc: 0.4756\n",
      "Epoch 23/100\n",
      "47478/47478 [==============================] - 7s 152us/step - loss: 2.2528 - acc: 0.4410 - val_loss: 2.0943 - val_acc: 0.4747\n",
      "Epoch 24/100\n",
      "47478/47478 [==============================] - 7s 152us/step - loss: 2.2570 - acc: 0.4359 - val_loss: 2.1289 - val_acc: 0.4658\n",
      "Epoch 25/100\n",
      "47478/47478 [==============================] - 7s 153us/step - loss: 2.2471 - acc: 0.4417 - val_loss: 2.1562 - val_acc: 0.4556\n",
      "Epoch 26/100\n",
      "47478/47478 [==============================] - 7s 151us/step - loss: 2.2517 - acc: 0.4388 - val_loss: 2.1736 - val_acc: 0.4483\n",
      "Epoch 27/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2493 - acc: 0.4384 - val_loss: 2.1946 - val_acc: 0.4505\n",
      "Epoch 28/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.2443 - acc: 0.4412 - val_loss: 2.2232 - val_acc: 0.4466\n",
      "Epoch 29/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2466 - acc: 0.4426 - val_loss: 2.2464 - val_acc: 0.4413\n",
      "Epoch 30/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2339 - acc: 0.4413 - val_loss: 2.2604 - val_acc: 0.4345\n",
      "Epoch 31/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2336 - acc: 0.4444 - val_loss: 2.2834 - val_acc: 0.4302\n",
      "Epoch 32/100\n",
      "47478/47478 [==============================] - 7s 151us/step - loss: 2.2293 - acc: 0.4422 - val_loss: 2.3036 - val_acc: 0.4275\n",
      "Epoch 33/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2333 - acc: 0.4427 - val_loss: 2.3245 - val_acc: 0.4234\n",
      "Epoch 34/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2294 - acc: 0.4443 - val_loss: 2.3499 - val_acc: 0.4138\n",
      "Epoch 35/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2233 - acc: 0.4435 - val_loss: 2.3760 - val_acc: 0.4134\n",
      "Epoch 36/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2214 - acc: 0.4446 - val_loss: 2.3930 - val_acc: 0.4074\n",
      "Epoch 37/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2301 - acc: 0.4453 - val_loss: 2.4062 - val_acc: 0.3993\n",
      "Epoch 38/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.2177 - acc: 0.4455 - val_loss: 2.4279 - val_acc: 0.3991\n",
      "Epoch 39/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2302 - acc: 0.4426 - val_loss: 2.4555 - val_acc: 0.3936\n",
      "Epoch 40/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2280 - acc: 0.4448 - val_loss: 2.4747 - val_acc: 0.3929\n",
      "Epoch 41/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2067 - acc: 0.4467 - val_loss: 2.4795 - val_acc: 0.3908\n",
      "Epoch 42/100\n",
      "47478/47478 [==============================] - 8s 172us/step - loss: 2.2141 - acc: 0.4452 - val_loss: 2.5011 - val_acc: 0.3887\n",
      "Epoch 43/100\n",
      "47478/47478 [==============================] - 9s 187us/step - loss: 2.2097 - acc: 0.4494 - val_loss: 2.5195 - val_acc: 0.3787\n",
      "Epoch 44/100\n",
      "47478/47478 [==============================] - 8s 176us/step - loss: 2.2155 - acc: 0.4481 - val_loss: 2.5446 - val_acc: 0.3823\n",
      "Epoch 45/100\n",
      "47478/47478 [==============================] - 8s 158us/step - loss: 2.2103 - acc: 0.4472 - val_loss: 2.5516 - val_acc: 0.3844\n",
      "Epoch 46/100\n",
      "47478/47478 [==============================] - 8s 160us/step - loss: 2.2134 - acc: 0.4472 - val_loss: 2.5721 - val_acc: 0.3712\n",
      "Epoch 47/100\n",
      "47478/47478 [==============================] - 8s 159us/step - loss: 2.2055 - acc: 0.4483 - val_loss: 2.5851 - val_acc: 0.3729\n",
      "Epoch 48/100\n",
      "47478/47478 [==============================] - 8s 160us/step - loss: 2.2160 - acc: 0.4446 - val_loss: 2.5969 - val_acc: 0.3686\n",
      "Epoch 49/100\n",
      "47478/47478 [==============================] - 7s 156us/step - loss: 2.2106 - acc: 0.4470 - val_loss: 2.6111 - val_acc: 0.3642\n",
      "Epoch 50/100\n",
      "47478/47478 [==============================] - 8s 162us/step - loss: 2.1997 - acc: 0.4483 - val_loss: 2.6307 - val_acc: 0.3633\n",
      "Epoch 51/100\n",
      "47478/47478 [==============================] - 8s 159us/step - loss: 2.1966 - acc: 0.4491 - val_loss: 2.6449 - val_acc: 0.3607\n",
      "Epoch 52/100\n",
      "47478/47478 [==============================] - 8s 161us/step - loss: 2.2052 - acc: 0.4474 - val_loss: 2.6630 - val_acc: 0.3559\n",
      "Epoch 53/100\n",
      "47478/47478 [==============================] - 8s 159us/step - loss: 2.2064 - acc: 0.4468 - val_loss: 2.6869 - val_acc: 0.3507\n",
      "Epoch 54/100\n",
      "47478/47478 [==============================] - 7s 148us/step - loss: 2.2001 - acc: 0.4496 - val_loss: 2.6932 - val_acc: 0.3495\n",
      "Epoch 55/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1934 - acc: 0.4516 - val_loss: 2.7145 - val_acc: 0.3467\n",
      "Epoch 56/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.2050 - acc: 0.4493 - val_loss: 2.7369 - val_acc: 0.3478\n",
      "Epoch 57/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2041 - acc: 0.4499 - val_loss: 2.7495 - val_acc: 0.3469\n",
      "Epoch 58/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.2020 - acc: 0.4474 - val_loss: 2.7505 - val_acc: 0.3358\n",
      "Epoch 59/100\n",
      "47478/47478 [==============================] - 7s 148us/step - loss: 2.1903 - acc: 0.4536 - val_loss: 2.7887 - val_acc: 0.3326\n",
      "Epoch 60/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1916 - acc: 0.4501 - val_loss: 2.7828 - val_acc: 0.3369\n",
      "Epoch 61/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1987 - acc: 0.4519 - val_loss: 2.8095 - val_acc: 0.3264\n",
      "Epoch 62/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1882 - acc: 0.4533 - val_loss: 2.8289 - val_acc: 0.3247\n",
      "Epoch 63/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1847 - acc: 0.4541 - val_loss: 2.8343 - val_acc: 0.3273\n",
      "Epoch 64/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1933 - acc: 0.4508 - val_loss: 2.8629 - val_acc: 0.3198\n",
      "Epoch 65/100\n",
      "47478/47478 [==============================] - 7s 148us/step - loss: 2.1857 - acc: 0.4533 - val_loss: 2.8699 - val_acc: 0.3224\n",
      "Epoch 66/100\n",
      "47478/47478 [==============================] - 7s 148us/step - loss: 2.1867 - acc: 0.4522 - val_loss: 2.9017 - val_acc: 0.3168\n",
      "Epoch 67/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1867 - acc: 0.4502 - val_loss: 2.9221 - val_acc: 0.3166\n",
      "Epoch 68/100\n",
      "47478/47478 [==============================] - 7s 148us/step - loss: 2.1828 - acc: 0.4504 - val_loss: 2.9274 - val_acc: 0.3166\n",
      "Epoch 69/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1727 - acc: 0.4556 - val_loss: 2.9346 - val_acc: 0.3168\n",
      "Epoch 70/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1723 - acc: 0.4523 - val_loss: 2.9497 - val_acc: 0.3098\n",
      "Epoch 71/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1825 - acc: 0.4511 - val_loss: 2.9542 - val_acc: 0.3103\n",
      "Epoch 72/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1594 - acc: 0.4554 - val_loss: 2.9964 - val_acc: 0.3085\n",
      "Epoch 73/100\n",
      "47478/47478 [==============================] - 7s 148us/step - loss: 2.1841 - acc: 0.4506 - val_loss: 3.0084 - val_acc: 0.3092\n",
      "Epoch 74/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1792 - acc: 0.4528 - val_loss: 3.0238 - val_acc: 0.3007\n",
      "Epoch 75/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1791 - acc: 0.4523 - val_loss: 3.0198 - val_acc: 0.3004\n",
      "Epoch 76/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1750 - acc: 0.4504 - val_loss: 3.0510 - val_acc: 0.2975\n",
      "Epoch 77/100\n",
      "47478/47478 [==============================] - 7s 151us/step - loss: 2.1730 - acc: 0.4540 - val_loss: 3.0640 - val_acc: 0.2913\n",
      "Epoch 78/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1700 - acc: 0.4551 - val_loss: 3.0631 - val_acc: 0.2930\n",
      "Epoch 79/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1683 - acc: 0.4567 - val_loss: 3.0782 - val_acc: 0.2987\n",
      "Epoch 80/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1672 - acc: 0.4538 - val_loss: 3.0978 - val_acc: 0.2877\n",
      "Epoch 81/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1804 - acc: 0.4538 - val_loss: 3.1018 - val_acc: 0.2949\n",
      "Epoch 82/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1723 - acc: 0.4553 - val_loss: 3.1118 - val_acc: 0.2858\n",
      "Epoch 83/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1677 - acc: 0.4540 - val_loss: 3.1302 - val_acc: 0.2862\n",
      "Epoch 84/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1760 - acc: 0.4539 - val_loss: 3.1409 - val_acc: 0.2857\n",
      "Epoch 85/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1636 - acc: 0.4581 - val_loss: 3.1562 - val_acc: 0.2811\n",
      "Epoch 86/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1649 - acc: 0.4579 - val_loss: 3.1811 - val_acc: 0.2840\n",
      "Epoch 87/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1711 - acc: 0.4526 - val_loss: 3.1947 - val_acc: 0.2740\n",
      "Epoch 88/100\n",
      "47478/47478 [==============================] - 7s 151us/step - loss: 2.1733 - acc: 0.4543 - val_loss: 3.1682 - val_acc: 0.2789\n",
      "Epoch 89/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1635 - acc: 0.4554 - val_loss: 3.1956 - val_acc: 0.2815\n",
      "Epoch 90/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1605 - acc: 0.4579 - val_loss: 3.2310 - val_acc: 0.2793\n",
      "Epoch 91/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1598 - acc: 0.4550 - val_loss: 3.2474 - val_acc: 0.2755\n",
      "Epoch 92/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1608 - acc: 0.4549 - val_loss: 3.2353 - val_acc: 0.2742\n",
      "Epoch 93/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1674 - acc: 0.4553 - val_loss: 3.2572 - val_acc: 0.2736\n",
      "Epoch 94/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1598 - acc: 0.4541 - val_loss: 3.2710 - val_acc: 0.2685\n",
      "Epoch 95/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1537 - acc: 0.4576 - val_loss: 3.3042 - val_acc: 0.2636\n",
      "Epoch 96/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1659 - acc: 0.4565 - val_loss: 3.3255 - val_acc: 0.2676\n",
      "Epoch 97/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1620 - acc: 0.4570 - val_loss: 3.3033 - val_acc: 0.2674\n",
      "Epoch 98/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1503 - acc: 0.4570 - val_loss: 3.3365 - val_acc: 0.2657\n",
      "Epoch 99/100\n",
      "47478/47478 [==============================] - 7s 150us/step - loss: 2.1671 - acc: 0.4543 - val_loss: 3.3411 - val_acc: 0.2581\n",
      "Epoch 100/100\n",
      "47478/47478 [==============================] - 7s 149us/step - loss: 2.1630 - acc: 0.4556 - val_loss: 3.3707 - val_acc: 0.2600\n",
      "5307/5307 [==============================] - 0s 34us/step\n",
      "[3.3707021619273103, 0.26003391743941745]\n",
      "\n",
      "Fold  5\n",
      "Train on 47521 samples, validate on 5264 samples\n",
      "Epoch 1/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.4314 - acc: 0.4230 - val_loss: 1.2689 - val_acc: 0.6989\n",
      "Epoch 2/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.3866 - acc: 0.4246 - val_loss: 1.3304 - val_acc: 0.6761\n",
      "Epoch 3/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.3555 - acc: 0.4266 - val_loss: 1.3886 - val_acc: 0.6647\n",
      "Epoch 4/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.3265 - acc: 0.4323 - val_loss: 1.4363 - val_acc: 0.6497\n",
      "Epoch 5/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.2993 - acc: 0.4345 - val_loss: 1.4891 - val_acc: 0.6364\n",
      "Epoch 6/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.2974 - acc: 0.4368 - val_loss: 1.5222 - val_acc: 0.6204\n",
      "Epoch 7/100\n",
      "47521/47521 [==============================] - 7s 152us/step - loss: 2.2895 - acc: 0.4360 - val_loss: 1.5676 - val_acc: 0.6117\n",
      "Epoch 8/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.2732 - acc: 0.4383 - val_loss: 1.5997 - val_acc: 0.6028\n",
      "Epoch 9/100\n",
      "47521/47521 [==============================] - 7s 153us/step - loss: 2.2608 - acc: 0.4386 - val_loss: 1.6222 - val_acc: 0.5878\n",
      "Epoch 10/100\n",
      "47521/47521 [==============================] - 7s 152us/step - loss: 2.2565 - acc: 0.4374 - val_loss: 1.6504 - val_acc: 0.5838\n",
      "Epoch 11/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.2539 - acc: 0.4432 - val_loss: 1.6969 - val_acc: 0.5729\n",
      "Epoch 12/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.2337 - acc: 0.4437 - val_loss: 1.7257 - val_acc: 0.5589\n",
      "Epoch 13/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.2461 - acc: 0.4410 - val_loss: 1.7528 - val_acc: 0.5559\n",
      "Epoch 14/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.2386 - acc: 0.4420 - val_loss: 1.7842 - val_acc: 0.5488\n",
      "Epoch 15/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.2274 - acc: 0.4428 - val_loss: 1.8094 - val_acc: 0.5376\n",
      "Epoch 16/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.2418 - acc: 0.4378 - val_loss: 1.8435 - val_acc: 0.5321\n",
      "Epoch 17/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.2302 - acc: 0.4418 - val_loss: 1.8664 - val_acc: 0.5222\n",
      "Epoch 18/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.2234 - acc: 0.4426 - val_loss: 1.8963 - val_acc: 0.5213\n",
      "Epoch 19/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.2299 - acc: 0.4438 - val_loss: 1.9130 - val_acc: 0.5146\n",
      "Epoch 20/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.2273 - acc: 0.4426 - val_loss: 1.9336 - val_acc: 0.5093\n",
      "Epoch 21/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.2033 - acc: 0.4500 - val_loss: 1.9649 - val_acc: 0.4992\n",
      "Epoch 22/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.2085 - acc: 0.4477 - val_loss: 1.9927 - val_acc: 0.4915\n",
      "Epoch 23/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.2111 - acc: 0.4463 - val_loss: 2.0147 - val_acc: 0.4894\n",
      "Epoch 24/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.2002 - acc: 0.4482 - val_loss: 2.0340 - val_acc: 0.4867\n",
      "Epoch 25/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.2021 - acc: 0.4497 - val_loss: 2.0613 - val_acc: 0.4791\n",
      "Epoch 26/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.2054 - acc: 0.4477 - val_loss: 2.0750 - val_acc: 0.4738\n",
      "Epoch 27/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.2018 - acc: 0.4438 - val_loss: 2.1022 - val_acc: 0.4675\n",
      "Epoch 28/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1983 - acc: 0.4518 - val_loss: 2.1232 - val_acc: 0.4641\n",
      "Epoch 29/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1891 - acc: 0.4524 - val_loss: 2.1500 - val_acc: 0.4554\n",
      "Epoch 30/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1931 - acc: 0.4514 - val_loss: 2.1656 - val_acc: 0.4508\n",
      "Epoch 31/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1857 - acc: 0.4511 - val_loss: 2.1886 - val_acc: 0.4478\n",
      "Epoch 32/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1969 - acc: 0.4505 - val_loss: 2.2091 - val_acc: 0.4464\n",
      "Epoch 33/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1874 - acc: 0.4530 - val_loss: 2.2316 - val_acc: 0.4369\n",
      "Epoch 34/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1929 - acc: 0.4482 - val_loss: 2.2505 - val_acc: 0.4335\n",
      "Epoch 35/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1952 - acc: 0.4503 - val_loss: 2.2702 - val_acc: 0.4316\n",
      "Epoch 36/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1830 - acc: 0.4506 - val_loss: 2.2877 - val_acc: 0.4246\n",
      "Epoch 37/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1760 - acc: 0.4520 - val_loss: 2.3142 - val_acc: 0.4191\n",
      "Epoch 38/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1730 - acc: 0.4542 - val_loss: 2.3188 - val_acc: 0.4181\n",
      "Epoch 39/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1776 - acc: 0.4549 - val_loss: 2.3390 - val_acc: 0.4143\n",
      "Epoch 40/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1854 - acc: 0.4532 - val_loss: 2.3595 - val_acc: 0.4115\n",
      "Epoch 41/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1718 - acc: 0.4561 - val_loss: 2.3768 - val_acc: 0.4077\n",
      "Epoch 42/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1763 - acc: 0.4553 - val_loss: 2.4070 - val_acc: 0.3999\n",
      "Epoch 43/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1700 - acc: 0.4538 - val_loss: 2.4145 - val_acc: 0.3986\n",
      "Epoch 44/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1664 - acc: 0.4537 - val_loss: 2.4304 - val_acc: 0.3974\n",
      "Epoch 45/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1780 - acc: 0.4538 - val_loss: 2.4519 - val_acc: 0.3927\n",
      "Epoch 46/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1750 - acc: 0.4540 - val_loss: 2.4566 - val_acc: 0.3944\n",
      "Epoch 47/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1698 - acc: 0.4566 - val_loss: 2.4869 - val_acc: 0.3894\n",
      "Epoch 48/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1573 - acc: 0.4553 - val_loss: 2.5010 - val_acc: 0.3851\n",
      "Epoch 49/100\n",
      "47521/47521 [==============================] - 7s 152us/step - loss: 2.1562 - acc: 0.4561 - val_loss: 2.5196 - val_acc: 0.3832\n",
      "Epoch 50/100\n",
      "47521/47521 [==============================] - 7s 152us/step - loss: 2.1564 - acc: 0.4583 - val_loss: 2.5374 - val_acc: 0.3731\n",
      "Epoch 51/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1727 - acc: 0.4534 - val_loss: 2.5444 - val_acc: 0.3731\n",
      "Epoch 52/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1667 - acc: 0.4527 - val_loss: 2.5579 - val_acc: 0.3737\n",
      "Epoch 53/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1568 - acc: 0.4535 - val_loss: 2.5665 - val_acc: 0.3703\n",
      "Epoch 54/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1590 - acc: 0.4559 - val_loss: 2.5875 - val_acc: 0.3722\n",
      "Epoch 55/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1570 - acc: 0.4563 - val_loss: 2.6041 - val_acc: 0.3693\n",
      "Epoch 56/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.1545 - acc: 0.4583 - val_loss: 2.6328 - val_acc: 0.3644\n",
      "Epoch 57/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1573 - acc: 0.4581 - val_loss: 2.6468 - val_acc: 0.3636\n",
      "Epoch 58/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1568 - acc: 0.4553 - val_loss: 2.6537 - val_acc: 0.3630\n",
      "Epoch 59/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1463 - acc: 0.4595 - val_loss: 2.6549 - val_acc: 0.3611\n",
      "Epoch 60/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1571 - acc: 0.4567 - val_loss: 2.6761 - val_acc: 0.3570\n",
      "Epoch 61/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1465 - acc: 0.4599 - val_loss: 2.6914 - val_acc: 0.3513\n",
      "Epoch 62/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1498 - acc: 0.4583 - val_loss: 2.7181 - val_acc: 0.3535\n",
      "Epoch 63/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.1456 - acc: 0.4595 - val_loss: 2.7297 - val_acc: 0.3505\n",
      "Epoch 64/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.1498 - acc: 0.4597 - val_loss: 2.7564 - val_acc: 0.3459\n",
      "Epoch 65/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1512 - acc: 0.4583 - val_loss: 2.7713 - val_acc: 0.3486\n",
      "Epoch 66/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1492 - acc: 0.4572 - val_loss: 2.7746 - val_acc: 0.3372\n",
      "Epoch 67/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1444 - acc: 0.4625 - val_loss: 2.7839 - val_acc: 0.3393\n",
      "Epoch 68/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1569 - acc: 0.4557 - val_loss: 2.8103 - val_acc: 0.3347\n",
      "Epoch 69/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1342 - acc: 0.4608 - val_loss: 2.8050 - val_acc: 0.3381\n",
      "Epoch 70/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1436 - acc: 0.4623 - val_loss: 2.8271 - val_acc: 0.3321\n",
      "Epoch 71/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1438 - acc: 0.4608 - val_loss: 2.8324 - val_acc: 0.3258\n",
      "Epoch 72/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1400 - acc: 0.4602 - val_loss: 2.8635 - val_acc: 0.3233\n",
      "Epoch 73/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1423 - acc: 0.4582 - val_loss: 2.8811 - val_acc: 0.3226\n",
      "Epoch 74/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1428 - acc: 0.4604 - val_loss: 2.8940 - val_acc: 0.3197\n",
      "Epoch 75/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1397 - acc: 0.4611 - val_loss: 2.8948 - val_acc: 0.3214\n",
      "Epoch 76/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1457 - acc: 0.4599 - val_loss: 2.9076 - val_acc: 0.3155\n",
      "Epoch 77/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1451 - acc: 0.4574 - val_loss: 2.9411 - val_acc: 0.3131\n",
      "Epoch 78/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1280 - acc: 0.4639 - val_loss: 2.9493 - val_acc: 0.3133\n",
      "Epoch 79/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1322 - acc: 0.4632 - val_loss: 2.9824 - val_acc: 0.3108\n",
      "Epoch 80/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1455 - acc: 0.4590 - val_loss: 2.9931 - val_acc: 0.3097\n",
      "Epoch 81/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1433 - acc: 0.4572 - val_loss: 2.9847 - val_acc: 0.3157\n",
      "Epoch 82/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1361 - acc: 0.4598 - val_loss: 3.0199 - val_acc: 0.3129\n",
      "Epoch 83/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1374 - acc: 0.4599 - val_loss: 3.0268 - val_acc: 0.3053\n",
      "Epoch 84/100\n",
      "47521/47521 [==============================] - 7s 151us/step - loss: 2.1320 - acc: 0.4595 - val_loss: 3.0345 - val_acc: 0.3060\n",
      "Epoch 85/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1292 - acc: 0.4631 - val_loss: 3.0323 - val_acc: 0.3005\n",
      "Epoch 86/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1335 - acc: 0.4605 - val_loss: 3.0699 - val_acc: 0.3021\n",
      "Epoch 87/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1293 - acc: 0.4623 - val_loss: 3.0889 - val_acc: 0.2983\n",
      "Epoch 88/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1330 - acc: 0.4605 - val_loss: 3.0762 - val_acc: 0.2973\n",
      "Epoch 89/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1287 - acc: 0.4656 - val_loss: 3.0993 - val_acc: 0.2924\n",
      "Epoch 90/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1228 - acc: 0.4612 - val_loss: 3.1089 - val_acc: 0.2950\n",
      "Epoch 91/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1402 - acc: 0.4607 - val_loss: 3.1274 - val_acc: 0.2952\n",
      "Epoch 92/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1362 - acc: 0.4586 - val_loss: 3.1226 - val_acc: 0.2891\n",
      "Epoch 93/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1274 - acc: 0.4644 - val_loss: 3.1362 - val_acc: 0.2924\n",
      "Epoch 94/100\n",
      "47521/47521 [==============================] - 7s 148us/step - loss: 2.1238 - acc: 0.4616 - val_loss: 3.1456 - val_acc: 0.2910\n",
      "Epoch 95/100\n",
      "47521/47521 [==============================] - 7s 153us/step - loss: 2.1228 - acc: 0.4640 - val_loss: 3.1541 - val_acc: 0.2907\n",
      "Epoch 96/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1236 - acc: 0.4606 - val_loss: 3.1596 - val_acc: 0.2855\n",
      "Epoch 97/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1232 - acc: 0.4648 - val_loss: 3.1827 - val_acc: 0.2853\n",
      "Epoch 98/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1176 - acc: 0.4642 - val_loss: 3.2032 - val_acc: 0.2834\n",
      "Epoch 99/100\n",
      "47521/47521 [==============================] - 7s 149us/step - loss: 2.1273 - acc: 0.4637 - val_loss: 3.1969 - val_acc: 0.2817\n",
      "Epoch 100/100\n",
      "47521/47521 [==============================] - 7s 150us/step - loss: 2.1227 - acc: 0.4645 - val_loss: 3.2237 - val_acc: 0.2798\n",
      "5264/5264 [==============================] - 0s 34us/step\n",
      "[3.2236951049700333, 0.2798252279635258]\n",
      "\n",
      "Fold  6\n",
      "Train on 47562 samples, validate on 5223 samples\n",
      "Epoch 1/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.3969 - acc: 0.4262 - val_loss: 1.2087 - val_acc: 0.7174\n",
      "Epoch 2/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.3410 - acc: 0.4334 - val_loss: 1.2515 - val_acc: 0.6977\n",
      "Epoch 3/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.3178 - acc: 0.4363 - val_loss: 1.3066 - val_acc: 0.6785\n",
      "Epoch 4/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.3010 - acc: 0.4341 - val_loss: 1.3392 - val_acc: 0.6690\n",
      "Epoch 5/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.2806 - acc: 0.4368 - val_loss: 1.3865 - val_acc: 0.6477\n",
      "Epoch 6/100\n",
      "47562/47562 [==============================] - 7s 147us/step - loss: 2.2670 - acc: 0.4393 - val_loss: 1.4408 - val_acc: 0.6322\n",
      "Epoch 7/100\n",
      "47562/47562 [==============================] - 7s 147us/step - loss: 2.2474 - acc: 0.4411 - val_loss: 1.4745 - val_acc: 0.6236\n",
      "Epoch 8/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.2231 - acc: 0.4449 - val_loss: 1.4914 - val_acc: 0.6207\n",
      "Epoch 9/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.2329 - acc: 0.4425 - val_loss: 1.5282 - val_acc: 0.6140\n",
      "Epoch 10/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.2224 - acc: 0.4454 - val_loss: 1.5599 - val_acc: 0.6033\n",
      "Epoch 11/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.2274 - acc: 0.4463 - val_loss: 1.5958 - val_acc: 0.5897\n",
      "Epoch 12/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.2122 - acc: 0.4451 - val_loss: 1.6266 - val_acc: 0.5818\n",
      "Epoch 13/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.2048 - acc: 0.4476 - val_loss: 1.6474 - val_acc: 0.5786\n",
      "Epoch 14/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.2021 - acc: 0.4503 - val_loss: 1.6739 - val_acc: 0.5683\n",
      "Epoch 15/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1987 - acc: 0.4479 - val_loss: 1.7004 - val_acc: 0.5581\n",
      "Epoch 16/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.2066 - acc: 0.4486 - val_loss: 1.7269 - val_acc: 0.5593\n",
      "Epoch 17/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.2031 - acc: 0.4492 - val_loss: 1.7531 - val_acc: 0.5524\n",
      "Epoch 18/100\n",
      "47562/47562 [==============================] - 7s 147us/step - loss: 2.1950 - acc: 0.4503 - val_loss: 1.7848 - val_acc: 0.5399\n",
      "Epoch 19/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1925 - acc: 0.4488 - val_loss: 1.8091 - val_acc: 0.5411\n",
      "Epoch 20/100\n",
      "47562/47562 [==============================] - 7s 147us/step - loss: 2.1828 - acc: 0.4515 - val_loss: 1.8245 - val_acc: 0.5363\n",
      "Epoch 21/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.1804 - acc: 0.4519 - val_loss: 1.8511 - val_acc: 0.5263\n",
      "Epoch 22/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.1782 - acc: 0.4510 - val_loss: 1.8753 - val_acc: 0.5181\n",
      "Epoch 23/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1723 - acc: 0.4537 - val_loss: 1.8969 - val_acc: 0.5146\n",
      "Epoch 24/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1842 - acc: 0.4526 - val_loss: 1.9092 - val_acc: 0.5152\n",
      "Epoch 25/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1831 - acc: 0.4540 - val_loss: 1.9303 - val_acc: 0.5095\n",
      "Epoch 26/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1645 - acc: 0.4531 - val_loss: 1.9598 - val_acc: 0.4982\n",
      "Epoch 27/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.1670 - acc: 0.4530 - val_loss: 1.9758 - val_acc: 0.4966\n",
      "Epoch 28/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1559 - acc: 0.4565 - val_loss: 1.9938 - val_acc: 0.4953\n",
      "Epoch 29/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1743 - acc: 0.4548 - val_loss: 2.0162 - val_acc: 0.4844\n",
      "Epoch 30/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1650 - acc: 0.4553 - val_loss: 2.0379 - val_acc: 0.4813\n",
      "Epoch 31/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1706 - acc: 0.4535 - val_loss: 2.0499 - val_acc: 0.4842\n",
      "Epoch 32/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1662 - acc: 0.4540 - val_loss: 2.0786 - val_acc: 0.4752\n",
      "Epoch 33/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.1586 - acc: 0.4574 - val_loss: 2.0920 - val_acc: 0.4700\n",
      "Epoch 34/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1670 - acc: 0.4548 - val_loss: 2.1140 - val_acc: 0.4698\n",
      "Epoch 35/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1609 - acc: 0.4562 - val_loss: 2.1260 - val_acc: 0.4601\n",
      "Epoch 36/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.1551 - acc: 0.4560 - val_loss: 2.1450 - val_acc: 0.4582\n",
      "Epoch 37/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1535 - acc: 0.4587 - val_loss: 2.1709 - val_acc: 0.4576\n",
      "Epoch 38/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1565 - acc: 0.4549 - val_loss: 2.1904 - val_acc: 0.4450\n",
      "Epoch 39/100\n",
      "47562/47562 [==============================] - 7s 151us/step - loss: 2.1502 - acc: 0.4569 - val_loss: 2.2160 - val_acc: 0.4440\n",
      "Epoch 40/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1512 - acc: 0.4574 - val_loss: 2.2216 - val_acc: 0.4488\n",
      "Epoch 41/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1542 - acc: 0.4555 - val_loss: 2.2455 - val_acc: 0.4421\n",
      "Epoch 42/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1475 - acc: 0.4603 - val_loss: 2.2597 - val_acc: 0.4383\n",
      "Epoch 43/100\n",
      "47562/47562 [==============================] - 7s 151us/step - loss: 2.1406 - acc: 0.4585 - val_loss: 2.2807 - val_acc: 0.4342\n",
      "Epoch 44/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1420 - acc: 0.4592 - val_loss: 2.2907 - val_acc: 0.4342\n",
      "Epoch 45/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1483 - acc: 0.4563 - val_loss: 2.3030 - val_acc: 0.4308\n",
      "Epoch 46/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1365 - acc: 0.4602 - val_loss: 2.3238 - val_acc: 0.4283\n",
      "Epoch 47/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1488 - acc: 0.4598 - val_loss: 2.3422 - val_acc: 0.4187\n",
      "Epoch 48/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1348 - acc: 0.4609 - val_loss: 2.3650 - val_acc: 0.4220\n",
      "Epoch 49/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1491 - acc: 0.4577 - val_loss: 2.3659 - val_acc: 0.4204\n",
      "Epoch 50/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1360 - acc: 0.4610 - val_loss: 2.3857 - val_acc: 0.4182\n",
      "Epoch 51/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1359 - acc: 0.4613 - val_loss: 2.3950 - val_acc: 0.4147\n",
      "Epoch 52/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1348 - acc: 0.4603 - val_loss: 2.4182 - val_acc: 0.4093\n",
      "Epoch 53/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1418 - acc: 0.4591 - val_loss: 2.4413 - val_acc: 0.4090\n",
      "Epoch 54/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1387 - acc: 0.4601 - val_loss: 2.4522 - val_acc: 0.4046\n",
      "Epoch 55/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1349 - acc: 0.4603 - val_loss: 2.4662 - val_acc: 0.3994\n",
      "Epoch 56/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1411 - acc: 0.4584 - val_loss: 2.4809 - val_acc: 0.3994\n",
      "Epoch 57/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1307 - acc: 0.4614 - val_loss: 2.4792 - val_acc: 0.3984\n",
      "Epoch 58/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1353 - acc: 0.4616 - val_loss: 2.4989 - val_acc: 0.3938\n",
      "Epoch 59/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1373 - acc: 0.4603 - val_loss: 2.5014 - val_acc: 0.3984\n",
      "Epoch 60/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1315 - acc: 0.4626 - val_loss: 2.5109 - val_acc: 0.3967\n",
      "Epoch 61/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1297 - acc: 0.4607 - val_loss: 2.5442 - val_acc: 0.3879\n",
      "Epoch 62/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1258 - acc: 0.4621 - val_loss: 2.5473 - val_acc: 0.3869\n",
      "Epoch 63/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1278 - acc: 0.4616 - val_loss: 2.5729 - val_acc: 0.3841\n",
      "Epoch 64/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1377 - acc: 0.4622 - val_loss: 2.5913 - val_acc: 0.3806\n",
      "Epoch 65/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1288 - acc: 0.4601 - val_loss: 2.6024 - val_acc: 0.3707\n",
      "Epoch 66/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1320 - acc: 0.4605 - val_loss: 2.6182 - val_acc: 0.3816\n",
      "Epoch 67/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1322 - acc: 0.4629 - val_loss: 2.6243 - val_acc: 0.3751\n",
      "Epoch 68/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1296 - acc: 0.4619 - val_loss: 2.6293 - val_acc: 0.3728\n",
      "Epoch 69/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1189 - acc: 0.4632 - val_loss: 2.6441 - val_acc: 0.3691\n",
      "Epoch 70/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1300 - acc: 0.4609 - val_loss: 2.6498 - val_acc: 0.3684\n",
      "Epoch 71/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1273 - acc: 0.4629 - val_loss: 2.6557 - val_acc: 0.3701\n",
      "Epoch 72/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.1305 - acc: 0.4631 - val_loss: 2.6850 - val_acc: 0.3617\n",
      "Epoch 73/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1229 - acc: 0.4623 - val_loss: 2.6996 - val_acc: 0.3622\n",
      "Epoch 74/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1199 - acc: 0.4628 - val_loss: 2.7055 - val_acc: 0.3619\n",
      "Epoch 75/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.1224 - acc: 0.4636 - val_loss: 2.7189 - val_acc: 0.3567\n",
      "Epoch 76/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1179 - acc: 0.4636 - val_loss: 2.7196 - val_acc: 0.3613\n",
      "Epoch 77/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1221 - acc: 0.4640 - val_loss: 2.7404 - val_acc: 0.3563\n",
      "Epoch 78/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1139 - acc: 0.4643 - val_loss: 2.7605 - val_acc: 0.3441\n",
      "Epoch 79/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1217 - acc: 0.4655 - val_loss: 2.7828 - val_acc: 0.3521\n",
      "Epoch 80/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1186 - acc: 0.4622 - val_loss: 2.7751 - val_acc: 0.3534\n",
      "Epoch 81/100\n",
      "47562/47562 [==============================] - 7s 152us/step - loss: 2.1194 - acc: 0.4647 - val_loss: 2.8125 - val_acc: 0.3477\n",
      "Epoch 82/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1228 - acc: 0.4636 - val_loss: 2.8113 - val_acc: 0.3473\n",
      "Epoch 83/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1216 - acc: 0.4627 - val_loss: 2.8352 - val_acc: 0.3419\n",
      "Epoch 84/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1040 - acc: 0.4652 - val_loss: 2.8614 - val_acc: 0.3397\n",
      "Epoch 85/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1108 - acc: 0.4615 - val_loss: 2.8543 - val_acc: 0.3381\n",
      "Epoch 86/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1129 - acc: 0.4640 - val_loss: 2.8710 - val_acc: 0.3377\n",
      "Epoch 87/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1124 - acc: 0.4664 - val_loss: 2.8745 - val_acc: 0.3326\n",
      "Epoch 88/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1041 - acc: 0.4676 - val_loss: 2.8922 - val_acc: 0.3343\n",
      "Epoch 89/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1175 - acc: 0.4625 - val_loss: 2.9011 - val_acc: 0.3328\n",
      "Epoch 90/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1182 - acc: 0.4675 - val_loss: 2.9241 - val_acc: 0.3324\n",
      "Epoch 91/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1172 - acc: 0.4621 - val_loss: 2.9309 - val_acc: 0.3287\n",
      "Epoch 92/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.0988 - acc: 0.4681 - val_loss: 2.9340 - val_acc: 0.3310\n",
      "Epoch 93/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1161 - acc: 0.4659 - val_loss: 2.9376 - val_acc: 0.3264\n",
      "Epoch 94/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1099 - acc: 0.4662 - val_loss: 2.9693 - val_acc: 0.3236\n",
      "Epoch 95/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.1080 - acc: 0.4665 - val_loss: 2.9688 - val_acc: 0.3236\n",
      "Epoch 96/100\n",
      "47562/47562 [==============================] - 7s 151us/step - loss: 2.0982 - acc: 0.4669 - val_loss: 2.9897 - val_acc: 0.3226\n",
      "Epoch 97/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1022 - acc: 0.4672 - val_loss: 2.9894 - val_acc: 0.3174\n",
      "Epoch 98/100\n",
      "47562/47562 [==============================] - 7s 148us/step - loss: 2.1133 - acc: 0.4634 - val_loss: 3.0087 - val_acc: 0.3195\n",
      "Epoch 99/100\n",
      "47562/47562 [==============================] - 7s 150us/step - loss: 2.1074 - acc: 0.4665 - val_loss: 3.0217 - val_acc: 0.3171\n",
      "Epoch 100/100\n",
      "47562/47562 [==============================] - 7s 149us/step - loss: 2.0970 - acc: 0.4682 - val_loss: 3.0281 - val_acc: 0.3130\n",
      "5223/5223 [==============================] - 0s 34us/step\n",
      "[3.028121297760711, 0.3130384836358036]\n",
      "\n",
      "Fold  7\n",
      "Train on 47589 samples, validate on 5196 samples\n",
      "Epoch 1/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.3459 - acc: 0.4326 - val_loss: 1.2358 - val_acc: 0.7046\n",
      "Epoch 2/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.3005 - acc: 0.4381 - val_loss: 1.2842 - val_acc: 0.6905\n",
      "Epoch 3/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.2838 - acc: 0.4372 - val_loss: 1.3506 - val_acc: 0.6722\n",
      "Epoch 4/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.2572 - acc: 0.4435 - val_loss: 1.3769 - val_acc: 0.6617\n",
      "Epoch 5/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.2311 - acc: 0.4451 - val_loss: 1.4295 - val_acc: 0.6476\n",
      "Epoch 6/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.2266 - acc: 0.4461 - val_loss: 1.4698 - val_acc: 0.6322\n",
      "Epoch 7/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.2198 - acc: 0.4459 - val_loss: 1.4927 - val_acc: 0.6189\n",
      "Epoch 8/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1986 - acc: 0.4487 - val_loss: 1.5461 - val_acc: 0.6089\n",
      "Epoch 9/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.2052 - acc: 0.4465 - val_loss: 1.5706 - val_acc: 0.6008\n",
      "Epoch 10/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.1743 - acc: 0.4554 - val_loss: 1.6079 - val_acc: 0.5862\n",
      "Epoch 11/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1801 - acc: 0.4526 - val_loss: 1.6324 - val_acc: 0.5814\n",
      "Epoch 12/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.1719 - acc: 0.4529 - val_loss: 1.6712 - val_acc: 0.5681\n",
      "Epoch 13/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.1793 - acc: 0.4527 - val_loss: 1.7003 - val_acc: 0.5658\n",
      "Epoch 14/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.1579 - acc: 0.4560 - val_loss: 1.7156 - val_acc: 0.5631\n",
      "Epoch 15/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1738 - acc: 0.4528 - val_loss: 1.7479 - val_acc: 0.5537\n",
      "Epoch 16/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1660 - acc: 0.4540 - val_loss: 1.7662 - val_acc: 0.5491\n",
      "Epoch 17/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1600 - acc: 0.4548 - val_loss: 1.8048 - val_acc: 0.5416\n",
      "Epoch 18/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1595 - acc: 0.4542 - val_loss: 1.8279 - val_acc: 0.5319\n",
      "Epoch 19/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.1577 - acc: 0.4570 - val_loss: 1.8519 - val_acc: 0.5248\n",
      "Epoch 20/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.1516 - acc: 0.4566 - val_loss: 1.8665 - val_acc: 0.5196\n",
      "Epoch 21/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.1403 - acc: 0.4587 - val_loss: 1.8883 - val_acc: 0.5169\n",
      "Epoch 22/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1392 - acc: 0.4568 - val_loss: 1.9163 - val_acc: 0.5081\n",
      "Epoch 23/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1358 - acc: 0.4609 - val_loss: 1.9255 - val_acc: 0.5060\n",
      "Epoch 24/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.1375 - acc: 0.4591 - val_loss: 1.9469 - val_acc: 0.5044\n",
      "Epoch 25/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.1499 - acc: 0.4564 - val_loss: 1.9719 - val_acc: 0.5046\n",
      "Epoch 26/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1430 - acc: 0.4580 - val_loss: 1.9958 - val_acc: 0.4935\n",
      "Epoch 27/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1267 - acc: 0.4614 - val_loss: 2.0023 - val_acc: 0.4946\n",
      "Epoch 28/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.1372 - acc: 0.4593 - val_loss: 2.0212 - val_acc: 0.4908\n",
      "Epoch 29/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1358 - acc: 0.4581 - val_loss: 2.0426 - val_acc: 0.4796\n",
      "Epoch 30/100\n",
      "47589/47589 [==============================] - 7s 151us/step - loss: 2.1313 - acc: 0.4595 - val_loss: 2.0643 - val_acc: 0.4809\n",
      "Epoch 31/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1300 - acc: 0.4595 - val_loss: 2.0777 - val_acc: 0.4731\n",
      "Epoch 32/100\n",
      "47589/47589 [==============================] - 7s 151us/step - loss: 2.1290 - acc: 0.4636 - val_loss: 2.1050 - val_acc: 0.4690\n",
      "Epoch 33/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1369 - acc: 0.4593 - val_loss: 2.1166 - val_acc: 0.4650\n",
      "Epoch 34/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.1251 - acc: 0.4607 - val_loss: 2.1334 - val_acc: 0.4602\n",
      "Epoch 35/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1221 - acc: 0.4609 - val_loss: 2.1505 - val_acc: 0.4555\n",
      "Epoch 36/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.1302 - acc: 0.4588 - val_loss: 2.1709 - val_acc: 0.4552\n",
      "Epoch 37/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1146 - acc: 0.4614 - val_loss: 2.1957 - val_acc: 0.4478\n",
      "Epoch 38/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1149 - acc: 0.4638 - val_loss: 2.2123 - val_acc: 0.4459\n",
      "Epoch 39/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1155 - acc: 0.4648 - val_loss: 2.2242 - val_acc: 0.4411\n",
      "Epoch 40/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1141 - acc: 0.4640 - val_loss: 2.2487 - val_acc: 0.4388\n",
      "Epoch 41/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.1251 - acc: 0.4619 - val_loss: 2.2531 - val_acc: 0.4413\n",
      "Epoch 42/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.1211 - acc: 0.4640 - val_loss: 2.2650 - val_acc: 0.4375\n",
      "Epoch 43/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1103 - acc: 0.4653 - val_loss: 2.2849 - val_acc: 0.4344\n",
      "Epoch 44/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1060 - acc: 0.4679 - val_loss: 2.2999 - val_acc: 0.4296\n",
      "Epoch 45/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1164 - acc: 0.4654 - val_loss: 2.3145 - val_acc: 0.4294\n",
      "Epoch 46/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1132 - acc: 0.4631 - val_loss: 2.3340 - val_acc: 0.4274\n",
      "Epoch 47/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.1115 - acc: 0.4647 - val_loss: 2.3464 - val_acc: 0.4176\n",
      "Epoch 48/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0917 - acc: 0.4675 - val_loss: 2.3642 - val_acc: 0.4190\n",
      "Epoch 49/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.1022 - acc: 0.4649 - val_loss: 2.3753 - val_acc: 0.4172\n",
      "Epoch 50/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.1154 - acc: 0.4624 - val_loss: 2.3828 - val_acc: 0.4140\n",
      "Epoch 51/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1045 - acc: 0.4649 - val_loss: 2.4061 - val_acc: 0.4134\n",
      "Epoch 52/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1049 - acc: 0.4662 - val_loss: 2.4169 - val_acc: 0.4097\n",
      "Epoch 53/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.0996 - acc: 0.4690 - val_loss: 2.4213 - val_acc: 0.4115\n",
      "Epoch 54/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.1032 - acc: 0.4660 - val_loss: 2.4375 - val_acc: 0.4042\n",
      "Epoch 55/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1036 - acc: 0.4659 - val_loss: 2.4482 - val_acc: 0.4047\n",
      "Epoch 56/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.0997 - acc: 0.4655 - val_loss: 2.4701 - val_acc: 0.3980\n",
      "Epoch 57/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1009 - acc: 0.4661 - val_loss: 2.4981 - val_acc: 0.3957\n",
      "Epoch 58/100\n",
      "47589/47589 [==============================] - 7s 151us/step - loss: 2.0948 - acc: 0.4669 - val_loss: 2.5032 - val_acc: 0.3965\n",
      "Epoch 59/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.1121 - acc: 0.4623 - val_loss: 2.5065 - val_acc: 0.3903\n",
      "Epoch 60/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0942 - acc: 0.4692 - val_loss: 2.5260 - val_acc: 0.3876\n",
      "Epoch 61/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0925 - acc: 0.4693 - val_loss: 2.5466 - val_acc: 0.3872\n",
      "Epoch 62/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1013 - acc: 0.4672 - val_loss: 2.5632 - val_acc: 0.3770\n",
      "Epoch 63/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1016 - acc: 0.4657 - val_loss: 2.5619 - val_acc: 0.3805\n",
      "Epoch 64/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0913 - acc: 0.4698 - val_loss: 2.5820 - val_acc: 0.3730\n",
      "Epoch 65/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.1064 - acc: 0.4630 - val_loss: 2.6093 - val_acc: 0.3751\n",
      "Epoch 66/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.0933 - acc: 0.4662 - val_loss: 2.6171 - val_acc: 0.3703\n",
      "Epoch 67/100\n",
      "47589/47589 [==============================] - 7s 151us/step - loss: 2.0903 - acc: 0.4675 - val_loss: 2.6339 - val_acc: 0.3693\n",
      "Epoch 68/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.1015 - acc: 0.4662 - val_loss: 2.6276 - val_acc: 0.3726\n",
      "Epoch 69/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0892 - acc: 0.4700 - val_loss: 2.6567 - val_acc: 0.3653\n",
      "Epoch 70/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0976 - acc: 0.4684 - val_loss: 2.6654 - val_acc: 0.3712\n",
      "Epoch 71/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.0884 - acc: 0.4685 - val_loss: 2.6811 - val_acc: 0.3599\n",
      "Epoch 72/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0840 - acc: 0.4713 - val_loss: 2.6887 - val_acc: 0.3620\n",
      "Epoch 73/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0971 - acc: 0.4662 - val_loss: 2.7034 - val_acc: 0.3582\n",
      "Epoch 74/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.0859 - acc: 0.4687 - val_loss: 2.7156 - val_acc: 0.3562\n",
      "Epoch 75/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.0900 - acc: 0.4699 - val_loss: 2.7235 - val_acc: 0.3547\n",
      "Epoch 76/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.0967 - acc: 0.4666 - val_loss: 2.7264 - val_acc: 0.3564\n",
      "Epoch 77/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.0952 - acc: 0.4688 - val_loss: 2.7400 - val_acc: 0.3524\n",
      "Epoch 78/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0894 - acc: 0.4678 - val_loss: 2.7628 - val_acc: 0.3532\n",
      "Epoch 79/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0807 - acc: 0.4715 - val_loss: 2.7702 - val_acc: 0.3474\n",
      "Epoch 80/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.0750 - acc: 0.4723 - val_loss: 2.7725 - val_acc: 0.3460\n",
      "Epoch 81/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.0925 - acc: 0.4696 - val_loss: 2.7870 - val_acc: 0.3441\n",
      "Epoch 82/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.0895 - acc: 0.4674 - val_loss: 2.7935 - val_acc: 0.3397\n",
      "Epoch 83/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0891 - acc: 0.4701 - val_loss: 2.8212 - val_acc: 0.3408\n",
      "Epoch 84/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0774 - acc: 0.4704 - val_loss: 2.8411 - val_acc: 0.3430\n",
      "Epoch 85/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.0842 - acc: 0.4704 - val_loss: 2.8446 - val_acc: 0.3397\n",
      "Epoch 86/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0850 - acc: 0.4689 - val_loss: 2.8706 - val_acc: 0.3376\n",
      "Epoch 87/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0858 - acc: 0.4692 - val_loss: 2.8615 - val_acc: 0.3414\n",
      "Epoch 88/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0872 - acc: 0.4702 - val_loss: 2.8958 - val_acc: 0.3278\n",
      "Epoch 89/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.0831 - acc: 0.4717 - val_loss: 2.9071 - val_acc: 0.3283\n",
      "Epoch 90/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0797 - acc: 0.4712 - val_loss: 2.9046 - val_acc: 0.3264\n",
      "Epoch 91/100\n",
      "47589/47589 [==============================] - 7s 148us/step - loss: 2.0901 - acc: 0.4687 - val_loss: 2.8892 - val_acc: 0.3281\n",
      "Epoch 92/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.0841 - acc: 0.4713 - val_loss: 2.9189 - val_acc: 0.3204\n",
      "Epoch 93/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0875 - acc: 0.4680 - val_loss: 2.9148 - val_acc: 0.3281\n",
      "Epoch 94/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.0774 - acc: 0.4706 - val_loss: 2.9377 - val_acc: 0.3218\n",
      "Epoch 95/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0724 - acc: 0.4733 - val_loss: 2.9548 - val_acc: 0.3210\n",
      "Epoch 96/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0803 - acc: 0.4709 - val_loss: 2.9592 - val_acc: 0.3158\n",
      "Epoch 97/100\n",
      "47589/47589 [==============================] - 7s 149us/step - loss: 2.0696 - acc: 0.4745 - val_loss: 2.9760 - val_acc: 0.3170\n",
      "Epoch 98/100\n",
      "47589/47589 [==============================] - 7s 147us/step - loss: 2.0700 - acc: 0.4724 - val_loss: 2.9827 - val_acc: 0.3151\n",
      "Epoch 99/100\n",
      "47589/47589 [==============================] - 7s 150us/step - loss: 2.0918 - acc: 0.4671 - val_loss: 2.9787 - val_acc: 0.3176\n",
      "Epoch 100/100\n",
      "47589/47589 [==============================] - 7s 151us/step - loss: 2.0721 - acc: 0.4719 - val_loss: 3.0147 - val_acc: 0.3133\n",
      "5196/5196 [==============================] - 0s 34us/step\n",
      "[3.014657580182587, 0.31331793689746135]\n",
      "\n",
      "Fold  8\n",
      "Train on 47627 samples, validate on 5158 samples\n",
      "Epoch 1/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.3094 - acc: 0.4404 - val_loss: 1.1904 - val_acc: 0.7185\n",
      "Epoch 2/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.2815 - acc: 0.4402 - val_loss: 1.2386 - val_acc: 0.6983\n",
      "Epoch 3/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.2585 - acc: 0.4423 - val_loss: 1.2914 - val_acc: 0.6871\n",
      "Epoch 4/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.2359 - acc: 0.4477 - val_loss: 1.3281 - val_acc: 0.6716\n",
      "Epoch 5/100\n",
      "47627/47627 [==============================] - 7s 147us/step - loss: 2.1884 - acc: 0.4524 - val_loss: 1.3622 - val_acc: 0.6596\n",
      "Epoch 6/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.2048 - acc: 0.4504 - val_loss: 1.3972 - val_acc: 0.6501\n",
      "Epoch 7/100\n",
      "47627/47627 [==============================] - 7s 147us/step - loss: 2.1873 - acc: 0.4518 - val_loss: 1.4324 - val_acc: 0.6373\n",
      "Epoch 8/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.1794 - acc: 0.4523 - val_loss: 1.4770 - val_acc: 0.6282\n",
      "Epoch 9/100\n",
      "47627/47627 [==============================] - 7s 147us/step - loss: 2.1740 - acc: 0.4563 - val_loss: 1.5075 - val_acc: 0.6208\n",
      "Epoch 10/100\n",
      "47627/47627 [==============================] - 7s 147us/step - loss: 2.1667 - acc: 0.4536 - val_loss: 1.5414 - val_acc: 0.6154\n",
      "Epoch 11/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1693 - acc: 0.4541 - val_loss: 1.5631 - val_acc: 0.5983\n",
      "Epoch 12/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1560 - acc: 0.4561 - val_loss: 1.5899 - val_acc: 0.5985\n",
      "Epoch 13/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1539 - acc: 0.4567 - val_loss: 1.6169 - val_acc: 0.5867\n",
      "Epoch 14/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1383 - acc: 0.4584 - val_loss: 1.6397 - val_acc: 0.5851\n",
      "Epoch 15/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.1458 - acc: 0.4594 - val_loss: 1.6737 - val_acc: 0.5743\n",
      "Epoch 16/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1347 - acc: 0.4602 - val_loss: 1.7060 - val_acc: 0.5731\n",
      "Epoch 17/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1336 - acc: 0.4616 - val_loss: 1.7321 - val_acc: 0.5585\n",
      "Epoch 18/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1404 - acc: 0.4618 - val_loss: 1.7550 - val_acc: 0.5554\n",
      "Epoch 19/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1338 - acc: 0.4597 - val_loss: 1.7756 - val_acc: 0.5440\n",
      "Epoch 20/100\n",
      "47627/47627 [==============================] - 7s 147us/step - loss: 2.1313 - acc: 0.4614 - val_loss: 1.7951 - val_acc: 0.5477\n",
      "Epoch 21/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.1187 - acc: 0.4630 - val_loss: 1.8112 - val_acc: 0.5372\n",
      "Epoch 22/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1167 - acc: 0.4648 - val_loss: 1.8235 - val_acc: 0.5364\n",
      "Epoch 23/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1305 - acc: 0.4618 - val_loss: 1.8496 - val_acc: 0.5289\n",
      "Epoch 24/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1287 - acc: 0.4619 - val_loss: 1.8807 - val_acc: 0.5229\n",
      "Epoch 25/100\n",
      "47627/47627 [==============================] - 7s 147us/step - loss: 2.1124 - acc: 0.4637 - val_loss: 1.8953 - val_acc: 0.5186\n",
      "Epoch 26/100\n",
      "47627/47627 [==============================] - 7s 147us/step - loss: 2.1254 - acc: 0.4636 - val_loss: 1.9082 - val_acc: 0.5128\n",
      "Epoch 27/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1197 - acc: 0.4615 - val_loss: 1.9237 - val_acc: 0.5056\n",
      "Epoch 28/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1230 - acc: 0.4623 - val_loss: 1.9463 - val_acc: 0.5045\n",
      "Epoch 29/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1118 - acc: 0.4641 - val_loss: 1.9637 - val_acc: 0.4944\n",
      "Epoch 30/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.1056 - acc: 0.4649 - val_loss: 1.9894 - val_acc: 0.4889\n",
      "Epoch 31/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1126 - acc: 0.4631 - val_loss: 1.9997 - val_acc: 0.4903\n",
      "Epoch 32/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1100 - acc: 0.4675 - val_loss: 2.0183 - val_acc: 0.4868\n",
      "Epoch 33/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1003 - acc: 0.4652 - val_loss: 2.0405 - val_acc: 0.4804\n",
      "Epoch 34/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.1029 - acc: 0.4666 - val_loss: 2.0614 - val_acc: 0.4806\n",
      "Epoch 35/100\n",
      "47627/47627 [==============================] - 7s 147us/step - loss: 2.1024 - acc: 0.4658 - val_loss: 2.0783 - val_acc: 0.4698\n",
      "Epoch 36/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.1073 - acc: 0.4630 - val_loss: 2.0884 - val_acc: 0.4742\n",
      "Epoch 37/100\n",
      "47627/47627 [==============================] - 7s 146us/step - loss: 2.0974 - acc: 0.4655 - val_loss: 2.1116 - val_acc: 0.4620\n",
      "Epoch 38/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.1095 - acc: 0.4636 - val_loss: 2.1233 - val_acc: 0.4636\n",
      "Epoch 39/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0929 - acc: 0.4694 - val_loss: 2.1425 - val_acc: 0.4604\n",
      "Epoch 40/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0956 - acc: 0.4660 - val_loss: 2.1501 - val_acc: 0.4519\n",
      "Epoch 41/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1003 - acc: 0.4679 - val_loss: 2.1684 - val_acc: 0.4473\n",
      "Epoch 42/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.0980 - acc: 0.4675 - val_loss: 2.1988 - val_acc: 0.4477\n",
      "Epoch 43/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.0923 - acc: 0.4672 - val_loss: 2.2146 - val_acc: 0.4407\n",
      "Epoch 44/100\n",
      "47627/47627 [==============================] - 7s 146us/step - loss: 2.0992 - acc: 0.4679 - val_loss: 2.2349 - val_acc: 0.4391\n",
      "Epoch 45/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0945 - acc: 0.4689 - val_loss: 2.2451 - val_acc: 0.4407\n",
      "Epoch 46/100\n",
      "47627/47627 [==============================] - 8s 173us/step - loss: 2.0949 - acc: 0.4660 - val_loss: 2.2548 - val_acc: 0.4335\n",
      "Epoch 47/100\n",
      "47627/47627 [==============================] - 8s 158us/step - loss: 2.1035 - acc: 0.4651 - val_loss: 2.2714 - val_acc: 0.4352\n",
      "Epoch 48/100\n",
      "47627/47627 [==============================] - 7s 151us/step - loss: 2.0809 - acc: 0.4689 - val_loss: 2.2906 - val_acc: 0.4283\n",
      "Epoch 49/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0884 - acc: 0.4698 - val_loss: 2.3059 - val_acc: 0.4221\n",
      "Epoch 50/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0909 - acc: 0.4712 - val_loss: 2.3283 - val_acc: 0.4180\n",
      "Epoch 51/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0866 - acc: 0.4687 - val_loss: 2.3324 - val_acc: 0.4110\n",
      "Epoch 52/100\n",
      "47627/47627 [==============================] - 7s 152us/step - loss: 2.0950 - acc: 0.4667 - val_loss: 2.3466 - val_acc: 0.4135\n",
      "Epoch 53/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0836 - acc: 0.4697 - val_loss: 2.3605 - val_acc: 0.4100\n",
      "Epoch 54/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0911 - acc: 0.4671 - val_loss: 2.3846 - val_acc: 0.4079\n",
      "Epoch 55/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.1005 - acc: 0.4671 - val_loss: 2.3825 - val_acc: 0.4062\n",
      "Epoch 56/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0868 - acc: 0.4701 - val_loss: 2.4062 - val_acc: 0.4040\n",
      "Epoch 57/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0841 - acc: 0.4705 - val_loss: 2.4168 - val_acc: 0.4003\n",
      "Epoch 58/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.0881 - acc: 0.4685 - val_loss: 2.4277 - val_acc: 0.3998\n",
      "Epoch 59/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0819 - acc: 0.4695 - val_loss: 2.4568 - val_acc: 0.3951\n",
      "Epoch 60/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0778 - acc: 0.4689 - val_loss: 2.4650 - val_acc: 0.3949\n",
      "Epoch 61/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0757 - acc: 0.4734 - val_loss: 2.4796 - val_acc: 0.3924\n",
      "Epoch 62/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0938 - acc: 0.4688 - val_loss: 2.4823 - val_acc: 0.3957\n",
      "Epoch 63/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.0775 - acc: 0.4700 - val_loss: 2.5043 - val_acc: 0.3916\n",
      "Epoch 64/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0808 - acc: 0.4677 - val_loss: 2.5197 - val_acc: 0.3883\n",
      "Epoch 65/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0843 - acc: 0.4686 - val_loss: 2.5297 - val_acc: 0.3860\n",
      "Epoch 66/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0886 - acc: 0.4658 - val_loss: 2.5390 - val_acc: 0.3856\n",
      "Epoch 67/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0795 - acc: 0.4723 - val_loss: 2.5461 - val_acc: 0.3854\n",
      "Epoch 68/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.0745 - acc: 0.4712 - val_loss: 2.5536 - val_acc: 0.3837\n",
      "Epoch 69/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.0773 - acc: 0.4723 - val_loss: 2.5653 - val_acc: 0.3786\n",
      "Epoch 70/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0664 - acc: 0.4731 - val_loss: 2.5768 - val_acc: 0.3829\n",
      "Epoch 71/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0832 - acc: 0.4711 - val_loss: 2.6000 - val_acc: 0.3757\n",
      "Epoch 72/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0832 - acc: 0.4694 - val_loss: 2.5937 - val_acc: 0.3755\n",
      "Epoch 73/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0750 - acc: 0.4729 - val_loss: 2.6106 - val_acc: 0.3748\n",
      "Epoch 74/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0813 - acc: 0.4709 - val_loss: 2.6177 - val_acc: 0.3748\n",
      "Epoch 75/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0649 - acc: 0.4721 - val_loss: 2.6467 - val_acc: 0.3693\n",
      "Epoch 76/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0665 - acc: 0.4748 - val_loss: 2.6681 - val_acc: 0.3662\n",
      "Epoch 77/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0758 - acc: 0.4703 - val_loss: 2.6613 - val_acc: 0.3666\n",
      "Epoch 78/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0601 - acc: 0.4752 - val_loss: 2.6831 - val_acc: 0.3639\n",
      "Epoch 79/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0659 - acc: 0.4753 - val_loss: 2.6960 - val_acc: 0.3616\n",
      "Epoch 80/100\n",
      "47627/47627 [==============================] - 7s 148us/step - loss: 2.0685 - acc: 0.4696 - val_loss: 2.7083 - val_acc: 0.3602\n",
      "Epoch 81/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0744 - acc: 0.4698 - val_loss: 2.6986 - val_acc: 0.3627\n",
      "Epoch 82/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0664 - acc: 0.4744 - val_loss: 2.7288 - val_acc: 0.3591\n",
      "Epoch 83/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0675 - acc: 0.4737 - val_loss: 2.7378 - val_acc: 0.3561\n",
      "Epoch 84/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0720 - acc: 0.4711 - val_loss: 2.7461 - val_acc: 0.3536\n",
      "Epoch 85/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0802 - acc: 0.4719 - val_loss: 2.7596 - val_acc: 0.3501\n",
      "Epoch 86/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0681 - acc: 0.4741 - val_loss: 2.7698 - val_acc: 0.3499\n",
      "Epoch 87/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0654 - acc: 0.4734 - val_loss: 2.7803 - val_acc: 0.3534\n",
      "Epoch 88/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0661 - acc: 0.4710 - val_loss: 2.7746 - val_acc: 0.3486\n",
      "Epoch 89/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0588 - acc: 0.4734 - val_loss: 2.7952 - val_acc: 0.3455\n",
      "Epoch 90/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0700 - acc: 0.4711 - val_loss: 2.7851 - val_acc: 0.3465\n",
      "Epoch 91/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0571 - acc: 0.4757 - val_loss: 2.8042 - val_acc: 0.3432\n",
      "Epoch 92/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0669 - acc: 0.4740 - val_loss: 2.8297 - val_acc: 0.3354\n",
      "Epoch 93/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0586 - acc: 0.4756 - val_loss: 2.8417 - val_acc: 0.3339\n",
      "Epoch 94/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0596 - acc: 0.4747 - val_loss: 2.8462 - val_acc: 0.3342\n",
      "Epoch 95/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0590 - acc: 0.4761 - val_loss: 2.8568 - val_acc: 0.3356\n",
      "Epoch 96/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0616 - acc: 0.4765 - val_loss: 2.8518 - val_acc: 0.3370\n",
      "Epoch 97/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0587 - acc: 0.4734 - val_loss: 2.8652 - val_acc: 0.3348\n",
      "Epoch 98/100\n",
      "47627/47627 [==============================] - 7s 149us/step - loss: 2.0660 - acc: 0.4737 - val_loss: 2.8784 - val_acc: 0.3284\n",
      "Epoch 99/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0628 - acc: 0.4728 - val_loss: 2.9005 - val_acc: 0.3271\n",
      "Epoch 100/100\n",
      "47627/47627 [==============================] - 7s 150us/step - loss: 2.0694 - acc: 0.4706 - val_loss: 2.9025 - val_acc: 0.3325\n",
      "5158/5158 [==============================] - 0s 34us/step\n",
      "[2.902482877872211, 0.33249321444730695]\n",
      "\n",
      "Fold  9\n",
      "Train on 47651 samples, validate on 5134 samples\n",
      "Epoch 1/100\n",
      "47651/47651 [==============================] - 7s 150us/step - loss: 2.2971 - acc: 0.4390 - val_loss: 1.1778 - val_acc: 0.7193\n",
      "Epoch 2/100\n",
      "25500/47651 [===============>..............] - ETA: 3s - loss: 2.2450 - acc: 0.445"
     ]
    }
   ],
   "source": [
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    print('\\nFold ',j)\n",
    "    X_train_cv = X[train_idx]\n",
    "    y_train_cv = np_utils.to_categorical(Y[train_idx], 319)\n",
    "    X_valid_cv = X[val_idx]\n",
    "    y_valid_cv= np_utils.to_categorical(Y[val_idx], 319)\n",
    "    \n",
    "    hist = model.fit(X_train_cv, y_train_cv, \n",
    "                     epochs=100, \n",
    "                     batch_size=300, \n",
    "                     verbose=1, \n",
    "                     validation_data=(X_valid_cv, y_valid_cv), \n",
    "                     shuffle=True)\n",
    "    print(model.evaluate(X_valid_cv, y_valid_cv))\n",
    "\n",
    "    \n",
    "model.save_weights('semi_model.h5')\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('semi_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('forth_model.h5')\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('forth_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 3.0])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
